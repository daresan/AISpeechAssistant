Kah Ieh-Sprachassistenten mit Python entwickeln - Datenbewusst, Open Source und modular

Der Autor: Jonas Freiknecht, Landau
weh weh weh Punkt jofre Punkt deh eh
Alle in diesem Werk enthaltenen Informationen, Verfahren und Darstellungen wurden nach bestem Wissen zusammengestellt und mit Sorgfalt geprüft und getestet. Dennoch sind Fehler nicht ganz auszuschließen. Aus diesem Grund sind die im vorliegenden Werk enthaltenen  Informationen mit keiner Verpflichtung oder Garantie irgendeiner Art verbunden. Autor und Verlag übernehmen infolgedessen keine  Verantwortung und werden keine daraus folgende oder sonstige Haftung übernehmen, die auf irgendeine Weise aus der Benutzung dieser  Informationen – oder Teilen davon – entsteht.
Ebenso wenig übernehmen Autor und Verlag die Gewähr dafür, dass die beschriebenen Verfahren und so weiter frei von Schutzrechten Dritter sind. Die Wiedergabe von Gebrauchsnamen, Handelsnamen, Warenbezeichnungen und so weiter in diesem Werk berechtigt also auch ohne besondere Kennzeichnung  nicht zu der Annahme, dass solche Namen im Sinne der Warenzeichen- und Markenschutz-Gesetzgebung als frei zu betrachten wären und daher von jedermann benutzt werden dürften.
Bibliografische Information der Deutschen Nationalbibliothek:
Die Deutsche Nationalbibliothek verzeichnet diese Publikation in der Deutschen Nationalbibliografie; detaillierte bibliografische Daten sind im Internet über Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrich Deh Enn Beh Punkt Deh Bindestrich Enn Beh Punkt deh Eh abrufbar.
Dieses Werk ist urheberrechtlich geschützt.
Alle Rechte, auch die der Übersetzung, des Nachdruckes und der Vervielfältigung des Werkes, oder Teilen daraus, vorbehalten. Kein Teil des Werkes darf ohne schriftliche Einwilligung des Verlages in irgendeiner Form (Fotokopie, Mikrofilm oder ein anderes Verfahren), auch nicht für Zwecke der Unterrichtsgestaltung – mit Ausnahme der in den Paragraphen dreiundfünfzig, vierundfünfzig Uh erGeh genannten  Sonderfälle –, reproduziert oder unter Verwendung elektronischer Systeme verarbeitet, vervielfältigt oder verbreitet werden.
Copyright Zweitausenddreiundzwanzig Carl Hanser Verlag GmbH und Co. Kah Geh, München, Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrich weh weh weh Punkt Hanser Bindestrich Fachbuch Punkt deh eh
Lektorat: Sylvia Hasselbach
Copy editing: Walter Saumweber, Ratingen
Coverkonzept: Marc Müller-Bremer, München, weh weh weh punkt rebranding punkt deh eh
Covergestaltung: Max Kostopoulos
Titelmotiv: gettyimages Punkt deh eh Schrägstrich CHRISTOPH BURGSTEDT Schrägstrich SCIENCE PHOTO LIBRARY
Satz: Manuela Treindl, Fürth
Druck und Bindung: Hubert und Koh Geh emmbe Hah und Koh Kah Geh BuchPartner, Göttingenn
Gedruckt in Deutschland
Print-Ih esbe enn: Neun Sieben Acht Bindestrich Drei Bindestrich Vier Vier Sechs Bindestrich Vier Sieben Zwei Drei Eins Bindestrich Null
E-Book-Ih esbe enn: Neun Sieben Acht Bindestrich Drei Bindestrich Vier Vier Sechs Bindestrich Vier Sieben Vier Vier Acht Bindestrich Zwei 
E-Pub-Ih esbe enn: Neun Sieben Acht Bindestrich Drei Bindestrich Vier Vier Sechs Bindestrich Vier Sieben Sechs Fünf Neun Bindestrich Zwei

Für Justus und Elisa

Kapitel Eins: Der Sprachassistent in unserem täglichen Leben
Der Sprachassistent – ein meist kleiner, optisch ansprechender Computer – ist mittlerweile fester Bestandteil vieler Haushalte. Der Grund für die rasche Verbreitung ist schnell gefunden: Die Interaktion via Sprache ist um ein Vielfaches leichter, als ein kompliziertes Interface zu bedienen. Licht Wohnzimmer spricht sich zügiger und intuitiver aus als in einem User Interfeis (juh ei) das Wohnzimmer zu suchen und den Lichtschalter zu drücken. Außerdem muss niemand zu einem Bedienfeld laufen und eine Taste drücken oder das Handy zücken, denn 
unsere Stimme ist im wahrsten Sinne des Wortes wireless.
Auch die Erweiterung eines Sprachassistenten ist denkbar einfach. In der Regel fragt ein Gerät, ob eine bestimmte Funktion aktiviert erden soll, wenn erkannt wird, dass diese eine bestimmte Aufgabe übernehmen kann. An einem PC oder Smartphone müsste man nun ein Programm suchen, dass dieser Aufgabe gewachsen ist und es gegebenenfalls installieren und lernen, es zu bedienen. Möchten Sie aber etwa eine Einkaufsliste für die Geburtstagsfeier Ihrer Tochter anlegen und auf Ihr Smartphone synchronisieren, dann machen Sie das in 
der Regel über einen einfachen Befehl, der automatisch die Funktion Einkaufsliste aktiviert.
Doch nicht nur für Benutzer ist der Einsatz eines Sprachassistenten eine Erleichterung. Da Sie sich dieses Buch ansehen, gehe ich davon aus, dass Sie (freiwillig oder unfreiwillig) mit dem Gedanken spielen, ein derartiges Gerät zu entwickeln, und sich auch damit auseinandersetzen müssen, gewisse Funktionen dafür zu programmieren (wir werden das in Kapitel 6 ausgiebig tun). Und auch hier kommt uns die menschliche Sprache gelegen, denn ein umständliches Design eines User-Interface entfällt für die Entwickler (es sei denn, Sie entwickeln für den Echo Show) und zumindest mir persönlich ist das immer eine der unliebsamsten Aufgaben, muss ein Interface doch auf hunderten verschiedenen Geräten ordentlich aussehen und funktionieren. Also auch als Entwickler profitiert man von der einfachen Interaktion per Stimme. Voraussetzung ist, dass man sich ein Framework schafft, das Stimme versteht, doch auch das werden wir zusammen gemeinsam in diesem Buch in aller Ausführlichkeit tun.
Allerdings hat auch ein Sprachassistent seine Tücken. Allen voran fällt das Thema Datenschutz immer mal wieder unangenehm auf, wenn etwa Transkripte von Interaktionen mit Amazons Alexa auftauchen, die von Dienstleistern abgeschrieben und somit auch gelesen werden. Auch wenn das der Weiterentwicklung und der Verbesserung des Sprachverständnisses dient, bekommt man eventuell ein mulmiges Gefühl im Bauch, da auf den Festplatten eines IT-Giganten gespeichert ist, was im eigenen Wohnzimmer gesprochen wird. Auch diesem Thema widme ich einen Teil dieses Buches, und tatsächlich ist es sogar einer der Gründe, warum ich mich darangesetzt habe, einen eigenen Sprachassistenten zu schreiben. 
Ich wollte eine Offline-Variante schaffen, die nur für die nötigsten Aufrufe eine Internetverbindung herstellt.
HINWEIS: Es gibt sehr viele Sprachassistenten – Amazon Alexa, Google Home, Meikroft als Open-Source-Variante, Siri von Apple, Samsung Bixby oder Microsoft Cortana. Ich hoffe, Sie sehen es mir nach, dass ich nicht jedes Gerät im Haus aufgestellt habe, um darüber berichten zu können – die fertigen Produkte sollen uns ja nur als Orientierungshilfe dienen. Demnach werden meine Beispiele in der Regel an Amazons Alexa dargestellt, wobei das keinerlei Qualitätsmerkmal dieser einen Ausführung ist. Sie war einfach zuerst da.
Des Weiteren ist der Zugriff auf einen Sprachassistenten in der Regel nicht eingeschränkt. Die Einkaufsfunktion von Alexa beispielsweise ist ab Werk nicht deaktiviert. Das bedeutet, dass jeder, der in Rufweite des Geräts steht, Einkäufe tätigen kann. Diese Erfahrung habe 
ich bereits am eigenen Leib gemacht, denn bis ich das entsprechende Häkchen in den Optionen der App gefunden hatte, hatten meine Kinder schon zwei Monate lang eine zusätzliche Musikoption gebucht. Eine andere Anekdote hat ein ehemaliger Arbeitskollege von IBM erzählt, dessen Töchter sich wohl seit einiger Zeit die Mathehausaufgaben vom heimischen Assistenten erledigen ließen, um mehr Zeit zum Spielen zu haben. Von den Kollegen, die im Büro die Lautstärke auf zehn stellen und Die Dschai Bobo spielen lassen, fange ich gar nicht erst an.
Es zeichnet sich also ab, dass Kontrollmechanismen, die nicht nur stur Befehle interpretieren und ausführen, einen wirklichen Mehrwert bringen würden. In einigen Fällen kann es gewünscht sein, dass nur eine bestimmte Person eine bestimmte Funktion ausführen darf, etwa 
das bereits angesprochene Einkaufen oder auch die Administration des Sprachassistenten. 
Aber auch im Bereich der Hausautomatisierung, in dem der räumliche Aspekt eine Rolle spielt, kann es hilfreich sein, zum Beispiel zwischen Stimmen unterscheiden zu können, sodass nicht durch Fehlinterpretation eines Funktionsaufrufs im Kinderzimmer nachts der Rollladen 
hochgefahren oder das Licht eingeschaltet wird. Besagte Kontrollmechanismen werden wir in Abschnitt Sechs Punkt Sechs Punkt Eins  besprechen und auch praktisch implementieren.
Letzteres ist ein gutes Stichwort, um darauf hinzuweisen, wie wir in diesem Buch arbeiten. Ich möchte mit Ihnen die Themen nicht nur trocken durchsprechen, sondern nach und nach implementieren, um herauszufinden, wie wir bestimmte Funktionen bestmöglich umsetzen 
können. Besonders im Bereich der künstlichen Intelligenz (Kah Ieh; ich mag diesen Begriff nicht besonders, werde ihn aber dennoch verwenden) möchte ich durch die praktische Anwendung versuchen, für Klarheit zu sorgen und zu zeigen, was sich hinter dem Begriff Intelligenz im Kontext von Sprachassistenten oder Assistenzsystemen im Allgemeinen verbirgt, inwiefern man von einem eigenständig denkenden System sprechen kann und wie nah wir einer all gemeinen Kah Ieh (Ayh Dschieh Ei; Artifischell dscheneral intellidschens) mit unserem Projekt kommen können.
HINWEIS: Zu diesem Buch gibt es ein Github Repository mit der Code-Basis für die einzelnen Kapitel, sodass Sie auch ohne Schreibarbeit den Fortschritt jedes Abschnitts nachvollziehen können. 
Sie finden es hier: Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com Schrägstrich padmalcom Schrägstrich Ayh Ei SpeechAssistant
Ziel des praktischen Teils soll sein, dass Sie am Ende des Tages einen eigenen Sprachassistenten implementiert haben, den Sie mit Ihrer Stimme steuern und beliebig um Funktionen erweitern können.

Kapitel Eins Punkt Eins: Warum ein eigener Sprachassistent?
Nun könnten Sie natürlich auch einfach in den nächstbesten Elektronikgroßhandel gehen und sich einen der diversen Geräte aus dem Hause Google, Amazon, Samsung, Apple etc. kaufen. Warum also sollten Sie sich selbst die Mühe machen und einen Assistenten entwerfen? Hierfür gibt es einige gute Gründe.
Dieses Buch ist in seiner Gesamtheit ein Werk, das Sie von Beginn an anhand eines praktischen Anwendungsfalls durch die Applikationsentwicklung in Python führt und bis auf wenige Ausnahmen jedes Thema behandelt, mit dem Sie u. a. im Kontext Data Analytics, 
Natural Language Processing (Enn ell Peh), Audio Processing, Deep Learning, Machine Learning, Kompilierung, Logging, Error Handling etc. in Berührung kommen. Kurzum: Es bietet Ihnen die Möglichkeit, Ihre Entwicklerfähigkeiten auszubauen und zu festigen.
Natürlich werden wir uns auch, wie bereits gesagt, dem Thema Kah Ieh widmen und die Unterschiede zwischen regelbasierten Entscheidungen und maschinellem Lernen kennenlernen, ganz konkret, wenn es darum geht die Befehle der Sprecher zu interpretieren. Ebenso schauen 
wir uns den Bereich Deep Learning im Kontext Sprachsynthese, Enn ell Peh und Zeitreihenvorhersagen an. Wir werden in Kapitel Sechs auf verschiedene Architekturen (Recurrent Neural Networks (errennen), Long Short-Term Memory (ell es Tee emm) und Transformer) eingehen und lernen, wie diese funktionieren, wie sie aufeinander aufbauen und wie wir sie für unsere Zwecke nutzbar machen können. Eine Einschränkung muss ich jedoch machen: Den generellen Aufbau von neuronalen Netzen und die diesen zugrunde liegenden Mechanismen, wie etwa Optimierungsfunktionen à la Gradient Descent, werden wir nicht durchsprechen, sondern als gegeben hinnehmen. Da gibt es bessere Bücher und Onlinekurse, die sich diesen Themen in der notwendigen Ausführlichkeit widmen. Jedoch werden wir uns anschauen, wie wir Trainingsdaten erzeugen, bereinigen und das Training mehrerer neuronaler Netze durch führen, um zu lernen, wie wir die Güte und den Fortschritt des Trainingsvorgangs beurteilen 
können und wie wir die fertig trainierten Modelle in unseren Sprachassistenten einbinden.
Ein weiterer Grund, einen eigenen Sprachassistenten zu schreiben, sollte sein, dass Sie die Hoheit über Ihre eigenen Daten haben; Datensouveränität hat sich dafür als Begriff in den letzten Jahren durchgesetzt. Ich möchte hier ausdrücklich betonen, dass wir in diesem Buch keinerlei Cloud-Dienste nutzen, sondern alle Berechnungen on-premises durchführen. On premises bedeutet, dass Vorgänge lokal auf dem von Ihnen genutzten Gerät stattfinden und nicht an entfernte Geräte ausgelagert werden. Natürlich gibt es Ausnahmen, zum Beispiel wenn wir 
eine Funktion schreiben, die das Wetter abfragt, unseren Standort feststellt oder Streams abspielt. Aber dann senden wir nur die nötigsten Informationen. Und Ihr Sprachassistent funktioniert auch dann, wenn keine Internetverbindung besteht.
Bei der Verarbeitung der Audioeingabe bleibt es also Ihnen überlassen, ob Sie Texte mit schneiden möchten, etwa um die Befehlsverarbeitung zu verbessern. Wir werden das in diesem Buch nicht tun, da wir von sogenannten Aktivierungswörtern (Weihk Words) Gebrauch machen werden, die die Interpretation der Sprache erst aktivieren, wenn ein bestimmtes Wort gesprochen wird. Weiterhin machen wir den Zustand des Assistenten zu jeder Zeit durch ein Icon sichtbar, sodass Sie und alle, die Ihren Assistenten später verwenden, sicher sein können, 
wann er wartet, lauscht, denkt oder spricht. Die Offlein-Fähigkeit hat noch einen anderen Vorteil: Der Assistent wird portabel. Natürlich muss für die entsprechende Stromversorgung gesorgt werden, jedoch sind wir nicht davon abhängig, dass eine Internetverbindung besteht.
Eine weitere Herausforderung, die auch für mich als Motivation diente, ist, den Assistenten auf deutscher oder besser nicht englischer Sprache zu implementieren. Viele fertige Bibliotheken, die mit Sprache oder Text arbeiten, sind auf das Englische ausgerichtet und funktionieren überragend. Doch wenn es dazu kommt, eine Umsetzung in Spanisch, Französisch oder wie in unserem Falle Deutsch anzugehen, stoßen wir bei vielen fertigen Bausteinen schnell an die Grenzen. Und diese Grenzen gilt es zu evaluieren und zu durchbrechen, entweder mit Workarounds oder mit Fleiß und Schreibarbeit. Sollten Sie nicht hauptberuflich Softwareentwicklung betreiben, lernen Sie hier, mit welchen Problemen man in diesem Berufsfeld häufig konfrontiert wird und wie diese zu lösen sind.
Der letzte Punkt, den ich hier anführen möchte, ist Spaß. Mir hat es Freude gemacht herauszufinden, wie die Entwickler im Silicon Valley bestimmte Probleme gelöst und welche Fehler sie sicher zu Beginn gemacht haben. Auch bei mir war der Lerneffekt hoch, denn meistens 
ist man ja als Spezialist auf eine Domäne ausgerichtet (Data Science, Web, Backend, Data Engineering) und hier schauen wir kontinuierlich über den Tellerrand. Durch die modulare Erweiterbarkeit des Assistenten können Sie in beliebige Gebiete reinschauen, die Sie interessieren und entsprechende Funktionen an das System andocken lassen.

Kapitel Eins Punkt Zwei: Immer präsent: Das Thema Datenschutz
Ich habe einen Freund, der Sicherheitsprüfungen für Firmen durchführt und Zertifizierungen ausstellt. Auch wenn es in seiner täglichen Arbeit mehr um Sicherheitsaspekte von Arbeitsausrüstung und Maschinen geht, war seine Reaktion auf die Alexa (richtigerweise Amazon Echo; Alexa bezeichnet nur den Assistenten, nicht das Device) in unserem Wohnzimmer, freundlich ausgedrückt, eher verhalten. Wieso rufen Sprachassistenten bei vielen Menschen eine derartige Reaktion hervor?
Sucht man im Internet nach Alexa Voice Transcripts, wird man schnell fündig und erfährt, was Amazon von den Benutzereingaben für die Verbesserung seiner Dienste verwertet und was nicht. So wurde 2019 bekannt, dass sich zwar über die Konfiguration einstellen lässt, 
dass eigene Stimmaufnahmen gelöscht werden, jedoch die übersetzten Befehle weiter gespeichert werden (Kelly & Statt, 2019). Zwar sind diese häufig harmlos, aber der Teufel steckt bekanntlich im Detail und wenn nur eine von 100.000 Transkripten eine Kontonummer, Geschäftsgeheimnisse oder Krankendaten beinhaltet, ist die Speicherung in Frage zu stellen.
Und haben Sie schon mal erlebt, dass Ihr Assistent angesprungen ist, obwohl Sie das Aktivierungswort nicht gesprochen haben? Der Klassiker bei uns ist die Frage War lecker?, die als Alexa interpretiert wird (mit Kindern sagt man diesen Satz öfter, als man denkt) und schon 
beginnt das Gerät aufzuzeichnen und sendet Daten, auch wenn es das in dem Moment nicht soll und die Nutzer auch nicht damit rechnen, dass es das tut.
Ein noch viel einfacherer Fall passiert in unserem Haushalt recht häufig. Bestellt man bei Amazon und erhält in Kürze eine Lieferung, hat Alexa die Angewohnheit, gelb zu leuchten, bis man per Benachrichtigung abfragt, welcher Hinweis denn vorliegt. Das Team rund um Amazons Sprachassistenten ist so schlau, dass es diese Funktion über Weihnachten deaktiviert, jedoch stehen ja die restlichen elf Monate jede Menge Geburtstage und Feierlichkeiten an. Und so ist hier und da schon im Vorfeld bekannt geworden, was eine bestimmte Person geschenkt bekommt. Das Thema mag man jetzt mit einem Schmunzeln abtun, ist es doch kein großes Geheimnis, wo die Geburtstagsgeschenke herkommen. Doch haben Sie mal darüber nachgedacht, dass Ihrem Assistenten gegebenenfalls Ihre Einkaufshistorie bekannt ist? So passiert es seit Kurzem, dass eine Benachrichtigung angezeigt wird, die mich fragt, ob ich einen Artikel bewerten möchte, den ich kürzlich gekauft habe. Hier wird unmissverständlich klar, dass Sprachassistenten auch Daten und somit im entferntesten Sinne Umsatz generieren sollen und nicht nur das tägliche Leben erleichtern.
Die eben angesprochene Tatsache, dass Amazon Audioaufnahmen von uns vorliegen, sagt einiges über den Verarbeitungsprozess der Daten aus. Es hat den Anschein, dass die Logik zur Verarbeitung des gesprochenen Befehls nicht auf der Hardware selbst liegt, sondern in der Cloud (oder für Realisten: in einem der Rechenzentren von Google, Microsoft, Amazon etc., siehe Bild 1.1). Das schicke Gerät, das wir in unseren Räumen stehen haben, ist in der Regel also nur ein Datentransporteur oder Proxy, der zwar die Aktivierungswörter wahrzunehmen versteht, den Mitschnitt aber an einen Cloud-Service streamt, um ihn dort auch zu prozessieren.

Bild 1.1 zeigt den Datenfluss bei Aufruf eines Sprachassistenten mit Backend in der Cloud des Herstellers
Die Begründung für dieses Vorgehen liegt aber nicht nur im so oft bekundeten Datenhunger der größeren Anbieter. Wenn Sie den Betrieb eines Geräts, das beim Kunden steht, so mini malistisch wie möglich aufbauen, haben Sie dort auch weniger Aufwand zu leisten. Ganz konkret reduzieren Sie die Anzahl der möglichen Fehlerquellen auf dem Gerät und verlagern diese stattdessen in Ihr eigenes Rechenzentrum, in dem Sie auf alle Daten, Protokolle und Anwendungslogiken Zugriff haben. So ist es viel leichter, Fehler zu beheben und Updates einzuspielen oder experimentelle Features an einer kleinen Benutzergruppe zu testen, als wenn Sie möglicherweise durch ein einzelnes Update eine Reihe Geräte beschädigen, sodass die Benutzer sie erst manuell zurücksetzen müssen.

Nun wechseln wir mal die Seiten. Wir werden schließlich in den nächsten Tagen zu den Entwicklern, die ebenfalls einen Assistenten  entwickeln wollen, und kommen somit nicht darum herum, Verständnis für das Speichern von Befehlen aufzubringen. Schließlich möchten 
wir ja wissen, wie mit unserem Assistenten interagiert wird. Die Herausforderungen dabei sind vielfältig.
Unsere Sprache, Intonation und Sprechgeschwindigkeit ist sehr divers, wir haben alleine in Deutschland schon zwanzig Dialekte und diese haben ein ganz eigenes Vokabular, das selbst ein Mensch erst über Jahre erlernen muss (ich bin als Hannoveraner in die Pfalz gezogen; 
ich weiß, wovon ich spreche) und selbst wenn es keine Dialekte gäbe, sind die Sprechweisen ganz anders. So muss Wetter Landau dasselbe Ergebnis liefern wie Wie ist das Wetter in Landau?, Gummibärchen einkaufen wie Erinnere mich daran, Gummibärchen zu kaufen und Still dasselbe wie Stopp, Ruhe oder womöglich Schweigefuchs. Sie sehen also, dass Daten un erlässlich sind, um das Verständnis der Intentionen der Benutzer zu verbessern und aktuell zu halten. In Kapitel 6, wenn wir kleine Modelle anlernen, die verschiedene Formulierungen von Befehlen verstehen, und Sie grübelnd vor dem PC sitzen und überlegen, wie ein Benutzer eine bestimmte Funktion aufrufen könnte, würden Sie sich eine Tabelle mit den zwanzig häufigsten Formulierungen wünschen, das verspreche ich Ihnen. Und genau das ist ein Verwendungszweck für die gesammelten Daten.
Natürlich ist das kein Freibrief und auch keine gute Begründung dafür, persönliche Gespräche mitzuschreiben, doch vielleicht zumindest eine Erklärung. Historisch ist der Umstand wohl so zu begründen, dass in den USA erst die Innovation kommt, dann die ethischen und 
datenschutzrechtlichen Bedenken und Regeln. In Deutschland ist es umgekehrt, wir machen erst die Regeln und schränken uns dadurch häufig sehr ein – und erwarten das natürlich auch von den Ländern, die uns ihre Technologie liefern. Ein klassischer Clash of Cultures.

Kapitel Ein Punkt Drei Für wen ist dieses Buch gedacht?
Die Frage ist gar nicht so leicht zu beantworten, denn im Prinzip ist dieses Buch für all diejenigen hilfreich, die einfache Aufgaben in ihrer Umgebung per Stimme automatisieren möchten. Mancher möchte vielleicht durch die Wetteransage geweckt werden, ein anderer 
möchte sein Garagentor per Sprachbefehl öffnen, eine Dritte möchte das WLAN im Auditorium abschalten, weil ihre Vorlesung beginnt.
Falls Sie die Herausforderung etwas generalistischer betrachten, ist es für Sie vielleicht auch nur interessant, den Entwicklungsprozess eines eigenen Sprachassistenten mit mir durchzugehen, die Funktionsweise zu verstehen und zu lernen, wie sich etwas Ähnliches 
in Python umsetzen lässt.
Vielleicht kommen Sie aber auch aus einem kleinen Start-Up und haben eine absolut einzigartige Geschäftsidee, die es erfordert, dass Sie ein kleines IoT-Device (Internet of Things) per Sprache steuern können.
Schließlich hoffe ich, dass dieses Buch auch von dem ein oder anderen zur Unterhaltung gelesen wird, denn theoretische Bücher gibt es genug und zumindest mich inspirieren technisch praktische Werke oft mehr, als solche, die nur Luftschlösser bauen. Mein Bestreben ist, 
besonders die herausfordernden Themen, dazu zählen ich vor allem die KI-Architekturen, so zu erklären, dass sie verständlich sind und – viel wichtiger – klar wird, wozu diese dienen und wie sie uns in unserem ganz konkreten Fall weiterhelfen. In dem Kontext sollen auch die 
Neuerungen des maschinellen Lernens der letzten Jahre angesprochen werden, weswegen wir u. a. auch das Thema Quantum Machine Learning kurz betrachten und ausprobieren – es aber nicht in unseren Assistenten einfließen lassen.
Wichtig ist, dass Sie einige Dinge mitbringen. Sie sollten bereits ein wenig Programmiererfahrung haben, denn ich werde nicht bei Variablen und Schleifen beginnen, sondern die Seiten nutzen, um tiefer einzusteigen. Ich werde für alle Beispiele einen Windows-PC  verwenden, jedoch ist Python dankenswerterweise in hohem Maße betriebssystemunabhängig, sodass der Assistent auch auf macOS, Ubuntu oder CentOS laufen sollte. Sollten Sie planen, das Training der TTS-Engine selber durchzuführen, benötigen Sie weiterhin eine NVIDIA-Grafikkarte mit mindestens 8 GB Speicher. Alternativ können Sie für das Training auch einen Cloud-Dienst nutzen, etwa Google Colab. Dieser stellt GPUs (Graphics Processing Units) oder TPUs (Tensor Processing Units) für das Training neuronaler Netze kostenlos zu Verfügung. Da dieser Teil jedoch optional ist, kann er auch getrost übersprungen und auf andere Frameworks oder meine vortrainierten Modelle zurückgegriffen werden.

Kapitel Eins Punkt Vier Aufbau des Buches
Bisher habe ich meine Bücher immer so aufgebaut, dass ich die darin besprochenen Themen aufbereitet und sequenziell besprochen habe. Das hatte den Vorteil, dass ich pro Thema beliebig in die Tiefe gehen konnte, ohne den Faden zu verlieren – es gab ja auch keinen. Diesmal 
möchte ich es etwas anders machen und mich mit Ihnen dem dann doch sehr technischen Inhalt von der fachlichen Seite nähern, sprich wir entwickeln einen Sprachassistenten und Sie lernen dabei entweder in den Hauptkapiteln oder in ergänzenden Abschnitten vertieft die Techniken kennen, die einfach anmutenden Bibliotheken zugrunde liegen. Damit haben wir ein klares Ziel vor Augen, nämlich den Assistenten fertigzustellen, und lernen aber gleichzeitig noch einiges über neuronale Netze, die maschinelle Verarbeitung von Sprache, Trainingsdaten für KI-Modelle und so weiter
Besagter roter Faden gestaltet sich nun so: In Kapitel 2 lernen Sie, wie man eine solide Entwicklungsumgebung in Python aufsetzt und in einer kurzen Auffrischung das Grundgerüst unserer Applikation erstellt. In Kapitel 3 widmen wir uns der Sprachsynthese, also der Wiedergabe von Sprache über den Computer. Kapitel 4 behandelt die Spracherkennung sowie die Verwendung von Aktivierungswörtern. Ebenso werden Sie verstehen lernen, wie der Fingerabdruck einer Stimme zu erkennen und für die Benutzerauthentifizierung verwendet werden kann. Darauf folgt die Interpretation von Text und das Erkennen der Intention der Sprecher in Kapitel 5. Wir werden hier weiterhin die Unterschiede zwischen regelbasierten und Machine-Learning-Ansätzen ermitteln und schauen, wann welches Verfahren zum Einsatz kommen  sollte. In Kapitel 6 beginnt das Handwerk. Wir haben bereits ein robustes Framework implementiert und entwickeln nun dafür die  entsprechenden Funktionen. 
Da Funktion ein fester Begriff aus der Programmierung ist und ich Missverständnisse ver meiden möchte, nennen wir diese ab hier Intents. In einigen dieser Intents werden wir das Framework unseres Sprachassistenten erweitern, etwa um eine Lautstärkeregelung, die Verwendung von Callbacks in Intents, Dialoge über mehrere Fragen und Antworten hinweg und vieles mehr. In Kapitel 7 implementieren wir eine minimale UI, um von der Konsolenanwendung wegzukommen und Sie lernen, wie Toasts und Benachrichtigungen in Windows zu verwenden. Abschließend möchte ich in Kapitel 8 zeigen, wie eine Python-Anwendung kompiliert und paketiert werden kann, sodass Sie mit einem fertigen Installer zu Ihren Freunden und Kollegen gehen und Ihren eigenen Assistenten präsentieren können. Kapitel 9 beinhaltet dann die obligatorische,  tränenreiche Verabschiedung und der Ausblick auf die Perspektiven, Studien- und Berufsfelder, denen Sie sich nach dieser Lektüre widmen können.

Kapitel Zwei Die Entwicklungsumgebung
Eine Entwicklungsumgebung enthält prinzipiell erst einmal eine IDE (Integrated Developer Environment), die den Entwicklungsprozess durch einen Editor samt Syntaxhervorhebung, Build- und Versionsverwaltungswerkzeuge sowie die Codeanalyse unterstützt. Heutzutage verfügt jede gute IDE weiterhin über Werkzeuge zur Codeoptimierung und -Vervollständigung, sodass den Entwicklern möglichst viel Arbeit abgenommen wird. Dabei werden Typüberprüfungen automatisch durchgeführt, veraltete Pakete gesucht oder potenzielle Ausnahmefehler (Exceptions) aufgezeigt. Kurzum, dem Entwickler wird jegliche Arbeit abgenommen, sodass er sich bestmöglich auf die Anwendungslogik konzentrieren kann.
Zusätzlich machen uns in einigen Programmiersprachen Umgebungs- und Paketmanager das Leben leichter. Sie helfen uns, mehrere Projekte logisch voneinander zu trennen und bieten Funktionen an, um Abhängigkeiten per Mausklick oder Befehl für eine ganz bestimmte 
Laufzeitumgebung zu installieren, sodass wir auch hier nicht selber mühselig auf die Suche nach den richtigen Paketen gehen und diese herunterladen müssen.
Um genau diese beiden Bestandteile wollen wir uns in diesem Kapitel kümmern. Ist das geschehen, schauen wir uns an, wie ein Environment in Python aufgesetzt wird und wie wir die Anwendungsarchitektur des Sprachassistenten gestalten wollen. Zu guter Letzt wiederholen wir kurz ein paar Basics, um ein einfaches Grundgerüst für den Assistenten aufbauen zu können.
Kapitel Zwei Punkt Eins Technologieauswahl
Lassen Sie uns kurz in die Abgründe der Glaubenskriege um die beste Programmiersprache hinabsteigen und ganz rational überlegen, warum wir für unser Projekt Python einsetzen. Schließlich hätten wir auch C#, Go, Kotlin oder Java verwenden können, oder? Wenn man bereit ist, etwas Mehraufwand in Kauf zu nehmen oder die Architektur des Sprachassistenten zum Beispiel auf einen Microservice-basierten Ansatz (siehe Abschnitt 5.5) umstellt, ja. Da wir aber alle analytischen Funktionen in einer einzelnen Anwendung kapseln wollen, führt kaum ein Weg an Python vorbei. Die Sprache hat sich einfach zu sehr im analytischen Bereich etabliert, als dass man sie durch eine beliebige andere ersetzen könnte (ich selber komme aus der Java-Welt und musste mich auch erst mal umstellen). Es geht hier weniger um die Syntax. Vielmehr sind es die Pakete, die andere Entwickler bereitstellen und die wir für die Vielzahl von Aufgaben heranziehen, die wir während der Umsetzung bewältigen müssen. Eine Umfrage von Kaggle aus dem Jahre 2020 zeigt deutlich, dass Python mit fast 35 % am stärksten vertreten ist (siehe Bild Zwei Punkt Eins).

Bild Zwei Punkt Eins Von Data Scientists verwendete Programmiersprachen (Kaggle, 2021)
Sie werden sehen, dass es mit der Entscheidung für Python aber noch nicht getan ist. Im Machine Learning und besonders im Deep Learning entbrennt der nächste Konflikt. Welche Frameworks sollen verwendet werden, Tensorflow, PyTorch, JAX oder fast.ai? Ich habe mich  entschieden, mich hier nicht festzulegen, denn häufig existieren vorgefertigte Modelle und Architekturen für einen bestimmten  Anwendungsfall, die diktieren, welches Framework zu verwenden ist. Und das ist okay, geht es uns doch um die Umsetzung und nicht um eine 
Forschungsarbeit.
Sollten Sie sich in der Situation befinden, dass Sie in Ihrem Unternehmen für die Technologieauswahl für KI-Projekte verantwortlich sind, möchte ich Ihnen abermals den jährlichen Kaggle Survey ans Herz legen. Kaggle, als eine Plattform für Data Scientists unter der Schirmherrschaft von Google, fragt dort regelmäßig ab, wer sich zu einem KI-Spezialisten entwickelt, wie die Altersverteilung ist, wo man sich entsprechend weiterbilden kann und eben auch, welche Tools zum Einsatz kommen. Dort ist wunderbar zu erkennen, dass es keine teuren 
Werkzeuge von IBM und co. braucht, sondern dass weit über die Hälfte der Teilnehmer mit ganz normalen Open-Source-Frameworks arbeiten. Dem zum Trotz arbeiten jedoch wiederum etwa 80 % auf einer Cloud-Plattform der drei Anbieter Amazon, Google und Azure. Es zeigt sich also, dass Machine-Learning-Projekte nicht mit Zaubertools umzusetzen sind, sondern durch die Arbeit von speziell ausgebildetem Personal. Behalten Sie das bitte im Hinterkopf, sollte Ihre Chefin mal danach fragen.

Kapitel Zwei Punkt Zwei Der richtige Editor
Ich habe mal an einem Austausch mit der Anglia Ruskin University in Cambridge teilgenommen, was mir einerseits gezeigt hat, dass Cambridge eine wunderschöne, interessante Stadt ist und andererseits, wie unterschiedlich Programmiervorlesungen aussehen können. Von meiner Hochschule in Mannheim war ich es gewohnt, Programmieraufgaben mit nach Hause zu bekommen und in Ruhe lösen zu können. Dort war es etwas anders. Wir wurden in Zweiergruppen vor ein leeres Notepad gesetzt, das weder über Codeanalysefunktionen noch über Autovervollständigung verfügte, und sollten eine Anwendung in Java schreiben. Der Dozent ging herum und gab Hinweise, wenn wir nicht weiterkamen. Ich war aufgeschmissen.
Wenn ich aber heute eine Java-Vorlesung halten würde, würde ich es genauso machen, denn so lernt man selbstständig zu programmieren. Natürlich braucht man für gewisse Arbeiten einen Debugger, der es erlaubt, Anwendungen zu unterbrechen und den Zustand von Variablen zu prüfen, doch für ein Hello World, einen Taschenrechner oder einen Kalender reicht tatsächlich ein einfacher Editor und der entsprechende Compiler.
So schwer soll es Ihnen aber nicht gemacht werden. Die Editoren für Python bieten mittlerweile ein Set all der Tools, die uns damals verwehrt blieben. Zu den am häufigsten verwendeten gehören sicher Microsofts Visual Studio Code (VS Code), PyCharm oder Spyder (wobei ich in meiner Abteilung feststelle, dass die Tendenz ganz eindeutig zu VS Code geht). Suchen Sie sich gerne einen aus. Ich werde jedoch in Notepad++ arbeiten und immer ein Command Line-Fenster offen haben, da diese Fenster genau die Informationen und Befehle zeigen, die Sie zum Weiterkommen benötigen.
Die Features, auf die Sie bei der Auswahl achten sollten, liste ich im Folgenden auf. Zwar sind nicht alle wirklich notwendig, sie erleichtern aber die Arbeit ungemein.
Erstens Ein Codeeditor mit Syntaxhervorhebung und Autovervollständigung
Zweitens Projektorganisation, um alle dem Projekt zugehörigen Dateien in der IDE zu verwalten
Drittens Refactoring Tools, um etwa Variablen- oder Klassennamen und -Eigenschaften projektweit zu ändern
Viertens Ein Debugger, der in der Lage ist, die Anwendung an bestimmten Punkten zu unterbrechen und den Einblick in Variableninhalte gewährt
Fünftens Eine Test-Suite, die das Schreiben und Ausführen von Tests ermöglicht
Sechstens Datenbanktools, um im Editor den Inhalt bestimmter applikationsabhängiger Datenbanken zu prüfen und zu ändern
Siebtens Visualisierungswerkzeuge, um den Inhalt bestimmter Variablen (Listen, Matrizen) grafisch darzustellen
In der Regel ist die Empfehlung jedoch, dort zu arbeiten, wo Sie sich am wohlsten fühlen. Wenn Sie keine Anhaltspunkte haben, für welchen Editor Sie sich entscheiden sollten, würde ich Stand 2022 VS Code empfehlen.

HINWEIS: Das Jupyter Notebook sticht in mehrerlei Hinsicht aus dem Pulk an IDEs heraus, denn es ist einerseits die laut Kaggle am häufigsten verwendete Umgebung für Datenexploration und generell Data-Science-Projekte und andererseits ist die IDE webbasiert, sprich Sie arbeiten in Ihrem Browser damit. Das hat den Vorteil, dass sich besagte Notebooks leicht ins Internet hochladen und mit anderen teilen lassen.
Ein Jupyter Notebook besteht aus einzelnen Ausführungsschritten, die einen fertigen Text und optional Grafiken rendern können, sodass jeder Schritt in einer Art Markup präsentiert werden kann (siehe Bild Zwei Punkt Zwei). Dadurch sind Jupyter Notebooks hervorragend geeignet, um zu unterrichten und Wissen weiterzugeben.
Bild Zwei Punkt Zwei Jupyter Notebook eignet sich sehr gut für Datenexploration und Wissensvermittlung.
Da wir aber eher Anwendungsentwicklung betreiben, lohnt sich der Einsatz von Jupyter Notebook maximal für experimentelle Zwecke, also zum Beispiel um zu testen, wie bestimmte Bibliotheken performen oder ob ein komplexes Parsing von Intents das Ergebnis liefert, das wir erwarten. Da wir aber ansonsten das Ziel haben, eine Anwendung zu erstellen, die in Form eines Executable selbstständig und mit möglichst geringem Fußabdruck auf den PCs unserer Kunden oder Freunde läuft, werden wir den Großteil der Arbeit in einem konventionellen Editor erledigen.

Kapitel Zwei Punkt Drei Trennen verschiedener Projekte
Ein Python-Projekt besteht in der Regel aus mindestens einer Quelltextdatei, einer Laufzeitum gebung und meistens auch mehreren  Bibliotheken, die wir hinzuziehen, da diese Funktionen beinhalten, die wir sonst mühselig selber schreiben und testen müssten. Diese Bibliotheken installieren wir nicht etwa in einen Projektordner, sodass sie nur für ein einziges Projekt vorliegen, sondern eben in diese Laufzeitumgebung. Nun besteht das Problem, dass PCs in der Regel mit einer Python-Version ausgestattet werden und wenn wir nun mehrere Projekte parallel entwickeln (was recht häufig der Fall sein kann), dann kann es dazu kommen, dass es Überschneidungen zwischen den verschiedenen Bibliotheken und deren Abhängigkeiten gibt (siehe Bild Zwei Punkt Drei links).
Bild Zwei Punkt Drei Die Trennung verschiedener Umgebungen ist bei der Parallelentwicklung mehrerer Projekte unerlässlich, da es sonst zu Interferenzen zwischen den Abhängigkeiten kommen kann.
Das kann sich dann dadurch äußern, dass beispielsweise Bibliothek X von NumPy 1.21.1 und Bibliothek Y von NumPy 1.19.5 abhängt, und schon würde bei jeder Installation von NumPy die vorherige Installation überschrieben werden und die Funktionalität unserer Anwendungen gefährdet sein.
Nun wäre Python nicht so beliebt, wenn es dafür nicht eine einfache Lösungsmöglichkeit gäbe. Diese besteht darin, eine virtuelle Umgebung anzulegen, was wiederum auf mehre ren Wegen geschehen kann. Virtualenv war lange Jahre der Standard, der über den Befehl virtualenv meine_umgebung die Anlage eines eigenen, abgeschotteten Bereichs in der lokal installierten Laufzeitumgebung erlaubte. Über meine_umgebung\Scripts\activate konnte diese aktiviert und verwendet werden, sodass alle Bibliotheken, die von nun an installiert wurden, in dieser virtuellen Umgebung landeten. Anmerkung: Das gilt nur für Windows-Systeme, auf Linux-Systemen würde man source meine_umgebung\Scripts\activate verwenden
Das Problem, das hierbei bestand, war, dass lediglich eine einzige Version der Laufzeitumge bung unterstützt wurde. Wollte man beispielsweise für Python 2.7 und Python 3.8 entwickeln, 
musste man beide Umgebungen installieren und dann jeweils auf einer der beiden seine 
virtuelle Umgebung anlegen. Neben virtualenv entstand das Projekt pipenv, das einige neue 
Features mit sich brachte, etwa die Unterscheidung verschiedener Stadien eines Projekts 
(in der Regel Entwicklung, Test, Integration, Produktion), jedoch nicht das grundsätzliche 
Problem der verschiedenen Python-Versionen löste.
Irgendwann trat Anaconda auf den Plan. Anaconda erlaubt es genauso wie virtualenv, vir tuelle Umgebungen anzulegen, jedoch mit einigen Besonderheiten. Die Anlage einer neuen Umgebung über conda create -n meine_umgebung erlaubt über einen weiteren Parameter die  Spezifikation der Python-Version, die man für genau dieses Projekt nutzen möchte, beispielsweise conda create -n meine_umgebung python=3.7. Es werden also beim Einrichten nicht nur die benötigten Abhängigkeiten installiert, sondern auch die Runtime, die wir als Entwickler benötigen. Das Ergebnis sehen Sie in Bild Zwei Punkt Vier
Bild Zwei Punkt Vier Anlage eines neuen „Virtual Environments“ über Anaconda und Abfrage der Installation der neu hinzuzufügenden Pakete
Anaconda listet dabei explizit auf, welche Bibliotheken es in der Umgebung installiert. Diese sind oft betriebssystemabhängig und sorgen erst einmal für die Funktionsfähigkeit der Umgebung. So beinhaltet ca-certificates etwa notwendige SSL-Zertifikate, um auf verschiedene 
Datenquellen zuzugreifen, pip dient der Installation von Paketen oder wheel erlaubt das Arbeiten mit sogenannten Wheel-Dateien  (Installationspaketen für Python-Bibliotheken). Bestätigen Sie die Frage aus Bild Zwei Punkt Vier, wird Anaconda die benötigten Pakete selbstständig herunterladen oder aus dem lokalen Cache installieren, falls diese schon in aktueller Version vorliegen. Dabei wird die Python-Version in Betracht gezogen, die Sie gewählt haben, denn viele Bibliotheken werden für eine ganz bestimmte Version  zusammengestellt. 
Danach können Sie die Umgebung per Befehl, den Ihnen Anaconda ausgibt, aktivieren. Dazu kommen wir aber später noch. Sie sehen, dass uns hier die Arbeit abgenommen wird, die uns davon abhalten würde, sofort mit der Entwicklung zu starten.
Nun kommt noch ein Clou von Anaconda: Wir können nicht nur Python-Pakete installieren, sondern auch davon losgelöst andere Binaries, DLLs etc. Später werden wir sehen, wann genau das sinnvoll ist und wie uns damit eine Menge Arbeit abgenommen wird, wenn wir FFmpeg für unseren Streaming-Intent einrichten müssen.
TIPP: Sollten Sie in einer Umgebung arbeiten, die Sie durch verfügbaren Speicherplatz einschränkt, und nur die Grundfunktionalität von Anaconda benötigen, können Sie auf Miniconda ausweichen. Miniconda ist Anaconda sehr ähnlich, bringt aber nur die nötigsten Abhängigkeiten mit der Installation mit, sodass der Installer nur etwa 60 MB groß ist, während Anaconda auf etwa 480 MB kommt. Der häufigste Use Case für Miniconda ist das Einrichten von Umgebungen auf einem Raspberry Pi.

Kapitel Zwei Punkt Vier Versionsverwaltung über Git
Allem voran: Das ist kein Buch über Git. Da ich jedoch den Quelltext dieses Buches auf Github hoste, möchte ich Ihnen eine schnelle Einführung in die wichtigsten Befehle geben. Git als Versionskontrollsystem hat in den letzten Jahren so sehr an Popularität gewonnen, dass es fast für alle großen Projekte verwendet wird und SVN (Subversion) beinahe komplett vom Markt verdrängt hat.
HINWEIS: Github wurde 2018 für 7,5 Milliarden Dollar von Microsoft gekauft und nicht nur weiter kostenlos betrieben, sondern auch um viele Funktionen er weitert. Dennoch gibt es Projekte, die sich von Github fernhalten. Das vielleicht bekannteste ist Blender, ein Werkzeug zur Erstellung von 3D-Modellen, Filmen und Spielen. Die Begründung von Ton Roosendaal, dem Vorsitzenden der Blender Foundation, ist, dass so viel Risikokapital (ca. 350 Millionen Dollar) in das Produkt geflossen ist, dass die Investoren irgendwann Profit zurückbekommen wollen, und er geht davon aus, dass die Benutzer diejenigen sind, die diese Beträge zahlen werden (Roosendaal, 2018).
Also, was ist Git? Tatsächlich handelt es sich nur um eine Sammlung von Tools für die Verfolgung von Änderungen an Dateien, sei es Text, Quellcode oder auch Binärdateien. Dokumente lassen sich vergleichen, zusammenführen, aus einem und in ein Repository laden. Ein Repository ist ein Ordner auf Ihrer Festplatte, der alle Daten eines Projekts beinhaltet, die Sie nachverfolgen möchten. Dabei arbeitet Git dezentral, sprich es wird kein Server be nötigt, der die primäre, einzig wahre Version eines Projekts vorhält. Natürlich macht es aber Sinn, das zu tun, besonders wenn man, wie in meinem Fall, ein Projekt mit vielen Menschen teilen möchte.
Sollten Sie Git noch nicht auf Ihrem PC installiert haben, laden Sie es von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrich gitforwindows Punkt O.R.G. herunter und installieren Sie es. Auf Ubuntu erfolgt die Installation über apt install git, auf CentOS analog mit yum install git. Ein neues Projekt anlegen oder ein bestehendes klonen. Wenn Sie ein neues Projekt beginnen möchten, haben Sie im Prinzip zwei  Möglichkeiten:
Erstens Sie legen ein neues Projekt mit git init an,
oder Zweitens Sie klonen ein bestehendes Repository mit git clone.
Da ich den Code für den Sprachassistenten bereitstelle, beginnen wir mit Letzterem. Öffnen Sie eine Kommandozeile (Windows Button und C.M.D. tippen, alternativ Steuerung plus der R.-Taste und C.M.D. eingeben) und führen Sie folgenden Befehl aus.
git clone Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrich github Punkt com Schrägstrich padmalcom Schrägstrich AISpeechAssistant
TIPP: Besser als die Kommandozeile ist die Git Bash, die Sie ebenfalls über das Windows-Startmenü finden oder im Windows-Explorer per Rechtsklick in einen Ordner aufrufen können.
In dem Ordner, in dem Sie sich in der Kommandozeile befunden haben, wird nun ein Ordner AISpeechAssistant angelegt, in den Git alle Dateien aus eben diesem Repository herunterlädt. Wenn Sie wissen möchten, welche das sind, öffnen Sie den Link hinter git clone einfach 
in Ihrem Browser.
Auch wenn wir git init nicht benötigen, möchte ich den Befehl der Vollständigkeit halber noch erklären. Möchten Sie ein neues Repository für ein ganz neues Projekt anlegen (viel leicht möchten Sie ja ohne die Hilfe meines Repositorys arbeiten und den Assistenten von 
Grund auf neu schreiben, was ich sehr begrüßen würde), dann erstellen Sie einen neuen Ordner über M.K.dir neues_repository, gehen mit C.D. neues_repository in diesen Ordner und führen dort git init aus.
Nun können Sie in diesem Ordner neue Dateien, zum Beispiel eine main Punkt Pei, anlegen, die sie später dem Repository hinzufügen.
Sollten Sie ein Projekt mit git init angelegt haben, ist es empfehlenswert, dieses mit einem Remote Repository zu verbinden, um mit Ihrem Team gemeinsam daran arbeiten oder von mehreren PCs darauf zugreifen zu können. Dazu müssen Sie zuerst auf einem Git-Server Ihrer Wahl (Github, Gitlab) ein Remote Repository anlegen, indem Sie sich dort einen Account erstellen und den Anlageprozess im Browser durchlaufen. Nehmen wir an, Sie hätten das als Benutzer K.I.-Entwickler auf Github getan und ein Repository namens sprachassistent angelegt, dann würden Sie nun in der Kommandozeile folgenden Befehl absetzen, um die Remoteverbindung zwischen Ihrem lokalen Repository und dem auf Github herzustellen.
git remote add origin Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrich github Punkt com Schrägstrich Ka-Ieh Bindestrich Entwickler Schrägstrich Sprachassistent Punkt git
Es hat sich eingebürgert, dass das primäre Remote Repository den Namen origin bekommt, Sie können hier aber auch frei wählen.
HINWEIS: Wenn Sie ein git clone ausgeführt haben, wird das Repository automatisch gesetzt.
Prüfen des Repository-Status
Führen Sie in ihrem Repository nun git status aus, gibt der Befehl zurück, ob es Dateien gibt, die nicht nachverfolgt werden oder geändert wurden. Haben Sie eben im Zuge von git init die main Punkt Pei angelegt, werden Sie nun darauf hingewiesen, dass diese noch nicht versioniert wurde.
Hinzufügen neuer Dateien in ein Repository
Wenn Sie einem Repository neue Dateien hinzufügen möchten, dann können Sie das über git add tun. Indem Sie beispielsweise git add main Punkt Pei ausführen, wird main Punkt Pei dem Repository hinzugefügt und ab sofort von der Nachverfolgung mit einbezogen. Wenn Sie viele Dateien auf einmal hinzufügen möchten, verwenden Sie git add Bindestrich Ah, wobei Bindestrich Ah für all steht.
Erstellen eines Commits
Ein Commit ist eine Art Repository-Schnappschuss zu einer ganz bestimmten Zeit, etwa wenn wir unsere Datei main Punkt Pei hinzugefügt haben. Achten Sie darauf, dass ein Commit nur möglich ist, wenn es Änderungen an Dateien am Projekt gegeben hat (Änderungen an Ordnern sind 
für Git uninteressant, solange diese keine Dateien beinhalten). Einen Commit erstellen Sie mit git commit. Dieser Aufruf wird einen Editor (in der Regel in der Kommandozeile) öffnen, in dem Sie einen Kommentar zu Ihrem Commit eintragen müssen, um die Änderungen zu beschreiben. Alternativ können Sie die Nachricht auch direkt im Befehl über git commit Bindestrich-M. „main Punkt py“ angelegt angeben.
Änderungen hochladen
Haben Sie nun lokal einige Änderungen (also einen oder mehrere Commits) gemacht, können Sie diese über git push in das Remote Repository hochladen. In der Regel reichen genau diese zwei Wörter, Sie haben allerdings noch Kontrolle über den Namen des Remote Repositorys und den Branch, auf den Sie pushen möchten (zu Branches kommen wir gleich).
Voll ausgeschrieben könnte der Befehl git push origin main lauten, wenn wir das Remote Repository origin auswählen und auf den Branch main pushen.
HINWEIS: Github hat den Branch master kürzlich aus Political-Correctness-Grün den in main umbenannt, was längst überfällig war, da Master/Slave-Beziehungen in anderen Feldern der IT aus dem Sprachgebrauch verbannt wurden.
Ausschluss von Dateien und Ordnern über Punkt gitignore
Nicht immer ist es sinnvoll, alle Dateien in ein Repository hochzuladen. Besonders Binaries und temporäre Dateien sollten auf dem Rechner des jeweiligen Entwicklers verbleiben und erst bei Bedarf wieder generiert werden. Um bestimmte Dateitypen und Ordnerstrukturen von der Nachverfolgung auszuschließen, können Sie eine Datei anlegen, die Punkt gitignore heißt und im Root-Verzeichnis des Repositorys liegt.
In einem Python-Projekt sollten beispielsweise folgende Dateien aus einem Repository aus geschlossen werden:
Unterstrich Unterstrich peikäsch Unterstrich Unterstrich Schrägstrich – zu Bytecode kompilierter Python-Code im Ordner Unterstrich Unterstrich peikäsch Unterstrich Unterstrich
log Punkt Text. – Log-Dateien
Punkt speiderprodschekt – Projektdateien für den Editor Speider
Die Liste ist natürlich nicht vollständig, sondern nur ein Tropfen auf den heißen Stein. Schauen Sie in das Beispielprojekt, dort finden Sie eine recht umfassende Liste, die teilweise von Github generiert werden kann, wenn man dort ein Repository anlegt. Ich habe selber noch 
einige Einträge am Ende hinzugefügt, die Sie später kennenlernen werden.
Änderungen herunterladen
Es kann vorkommen, dass ich ab und an noch Fehler im Code beseitige und Änderungen in das offizielle Repository pushe. Führen Sie also gerne regelmäßig git pull oder git pull origin main aus, um den neusten Stand des Codes vom Remote Repository zu bekommen.
Einen Branch erstellen
Nun kann es vorkommen, dass Sie mitten in Ihrem Entwicklungsprozess einen komplett neuen Weg einschlagen möchten, beispielsweise weil Sie eine tolle neue Bibliothek gefunden haben, die Ihnen das Verständnis der Benutzerwünsche erheblich vereinfacht. Diese neue 
Bibliothek einzubinden bringt aber nun viele Veränderungen im gesamten Programm mit sich, sodass Sie sich Sorgen machen, ob Sie alles im Falle eines Scheiterns wieder zurück gedreht bekommen.
Hier kommen Branches ins Spiel. Aus dem main-Branch kann ein git branch intent Unterstrich parsing ausgeführt werden, um den Branch intent Unterstrich parsing zu erstellen. Dieser ähnelt in seinem Zustand dem Branch main, kann aber komplett losgelöst davon bearbeitet werden.
In einen Branch wechseln
Möchten Sie nun in den Branch intent Unterstrich parsing wechseln, führen Sie ein git checkout intent Unterstrich parsing aus. Sollten Sie die Git Bash verwenden, werden Sie sehen, dass sich der Name des aktiven Branches von main auf intent Unterstrich parsing ändert.
Zwei Branches zusammenführen
Prima, Sie hatten Erfolg bei der Implementierung der neuen Funktion und möchten nun das Ergebnis aus dem Branch intent Unterstrich parsing in den Branch main übertragen. Hier kommt das Feature merge ins Spiel. Commiten Sie Ihre Arbeit im Branch intent Unterstrich parsing und wechseln Sie zurück in den Branch main mit git checkout main. Sie werden feststellen, dass Sie ihre Änderungen aus intent Unterstrich parsing nicht mehr sehen. Diese sind aber noch da, nur eben nicht in main. Führen Sie ein git merge intent Unterstrich parsing aus. Alle Neuerungen werden nun in main übernommen.
HINWEIS: Leider geht ein Merge fast nie so leicht über die Bühne. Häufig kommt es zu sogenannten Merge-Konflikten, da es ja sein kann, dass in verschiedenen Branches an verschiedenen Dateien gearbeitet wurde und Git nicht immer weiß, welche Änderungen denn jetzt bestehen bleiben sollen. Zwar ist das Werkzeug in der Lage, viele Konflikte selber zu lösen, aber eben nicht alle. Dann sind die Entwickler gefragt, und sie sitzen häufig zu dritt an einem Bildschirm und gehen Konflikt für Konflikt durch den Code, um alle Konflikte nach und nach zu lösen. Der richtige Spaß folgt dann bei Konfigurationsdateien für Tools, die ein Programm generiert
Haben Sie die Branches erfolgreich zusammengeführt, können Sie intent Unterstrich parsing mit git branch Bindestrich-D. Abstand intent Unterstrich parsing löschen.
Commits rückgängig machen
Manchmal läuft die Entwicklung eines Features nicht wie gedacht und wir entscheiden uns, dass es besser ist, einen Schritt zurückzugehen und einen ganzen oder teilweisen Commit rückgängig zu machen. Zuerst können wir uns eine Commit-Historie über git log ausgeben 
lassen. Diese zeigt dann untereinander, welche Commits gemacht und wie diese kommentiert wurden, und vor allem, welche ID diese Commits hatten, denn jeder bekommt eine eindeutige Identifikationsnummer.
Mit dieser recht langen Identifikationsnummer können wir nun über git show 1234 alle Änderungen in Commit 1234 anzeigen lassen. Nehmen wir mal an, wir hätten an der main Punkt Pei gearbeitet und möchten diese nun zurückdrehen, dann würden wir das mit git checkout 
1234 main Punkt Pei erreichen.
Einen ganzen Commit rückgängig machen (also nicht nur eine einzelne Datei) ist ebenfalls möglich, nämlich über den Befehl git revert HEAD. HEAD referenziert den letzten Commit und kann durch eine beliebige Commit ID ersetzt werden.
TIPP: Haben Sie Ihre Änderungen noch nicht gepusht und möchten nur den Commit aus der lokalen Historie bekommen, genügt der Befehl git commit Bindetrich amend.

Kapitel Zwei Punt Fünf Die Komponenten unserer Anwendung
In diesem Abschnitt möchte ich Ihnen grob vorstellen, wie der Aufbau eines Sprachassistenten aussehen kann. Beginnen wir mit dem Block-Sprachassistent in der Mitte von Bild Zwei Punkt Fünf. Eine zentrale Aufgabe ist hier das Entgegennehmen von Sprachbefehlen, wozu wir einmal eine Audioeingabe sowie eine Funktion für die Umwandlung von Sprache in Text (Text-To-Speech) implementieren werden. Zuvor muss jedoch ein Aktivierungswort (Wake Word) genannt worden sein, vorher wird keine Übersetzung von Sprache in Text stattfinden. Ist das geschehen, wird der gesprochene Befehl von der Dialog Engine interpretiert und an das Intent Processing weitergeleitet. Zuvor wollen wir jedoch noch einiges tun, um die Verwendung unseres Sprachassistenten sicherer zu machen. Es soll versucht werden, anhand der Stimme 
per Speaker Identification zu erkennen, wer dort gesprochen hat, und per Abfrage an das User Management, ob diese Person überhaupt die Berechtigung hat, den Befehl auszuführen.
Bild Zwei Punkt Fünf Der Aufbau der Anwendung gliedert sich in den Sprachassistenten, eine Datenbank zur Verwaltung von Benutzerdaten und eine Ansammlung verschiedener Intents.
Zuvor muss das Intent Processing jedoch zurückmelden, welcher Befehl für die Anfrage überhaupt in Frage kommt (was uns später noch in die Tiefen eines der Frameworks eintauchen lassen wird). Ist diese Autorisierung erfolgt, wird der passende Intent aufgerufen oder zu rückgemeldet, dass der Intent nicht aufgerufen werden darf, oder dass kein passender Intent gefunden wurde. Nach einem erfolgreichen Aufruf erhält der Intent einige globale Konfigurationsparameter aus dem Configuration Management und führt seine Funktion aus. Eine 
solche Konfiguration kann etwa die Sprache sein, die wir für den Assistenten gewählt haben.
HINWEIS: Bild Zwei Punkt Fünf zeigt, dass der gesamte Aufbau unserer Anwendung auf einer einzigen Python-Umgebung läuft. Wir werden uns später in Abschnitt Fünf Punkt Fünf noch einen Microservice-Ansatz anschauen, der die Intents als eigenständige Anwendungen zu betreiben erlaubt.
Zu guter Letzt wird die Rückgabe des Intents, sollte er denn eine haben, über eine Text-To Speech-Engine zurück in Sprache umgewandelt und durch die Lautsprecher ausgegeben. Moment, kann es denn Intents geben, die keine Rückgabe haben? Klar, die bekanntesten Vertreter sind entweder Unterbrechungsbefehle wie Stop oder Lautstärkeänderungen. Auch das Abspielen von Streams oder Musik hat nicht immer eine anschließende Sprachausgabe zur Folge.

Kapitel Zwei Punkt Sechs Erstellen eines Klassengrundgerüsts
Wir werden nun ein Grundgerüst erstellen, das als minimale Basis für unsere Anwendung dient. Dabei möchte ich einige Basics wiederholen, die dem einen oder anderen zu trivial erscheinen mögen. Tippen Sie in dem Fall einfach den Code ab und erfreuen Sie sich an den gewonnenen zwanzig Minuten.
Installation von Anaconda
Wir werden Anaconda verwenden, um unsere virtuelle Umgebung zu verwalten. Dazu müssen wir es zunächst installieren. Laden Sie sich die aktuelle Version der 64-Bit Individual Edition von www.anaconda.com herunter und installieren Sie diese. Zum aktuellen Zeitpunkt ist das 
Version Zweitausendeinundzwanzig Punkt Null-Fünf. Nach der Installation sollten Sie in Ihrem Startmenü den Eintrag Anaconda-Drei (64-Bit) vorfinden und darunter den Anaconda Prompt. Mit diesem werden wir viel Zeit verbringen, ein Shortcut lohnt sich also. Öffnen Sie diesen und gehen Sie in ein Verzeichnis, das Sie für die weitere Entwicklung Ihres Sprachassistenten verwenden möchten. Ich habe zwar im Beispiel-Repository für jedes Beispiel einen Unterordner angelegt, das müssen Sie aber nicht tun. Ein Ordner Sprachassistent oder eine entsprechende Abkürzung in Ihrem Benutzerverzeichnis reicht völlig.
Im Anaconda Prompt werden Sie sehen, dass Ihrem Pfad ein Umgebungsname in Klammern vorangestellt ist. Die Umgebung (base) ist die  Basisumgebung, in der wir nicht arbeiten sollten. Legen Sie also Ihr erstes Environment bzw. Ihre erste Umgebung an:
conda create Bindestrich-N. Null-Null-Eins Unterstrich Hello Unterstrich Wörld peiton Ist gleich Drei Punkt Acht
Bestätigen Sie die Nachfrage, ob die benötigten Pakete heruntergeladen werden dürfen, mit Y. Sie werden merken, dass Anaconda, wie angekündigt, keinen Ordner anlegt. Schauen Sie aber in Ihr Benutzerverzeichnis und dann in den Ordner Anaconda Drei Rückstrich envs, dann werden Sie einen Ordner Null-Null-Eins Unterstrich Hello Unterstrich Wörld vorfinden, der alle Dateien beherbergt, die Sie ab der Aktivierung in Ihre Umgebung installieren.
Aktivierung ist ein gutes Stichwort, denn das werden wir zunächst tun.
conda activate Null-Null-Eins Unterstrich Hello Unterstrich Wörld



22 2 Die Entwicklungsumgebung
Nun sollte sich der Umgebungsname von (base) in (001_hello_world) ändern, was zeigt, dass 
der Wechsel geklappt hat. Um wieder in die Umgebung (base) zurückzukehren, tippen Sie 
einfach conda deactivate. Das soll aber hier erst einmal nicht wieder passieren.
Legen Sie nun noch einen Arbeitsordner mit Namen 001_hello_world im Verzeichnis Sprach assistent an und wechseln Sie mit cd 001_hello_world dort hinein.
Einstieg in die Entwicklung
Endlich ist es so weit, wir beginnen mit der Entwicklung. Öffnen Sie die IDE oder den Editor 
Ihrer Wahl. Ich möchte, so weit es geht, objektorientiert arbeiten. Das heißt, dass wir unse ren Code in logisch unterteilte Klassen gliedern und mit Klassenfunktionen und -variablen 
versehen. Wir beginnen damit, eine Klasse VoiceAssistant zu erstellen.
TIPP: Manchmal steht hinter einem Klassennamen eine Klammer (), manchmal 
nicht. Tatsächlich benötigt man diese Klammern, wenn die Klasse von einer ande ren Klasse erben soll, sprich auf deren Struktur und Funktionsweise aufbauen soll. 
Leere Klammern waren in Python 2.x noch notwendig (ebenso wie die Angabe, 
dass eine Klasse immer von object erben sollte), in Python 3.x kann man die Klam mern aber weglassen, wenn die Klasse nicht von einer anderen erbt.
Im Folgenden erstellen wir eine Methode run(), die die Logik unseres Sprachassistenten 
ausführen soll (siehe Listing 2.1). Diese wird uns über das ganze Buch hinweg begleiten. Die 
Definition einer Methode wird mit def eingeleitet und kann beliebig viele Parameter enthalten. 
Da es sich um eine Klassenmethode handeln soll, versehen wir sie mit dem Parameter self, 
der die Instanz der Mutterklasse referenziert.
HINWEIS: Der Begriff Instanz ist bekannt? Eine Klasse ist eine Art Schablone, wie 
der Bauplan eines Autos. Die Instanz einer Klasse ist dann das Auto, das bei Ihnen 
auf dem Hof steht, also etwas, das ganz genau dem Bauplan entspricht und die 
Funktionen ausführen kann, die im Plan genau beschrieben sind.
Die Definition der Methode run() muss eingerückt werden – wie, bestimmen Sie! Entweder 
per Tab oder mit einigen Leerzeichen. Wichtig ist nur, dass Sie sich konsequent an die Ein rückung halten, denn diesbezüglich ist der Compiler sehr streng. Geschweifte Klammern wie 
in Java oder C# gibt es in Python nicht. Das PEP 8 (Python Enhancement Proposal) empfiehlt 
jedoch die Verwendung von vier Leerzeichen.
Listing 2.1 Definition einer Klasse für den Sprachassistenten
1. class VoiceAssistant:
2. def run(self):
3. print("Los geht’s!")
4.
5. if __name__ == "__main__":
6. va = VoiceAssistant()
7. va.run()



2.6 Erstellen eines Klassengrundgerüsts 23
In run() fügen wir nun, ebenfalls eingerückt, einen print()-Befehl hinzu, der den Text Los 
geht’s! ausgibt. Denn irgendetwas muss unsere Methode ja tun, leere Methodenrümpfe sind 
in Python nämlich nicht erlaubt. Sollten Sie einmal leere Methoden schreiben müssen, ver sehen Sie diese mit einem pass, damit der Compiler sich nicht beschwert.
Nun fügen wir (nicht mehr eingerückt) die Zeilen 5–7 aus Listing 2.1 hinzu. Dieses Konstrukt 
sagt aus, dass, wenn diese Datei (also die main Punkt Pei) die Hauptdatei ist, die über Python auf gerufen wurde, der Code darunter nicht ausgeführt werden soll. Falls Sie sich mit anderen 
Sprache auskennen: Das Konstrukt entspricht in Java etwa dem public static void 
main(String[] args). In diesem If-Zweig erstellen wir nun eine Instanz der Klasse Voice Assistant und weisen diese va zu. Darauf rufen wir die Methode run() auf.
Wenn Sie die Datei nun als main Punkt Pei in Ihrem Arbeitsordner 001_hello_world speichern, kön nen Sie diese im Anaconda Prompt mit python main Punkt Pei ausführen. Sie werden feststellen, 
dass Ihr Programm Los geht’s! ausgibt.
HINWEIS: Okay, das hätten wir auch einfacher haben können, indem wir einfach 
die Zeile print(„Los geht’s!“) in die main Punkt Pei tippen und es ausführen. Denken 
Sie aber daran, dass wir objektorientiert arbeiten wollen, um später mehr Struktur 
im Programm zu haben, den diese werden wir bei der Komplexität brauchen.
Professionelles Logging
Logging über print ist etwas umständlich, weswegen wir eine professionelle Bibliothek ver wenden möchten. Diese ermöglicht es uns, im Unterschied zu print automatisch Zeitstempel 
an jede Meldung anzuhängen, ebenso wie einen Log-Level (zum Beispiel Debug, Info, Warning, Error), 
um den Schweregrad einer auftretenden Logmeldung als Entwickler besser einschätzen zu 
können. Weiterhin gibt ein gutes Logging-Framework an, wo genau die Meldung aufgerufen 
wurde, also in welcher Zeile, in welcher Methode und in welcher Datei. Das macht es uns 
tatsächlich sehr einfach, Fehler zu finden und zu beheben.
Ich habe mich für das Paket loguru entschieden, da es einfach zu verwenden und gut zu kon figurieren ist. Zwar gibt es auch noch das Standard-Logging von Python, was ebenfalls mehr 
Funktionen als print bietet, jedoch ist dieses aus meiner Erfahrung immer etwas umständ lich einzurichten, zumindest, wenn man für verschiedene Klassen verschiedene Log-Level 
verwenden möchte. Lassen Sie uns das mal einrichten, indem wir in den Anaconda Prompt
wechseln und den Befehl pip install loguru eingeben. Damit haben wir unsere erste 
Abhängigkeit in unsere Umgebung installiert und können diese nun einbinden. Fügen Sie 
nun loguru als Import ein, indem Sie in die erste Zeile der main Punkt Pei Folgendes hinzufügen:
from loguru import logger
Damit ist logger aus dem Modul loguru in unserer Datei bekannt und kann eingesetzt werden. 
Das wollen wir auch gleich tun, indem wir print(„Los geht’s!“) durch logger.info(„Los 
geht’s!“) ersetzen.
Führen Sie nun die Datei erneut aus. Statt der einfachen Textausgabe sollten Sie im Prompt 
sehen, dass die Logmeldung in der Form Zeitstempel, Log-Level, Ausführungspunkt, Nach richt formatiert wird (siehe Bild 2.6).



24 2 Die Entwicklungsumgebung
Bild 2.6 Die Logmeldung wird mithilfe von loguru so formatiert, dass wir mehr Informationen über 
deren Herkunft und Zweck erhalten.
Sollten Sie die Meldung ModuleNotFoundError: No module named ‚loguru‘ erhalten, haben Sie 
entweder vergessen, das Import-Statement aufzunehmen oder loguru über pip zu installieren.
TIPP: Sie sehen, dass wir unseren Code weitestgehend in Englisch halten. Das hat 
einerseits den Zweck, dass wir englische Befehls- und Klassennamen nicht mit 
deutschen Namen mischen und so ein Kauderwelsch erzeugen würden, sondern 
auch den, dass wir leichter in internationalen Communities wie Stack Overflow
nach Rat fragen können und die anderen Entwickler unseren Code eher verstehen.
Noch ein paar Worte zu den Log-Levels. Diese werden hierarchisch aufgebaut und beinhalten:
1. TRACE
2. DEBUG
3. INFO
4. SUCCESS
5. WARNING
6. ERROR
7. CRITICAL
Wenn wir den Log-Level unseres Loggers auf INFO setzen, dann werden alle Meldungen von 
INFO und den Levels darüber ausgegeben, also SUCCESS, WARNING, ERROR und CRITICAL. 
Setzen wir den Level auf ERROR, werden lediglich ERROR und CRITICAL ausgegeben.
Wie aber setzen wir einen Log-Level? Schreiben Sie Folgendes unter den Import des Moduls 
loguru.
import sys
logger.remove()
logger.add(sys.stdout, level="ERROR")
Damit entfernen wir den für uns automatisch angelegten Logger und richten einen neuen 
ein, der in den Prompt schreibt (es ginge auch zum Beispiel in eine Datei) und erst Nachrichten ab 
dem Level ERROR ausgibt. Luxuriös, nicht wahr?
Die __init__()-Methode
Was fragt ein britischer Python-Anfänger zu dieser Methode? It’s a constructor, __init__?
Okay, es gibt bessere Witze da draußen, aber im Prinzip trifft es den Nagel auf den Kopf. 
__init__() wird zwar Initializer genannt, wird aber dennoch aufgerufen, wenn eine neue 
Klasseninstanz angefragt wird. Und eine solche Methode findet in der Regel Verwendung, 



2.6 Erstellen eines Klassengrundgerüsts 25
wenn es um die Initialisierung einer Instanz geht, so wie eben auch häufig bei Konstruktoren 
in anderen Sprachen. Sehen Sie mir also nach, wenn mir das ein oder andere Mal der Begriff 
Constructor rausrutscht.
Fügen Sie unserer Klasse VoiceAssistent die Methode __init__(self) hinzu und erstellen 
Sie darin eine Logausgabe mit logger.error() analog zu Listing 2.2. Ändern Sie dann in 
der Methode run() den Aufruf von logger.info() in logger.debug() und passen Sie auch 
in Zeile 5 den Level an, sodass dieser auf INFO steht.
Listing 2.2 Implementierung von Logger und __init__-Methode
1. from loguru import logger
2. import sys
3.
4. logger.remove()
5. logger.add(sys.stdout, level="INFO")
6.
7. class VoiceAssistant:
8.
9. def __init__(self):
10. logger.error("VoiceAssistant wird initialisiert.")
11.
12. def run(self):
13. logger.debug("Los geht’s!")
14.
15. if __name__ == '__main__':
16. va = VoiceAssistant()
17. va.run()
18. logger.info("Gestartet.")
Führen Sie nun Listing 2.2 aus, werden Sie Folgendes beobachten können: Da __init__()
zuerst aufgerufen wird (konkret in Zeile 9), wird die Error-Meldung auch zuerst ausgegeben. 
Es folgt run(), aber da DEBUG niedriger als INFO ist, wird die Meldung vom Logger nicht 
gezeigt. Zuletzt wird in Zeile 18 die INFO-Meldung ausgegeben.
Ganz schön trockene Kost, oder? Okay, lassen wir es erst mal dabei bewenden und suchen 
uns einen Anwendungsfall, der mehr begeistert und auch ein wenig mehr fordert. Begeben 
wir uns im nächsten Kapitel in die Welten der Sprachsynthese.






3 Erzeugung 
künstlicher Sprache
Das Erzeugen künstlicher Sprache, oder die Sprachsynthese, hat zur Aufgabe, Text in ver ständliche, akustische Laute zu wandeln. Das Feld ist schon sehr lange in der Informatik 
präsent. Haben Sie den Film Die Entdeckung der Unendlichkeit gesehen, der vom Leben des an 
ALS erkrankten Mathematikers Stephen Hawking handelt? Dort wird dargelegt, dass Sprach computer schon in den frühen 90er-Jahren existierten (Hawkins erster Sprachcomputer, der 
DECtalk DTC 01, war so groß wie ein Koffer) und Menschen, die Probleme hatten, sich verbal 
zu verständigen, eine Stimme gaben. Es wurde also schon seit längerem an sogenannten 
Text-To-Speech (TTS)-Systemen gearbeitet.
Heiga Zen bezeichnet Text-To-Speech (und ebenfalls Speech-To-Text) in einem seiner Vor träge als ein klassisches Mapping-Problem, in dem eine Sequenz aus Buchstaben in eine 
Sequenz von Lauten umgewandelt werden muss. Dem gehe immer eine Vorverarbeitung 
über Methoden des Natural Language Processing (NLP) voraus, um die zu synthetisierenden 
Texte in kleinste, normalisierte Einheiten umwandeln, die dem Synthetisierungsprozess eine 
größtmögliche Informationsdichte für das Mapping liefern. Dieser wiederum durchlaufe 
dann mit den bereitgestellten Informationen Prozesse der Prosodie1
- und Wellenvorhersage 
(Zen, 2018).
Ansätze für TTS-Systeme gibt es viele. Tan, Qin, Soong und Liu nennen in ihrem Paper vier 
(Tan, Qin, Soong, & Liu, 2021):
1. Articulatory Synthesis: Erzeugung von Sprache durch Simulation von Lippen-, Zungen und Stimmbandbewegung.
2. Formant Synthesis: Erzeugt Sprache auf Basis einer Reihe von Regeln, die häufig von 
Linguisten mit dem spezifischen Fachwissen angelegt werden.
3. Concatenative Synthesis: Am besten greifbar und für mich der intuitivste Ansatz 
ist die Konkatenierung von einzelnen Spracheinheiten auf Basis eines zuvor erstellen 
Audiokorpus. Abhängig von Präfix, Token (Wort, Silbe, Buchstabe) und Postfix wird das 
am besten passendste Sample herausgesucht und an die resultierende Waveform ange hängt.
4. Statistical Parametric Synthesis: Die konkatenative Synthese hat den Nachteil, dass sie 
sehr künstlich und wenig betont klingt. Mit der statistischen parametrischen Synthese 
versucht man dem zu begegnen, indem zuerst die akustischen Parameter generiert werden 
und auf deren Basis die natürliche Sprache wiederhergestellt wird.
1 Prosodie bezeichnet die Summe aller Eigenschaften einer Stimme, darunter Tempo, Rhythmus, Intonation und 
Satzmelodie.



28 3 Erzeugung künstlicher Sprache
5. Neural Speech Synthesis: Die Verwendung von Deep Learning in TTS (siehe Bild 3.1) 
basiert auf dem Training von Modellen, die in der Lage sind, linguistische Features (in 
unserem Falle die sogenannten Phoneme) in eine Wellenform zu transformieren. Googles 
WaveNet war eines der ersten, weitverbreiteten Netze, das diese Aufgabe überzeugend über nehmen konnte. Dazu kam die Verwendung von Mel Spectrograms, die die Verwendung 
von linguistischen Features drastisch vereinfachten und es erlaubten über Generative 
Adversarial Networks (GAN) Wellenformen aus diesen Spektrogrammen abzuleiten.
Text Textanalyse Akussches
Modell Synthese Linguissche
Features
Akussche 
Features
Waveform
Bild 3.1 Bausteine eines (neuronalen) TTS-Systems (Tan, Qin, Soong, & Liu, 2021).
Wir werden uns später in diesem Kapitel mit Neural Speech Synthesis beschäftigen und 
praktisch Schritt für Schritt durch den Trainings- und Anwendungsprozess gehen. Keine 
Sorge also, wenn diese etwas knappe Übersicht noch nicht ganz alle Fragen beseitigt hat.
Für den Fall, dass Sie den schnellen Weg bevorzugen und sich TTS über neuronale Netze für 
ein ruhiges Wochenende aufheben möchten, werden wir in den nächsten Abschnitten erst 
mal die traditionellen TTS-Frameworks in Python anschauen und zusehen, dass wir eines 
davon zum Laufen bekommen. Ganz im Sinne von erst kommt die Pflicht, dann folgt die Kür
fangen wir nun auch gleich damit an.
■ 3.1 Einsatz traditioneller Frameworks
Es gibt verschiedene Frameworks, die TTS-Funktionalität anbieten und sicherlich haben Sie 
auch eines davon auf Ihrem PC vorinstalliert. Da wären zum Beispiel Microsoft TTS, Apples NSSpeech Synthesizer oder das freie eSpeak, das auch auf Ubuntu und CentOS läuft. Nun könnten wir 
uns auf die Suche nach den entsprechenden Python-Modulen machen, oder aber wir machen 
es uns einfach und verwenden pyttsx3, ein Modul, das im Prinzip ein Fassaden-Pattern für 
die drei TTS-Engines ist und deren Funktionalität eine einheitliche API verpasst. Sprich, es 
ist selber kein Modul, dass TTS-Funktionen beinhaltet, sondern vorhandene standardisiert.
Erstellen Sie gerne eine neue Umgebung für dieses Beispiel – ich werde sie im Folgenden 
002_ttsx nennen – und aktivieren Sie diese. Die Befehle sind im Anaconda Prompt abzusetzen 
und lauten wie folgt:
conda create -n 002_ttsx python=3.8
conda activate 002_ttsx
Legen Sie auch einen Ordner im Verzeichnis Sprachassistent mit Namen 002_ttsx an, um die ses Kapitel entsprechend zu strukturieren. Den vollständigen Code sehen Sie in Listing 3.1. 
Kopieren Sie ihn in eine neue main Punkt Pei. Was wir lediglich noch tun müssen, ist ,pyttsx3 per 
pip zu installieren: pip install pyttsx3.



3.1 Einsatz traditioneller Frameworks 29
TIPP: Sie sind nun in einem neuen Environment und somit steht loguru nicht mehr 
bereit, bis Sie es nachinstalliert haben. Das können Sie einfach durch einen zwei ten pip-Aufruf nachholen oder Sie installieren beide Pakete sequenziell über pip 
install loguru pyttsx3.
Hier passiert einiges, was Ihnen neu erscheinen mag, aber dennoch gut zu verstehen sein 
sollte. Wir importieren zuallererst pyttsx3 in Zeile 2. Dann, und das ist vielleicht etwas 
überraschend, importieren wir zusätzlich das Logging-Modul von Python, von dem ich im 
vorherigen Teil behauptet habe, dass wir es nicht zum Loggen von Nachrichten verwenden 
wollen, weil es hier und da etwas umständlich zu bedienen ist. Tatsächlich soll es auch nicht 
für das Logging eingesetzt werden, sondern um das Logging von pyttsx3 zu unterdrücken. In 
Zeile 6 holen wir uns den Logger für das Modul mit dem Namen comtypes._comobject. Und 
diesen Logger für genau dieses Modul setzen wir dann auf WARNING, sodass alle anderen 
Ausgaben daraus unterdrückt werden. Warum? Wenn Sie das Programm über python main Punkt Pei
ausführen, kommentieren Sie die Zeile doch mal aus und wieder ein und vergleichen Sie die 
Ausgabe. Wir haben nun eine Menge Text gefiltert, der nicht explizit zum Betrieb unseres 
Sprachassistenten Auskunft gibt.
Des Weiteren ist zu sehen, dass in Zeile 14 die Initialisierung von pyttsx3 stattfindet. Dabei 
selektiert das Modul eine uns zur Verfügung stehende TTS-Engine und speichert den Ver weis darauf in self.tts. Mit self.tts legen wir eine Variable in unserer Klasseninstanz von 
VoiceAssistant an. Diese nutzen wir auch gleich in Zeile 17, um eine Liste aller Stimmen 
zu bekommen, zwischen denen wir aus dem Fundus unserer TTS-Engine wählen können.
Listing 3.1 Ein einfaches Beispiel für den Einsatz von pyttsx3
1. from loguru import logger
2. import pyttsx3
3. import logging
4.
5. # Unterdrücke Logausgaben der Hintergrunddienste von pyttsx3
6. logging.getLogger('comtypes._comobject').setLevel(logging.WARNING)
7.
8. class VoiceAssistant():
9.
10. def __init__(self):
11. logger.info("Initialisiere VoiceAssistant...")
12.
13. logger.info("Initialisiere Sprachausgabe...")
14. self.tts = pyttsx3.init();
15.
16. # Ausgabe aller Sprachengines und Sprachpakete
17. voices = self.tts.getProperty('voices')
18. for voice in voices:
19. logger.info(voice)
20.
21. voiceId = """HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\
22. Speech\Voices\Tokens\TTS_MS_DE-DE_HEDDA_11.0"""
23. self.tts.setProperty('voice', voiceId)
24. self.tts.say("Initialisierung abgeschlossen");



30 3 Erzeugung künstlicher Sprache
25. self.tts.runAndWait();
26. logger.debug("Sprachausgabeninitialisierung abgeschlossen.")
27.
28.
29. def run(self):
30. logger.info("VoiceAssistant Instanz wurde gestartet.")
31. self.tts.say("Ich bin bereit.");
32. self.tts.runAndWait();
33.
34.
35. if __name__ == '__main__':
36. logger.info("Anwendung wurde gestartet")
37. va = VoiceAssistant()
38. va.run()
Mit der anschließenden Schleife in Zeile 18 gehen wir Stimme für Stimme durch und schrei ben diese in das Konsolenfenster (siehe Bild 3.2). In Zeile 21 und 22 wähle ich eine davon 
aus (genau genommen die ID; diese kann bei Ihnen anders aussehen) und setze die Stimme 
in Zeile 23.
Bild 3.2 Ausgabe aller zur Verfügung stehenden Sprachen von Microsoft TTS in Windows 10
In Zeile 24 definieren wir nun, was die Engine sprechen soll, und zwar geben wir im Auf ruf der __init__()-Funktion bekannt, dass die Initialisierung abgeschlossen wurde. Doch 
damit wird der Satz noch nicht gesprochen, erst der Aufruf von runAndWait() startet die 
Wiedergabe.
TIPP: Sollten Sie aus bestimmten Gründen beim Aufruf keine verfügbaren Stim men angezeigt bekommen, greifen Sie auf eSpeak zurück. Das Tool kann als 
Windows-Binary von Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichespeak.sourceforge.net heruntergeladen werden und 
verfügt auch über die Fähigkeit, deutsche Texte zu sprechen.
Ihr PC sollte nun sprechen, zwar nicht gut bzw. nicht sehr authentisch, aber dennoch ver ständlich und das ist doch schon mal etwas. Bevor wir das Beispiel etwas ausbauen, wollen 
wir einen kurzen Exkurs in Richtung pip unternehmen, um einige weitere Funktionen 
kennenzulernen.



3.2 Text-To-Speech und Multiprocessing 31
Pakete auflisten und bestimmte Versionen installieren mit pip
Es kann für Sie und andere interessant sein zu sehen, welche Pakete Sie in Ihrer Umgebung 
installiert haben und in welcher Version sie vorliegen. Das herauszufinden ist einfach. Ver wenden Sie dazu den Befehl pip freeze.
Sie sehen, dass eine Auflistung aller Pakete und der dazugehörigen Versionen ausgegeben 
wird. Nun kann es sein, dass Sie einen bestimmten Grund haben, ein Paket in einer speziellen 
Version zu installieren, zum Beispiel weil das aktuelle Release einen Fehler enthält und somit für Ihren 
Zweck unbrauchbar ist. Geben Sie exemplarisch einmal pip install loguru==0.5.2 ein. 
Sie werden sehen, dass loguru in der aktuellen Version (0.5.3) deinstalliert und loguru 0.5.2 
installiert wird.
■ 3.2 Text-To-Speech und Multiprocessing
Der aufmerksame Entwickler wird wahrscheinlich schon gemerkt haben, dass sich in 
Listing 3.1 ein Problem verbirgt. Ganz konkret in Zeile 25 und in Zeile 32, denn dort ver wenden wir den Aufruf runAndWait() und wie der Name schon sagt, wartet das Programm 
auf die Ausführung dieser Funktion und schiebt alle anderen Prozesse auf die lange Bank. 
Und das ist schlecht, denn der Assistent ließe sich nicht unterbrechen und nähme auch keine 
anderen Befehle an, während er spricht. Aber das soll keine Herausforderung sein, der wir 
uns nicht gewachsen sehen. In Python gibt es schließlich Mittel und Wege, Ausführungen zu 
parallelisieren. Diese nennen sich Threading und Multiprocessing. Für was aber entscheiden 
wir uns? Gönnen wir uns eine kurze Gegenüberstellung:
 Threads laufen im selben Speicherraum, Prozesse haben ihren eigenen (siehe Bild 3.3).
 In einem Python-Prozess kann immer nur ein Thread gleichzeitig ausgeführt werden. Das 
klingt jedoch schlimmer als es ist, springt ein Prozess doch so schnell zwischen Threads 
hin und her, dass der Benutzer hiervon nichts mitbekommt.
 Prozesse sind leichter aufzurufen und zu verwalten.
 Threading vereinfacht den gemeinsamen Zugriff auf Objekte wie Klasseninstanzen und 
Variablen.
 Prozesse können jederzeit unterbrochen werden, Threads nicht.
Prozess
Speicherraum
Zeit
Thread 1 Thread 2 Thread 3
Bild 3.3 Mehrere Threads in einem Prozess teilen sich den Speicherraum und können so leichter 
auf gemeinsame Variablen zugreifen.



32 3 Erzeugung künstlicher Sprache
Das beste Argument für Multiprocessing habe ich mir für den Schluss aufgehoben. Um zu 
erreichen, dass unser Sprachassistent still ist, wenn wir den Befehl dazu geben, müssen wir 
auf Multiprocessing setzen. Das zweitstärkste Argument ist für mich die wirklich einfache 
Implementierung, wie wir nachfolgend sehen werden, auch wenn Threading viel häufiger 
ein Thema in der Entwicklung ist.
Für den TTS-Anteil werden wir gleich ein eigenes Modul anlegen und darin wiederum eine 
eigene Klasse. Lassen Sie uns aber vorher kurz das Grundgerüst in unserer main Punkt Pei ent sprechend Listing 3.2 überarbeiten. Darin importieren wir zunächst die Klasse Voice aus 
dem Modul TTS, die wir jeweils gleich anlegen werden. Dazu kommt eine Zeile später der 
Import von multiprocessing.
Listing 3.2 Das Grundgerüst des Sprachassistenten in Vorbereitung auf TTS mit Multiprocessing
1. from loguru import logger
2. import pyttsx3
3. from TTS import Voice
4. import multiprocessing
5.
6. class VoiceAssistant():
7.
8. def __init__(self):
9. logger.info("Initialisiere VoiceAssistant...")
10.
11. logger.info("Initialisiere Sprachausgabe...")
12. self.tts = Voice()
13. voices = self.tts.get_voice_keys_by_language("German")
14. if len(voices) > 0:
15. logger.info('Stimme {} gesetzt.', voices[0])
16. self.tts.set_voice(voices[0])
17. else:
18. logger.warning("Es wurden keine Stimmen gefunden.")
19. self.tts.say("Initialisierung abgeschlossen")
20. logger.debug("Sprachausgabe initialisiert")
21.
22. def run(self):
23. logger.info("VoiceAssistant-Instanz wurde gestartet.")
24.
25. if __name__ == '__main__':
26. multiprocessing.set_start_method('spawn')
27.
28. va = VoiceAssistant()
29. logger.info("Anwendung wurde gestartet")
30. va.run()
Dann ändern wir die Initialisierung in Zeile 13 ab auf die unserer eigenen Klasse (siehe 
Listing 3.3) und verwenden die noch fiktive Methode get_voice_keys_by_language(), um 
alle Stimmen für die Sprache Deutsch zu erhalten. Der Aufruf von say() in Zeile 19 sieht so 
ähnlich aus wie der von pyttsx3, benötigt aber kein runAndWait() mehr.
Eine Besonderheit ist noch in Zeile 26 zu sehen. Dort setzen wir die Startmethode eines 
neuen Prozesses auf spawn, warum das? Die Erklärung verbirgt sich in der Python-Doku 


3.2 Text-To-Speech und Multiprocessing 33
mentation des Moduls2
. Normalerweise nutzt multiprocessing fork, um einen neuen Prozess 
ins Leben zu rufen. Das steht jedoch nur auf Unix-Systemen zu Verfügung und da wir auf 
Windows entwickeln, müssen wir auf spawn umsteigen, um einen komplett neuen Python Interpreter-Prozess zu starten. Es ist wichtig, dass dieser Aufruf vor Erzeugung des ersten 
Kindprozesses gemacht wird.
Legen Sie nun im selben Ordner eine Datei mit Namen TTS.py an und öffnen Sie diese. Fügen 
Sie den Inhalt aus Listing 3.3 ein.
Listing 3.3 Die eigene TTS-Klasse
1. from loguru import logger
2. import time, pyttsx3
3. import multiprocessing
4.
5. def __speak__(text, voiceId):
6. engine = pyttsx3.init()
7. engine.setProperty('voice', voiceId)
8. engine.say(text)
9. engine.runAndWait()
10.
11. class Voice:
12.
13. def __init__(self):
14. self.process = None
15. self.voiceId = """HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\
 Speech\Voices\Tokens\TTS_MS_DE-DE_HEDDA_11.0"""
16.
17. def say(self, text):
18. if self.process:
19. self.stop()
20. p = multiprocessing.Process(target=__speak__, args=(text, self.voiceId))
21. p.start()
22. self.process = p
23.
24. def set_voice(self, voiceId):
25. self.voiceId = voiceId
26.
27. def stop(self):
28. if self.process:
29. self.process.terminate()
30.
31. def get_voice_keys_by_language(self, language=''):
32. result = []
33. engine = pyttsx3.init()
34. voices = engine.getProperty('voices')
35. for voice in voices:
36. if language == '':
37. result.append(voice.id)
38. elif language.lower() in voice.name.lower():
39. result.append(voice.id)
40. return result
2 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichdocs.python.org/3/library/multiprocessing.html



34 3 Erzeugung künstlicher Sprache
Lassen Sie uns das Listing einmal schnell durchgehen. In Zeile 2 sehen wir kurz ein neues 
Konstrukt der Imports. Wir können nämlich auch mehrere Module kommasepariert in eine 
Zeile schreiben, hier time und pyttsx3, wobei ersteres, wie der Name vermuten lässt, für zeit bezogene Funktionen verantwortlich ist.
In Zeile 5 folgt die globale Methode __speak__(), die nicht an eine Klasse gebunden ist. Der 
Inhalt kommt Ihnen sicher vertraut vor, denn sie kapselt die Logik, die im letzten Beispiel der 
Sprachwiedergabe gedient hat, inklusive des runAndWait(), das wir eigentlich loswerden 
wollten. Mit den vorangestellten und nachfolgenden zwei Unterstrichen teilen wir allen Be nutzern mit, dass diese Methode nicht ohne guten Grund manuell aufgerufen werden soll.
Erst in Zeile 11 beginnen wir die eigentliche Definition der Klasse Voice. Diese beinhaltet eine 
eigene Methode __init__(), die wir bereits aus unserer ersten Klasse kennen. Darin legen wir 
zwei Variablen an, process und voiceId. Die voiceId ist die Kennung der selektierten Sprache 
und bekommt einen Default-Wert zugewiesen. Die Variable prozess soll die Referenz auf den 
Prozess beinhalten, der für die Sprachwiedergabe verantwortlich ist. Diesen initialisieren 
wir mit None, was so viel bedeutet, dass der Wert per Definition nicht gesetzt (oder null) ist.
In der Funktion say() (Zeile 17) prüfen wir, ob process None ist. Wenn nämlich self.process
gesetzt ist, evaluiert das If-Statement in Zeile 18 zu True. Das hat zur Folge, dass die Methode 
stop() aufgerufen wird. Wird also derzeit etwas gesprochen und kommt ein neuer Sprech befehl rein, wird die laufende Wiedergabe unterbrochen.
Nun kommen wir endlich zum eigentlichen Multiprocessing. Per multiprocessing.Pro cess() starten wir einen neuen Prozess. Die Methode bekommt zwei Parameter mitgegeben, 
target und args. Ersterer bezeichnet die Methode, die in diesem neuen Prozess aufgerufen 
werden soll, args welche Argumente dieser Methode übergeben werden.
TIPP: Die Verwendung von () zur Übergabe mehrerer Werte ist bei args zulässig. 
Das Konstrukt nennt sich Tuple und ist ähnlich der Liste [], bloß dass das Tuple un veränderlich (immutable) ist. Einer Liste hingegen können beliebig Elemente hinzu gefügt werden, die Elemente einer Liste können umsortiert und auch nachträglich 
entfernt werden.
Wir sehen also, dass __speak__() in einem neuen Prozess aufgerufen wird und dass der 
Methode der zu sprechende Text sowie die ID der zu verwendenden Sprecherstimme mit gegeben wird. Im Anschluss wird der Prozess mit start() gestartet und die Referenz auf 
den Prozess unter self.process gespeichert.
Gehen wir zur stop()-Methode in Zeile 27. Hier prüfen wir erneut, ob process auf einen 
gültigen Prozess zeigt und falls dieser Prozess existiert, rufen wir darauf terminate() auf, 
sodass dieser sofort beendet wird. Damit endet auch die Sprachwiedergabe. Das ist schon die 
ganze Magie hinter dem Multiprocessing, das einem vom Namen her wahrscheinlich mehr 
Angst einjagt, als es eigentlich müsste.
Wir werfen zu guter Letzt noch einen Blick auf get_voice_keys_by_language(). Die Funk tion fragt die bereits bekannte Auflistung aller vorhandenen Sprachen ab, durchsucht jedoch 
den Namen der Stimme nach dem Schlüsselwort language. Wie Sie gesehen haben, haben 
wir im Aufruf German übergeben. Ist also German in voice.name vorhanden, dann wird die 



3.3 Trainieren einer eigenen TTS-Engine 35
Stimme mit in die Ergebnismenge aufgenommen. Ist keine Sprache per Parameter language
übergeben, wird jede Sprache zurückgegeben.
Speichern Sie die Datei und führen Sie main Punkt Pei aus. Es sollten sich keine großen Unter schiede zeigen, doch wir haben damit erreicht, dass wir unsere Sprachausgabe jederzeit 
unterbrechen können.
■ 3.3 Trainieren einer eigenen TTS-Engine
Die Verlockung, die eigene Stimme oder die einer berühmten Persönlichkeit in den eigenen 
Sprachassistenten zu integrieren, ist hoch und einige Forschungsarbeiten, YouTube-Videos 
und Frameworks versprechen uns sogar, dass das mittlerweile leicht möglich ist. Warum 
machen es also so wenige?
Tatsache ist, dass die Technologie immer noch in den Kinderschuhen steckt, denn es gibt 
mehrere Herausforderungen, denen man noch nicht richtig oder nur mit viel Aufwand be gegnen kann, darunter:
 Die Qualität der Sprachsynthese ist auch bei modernen Frameworks noch nicht perfekt.
 Für das Training einer eigenen TTS-Engine müssen mehrere Stunden Audio aufgenommen 
werden.
 Der Syntheseprozess ist auch mit einer entsprechenden GPU langsam, sodass es einige 
Sekunden dauert, bis die Ausgabe generiert ist.
 Das Training der Modelle ist nicht trivial.
Im Moment gibt es aus meiner Sicht zwei Projekte, die einer näheren Betrachtung wert sind, 
das sind TTS von coqui.ai und Real Time Voice Cloning von Corentin Jemine.
HINWEIS: Das Medium Buch ist für ein so schnelllebiges Thema vielleicht nicht 
die beste Wahl, kann man doch davon ausgehen, dass zur Veröffentlichung schon 
wieder drei neue Frameworks veröffentlicht wurden, die die Aufgabe besser oder 
zumindest anders erledigen. Die Vorgehensweise wird jedoch noch eine Weile die selbe bleiben, sodass wir uns getrost mit den Konzepten beschäftigen können.
Ersteres ist der inoffizielle Nachfolger von Mozilla TTS, das nach dem Weggang des Entwicklers 
und Leiters der Machine Learning Group Kelly Davis und nach Budgetkürzungen wohl nur 
noch eingeschränkt weiterentwickelt wird. Auch wenn coqui.ai vielversprechend aussieht 
und gewissenhaft gepflegt und weiterentwickelt wird, gab es für mich einige Gründe mit 
Real Time Voice Cloning zu arbeiten:
 Das Framework wurde entwickelt, um einer TTS-Engine in Sekunden eine weitere Stimme 
hinzufügen zu können, was besonders in unserem Anwendungsfall interessant ist. Stellen 
Sie sich vor, Ihr Sprachassistent könnte die Stimmen imitieren, die er grade gehört hat.



36 3 Erzeugung künstlicher Sprache
 Die Dokumentation von coqui.ai ist noch nicht sehr ausgefeilt und deckt nur wirklich ein fache Verfahren ab; das gilt sowohl für das Training als auch für die Anwendung der API.
 Experimente mit coqui.ai und dem vortrainierten deutschen Modell zeigten, dass die 
Synthese noch sehr langsam ist.
Ob sich diese Punkte maßgeblich auswirken und ihre Berechtigung haben, werden wir später 
sehen. Ich verspreche jedoch, dass wir ein ganz ordentliches Resultat zu sehen bekommen.
3.3.1 Einführung in Real Time Voice Cloning
Das Projekt wurde damals erstellt, um ein neuronales Netz (genaugenommen drei neuronale 
Netze) auf Basis vieler verschiedener Sprecher so zu trainieren, dass jede beliebige, bis dato 
unbekannte Stimme durch ein kleines, kurzes Sample geklont, also imitiert, werden konnte. 
Die Ergebnisse waren so vielversprechend, dass die Technologie verfeinert und über das 
Unternehmen resemble.ai kommerziell eingesetzt wurde.
Die Bestandteile des Frameworks sind in Bild 3.4 zu sehen.
a) b) c)
Bild 3.4 Die drei Bestandteile des Real-Time-Voice-Cloning-Frameworks sind a) Encoder, b) Synthesizer 
und c) Vocoder.
a) Encoder: Der Encoder nimmt eine Audiodatei entgegen und ist in der Lage, daraus ein 
Embedding abzuleiten, das eine Sprecherstimme mit all ihren Eigenarten repräsentiert. 
Ähnliche Stimmen haben ein ähnliches Embedding, wohingegen sich das Embedding von 
fremden Stimmen unterscheidet. In Bild 3.4 wird ein solches Embedding in a) grafisch 
dargestellt, was einen visuellen Vergleich drastisch vereinfacht3
. Ein Textverständnis kann 
der Encoder jedoch nicht aufbringen, er achtet lediglich auf Höhen, Tiefen, Intonation etc. 
Das neuronale Netz, das dem Encoder-Modell zugrunde liegt, wird darauf trainiert, eine 
Sequenz von Mel-Spektrogrammen auf einen Embedding Vector immer gleicher Größe 
abzubilden. Dessen Kern bildet ein dreischichtiges LSTM (Long Short-Term Memory), 
dessen Konzept Sie später noch kennenlernen (siehe Abschnitt 6.6.3.4).
3 Embeddings werden uns später erneut begegnen, wenn wir uns die Transformer-Architektur anschauen. Sie 
werden sehen, dass dort eine andere grafische Darstellung verwendet wird, nämlich ein Vektorraum, in dem 
wir Worte bzw. Token räumlich anordnen. Die Idee des Embeddings bleibt jedoch dieselbe. Wir repräsentieren 
ein Objekt, sei es eine Stimmfarbe oder ein Text, in numerischer Art und Weise und stellen Relationen zu den 
einzelnen Werten durch Nähe zueinander her.



3.3 Trainieren einer eigenen TTS-Engine 37
b) Synthesizer: Der Synthesizer nimmt zunächst einen Text entgegen und wandelt diesen 
in Phoneme um. Phoneme entsprechen in etwa der Lautschrift, die Sie vielleicht aus dem 
Englischvokabelbuch kennen. Diese standen immer hinter der eigentlichen Übersetzung 
und sollten Auskunft darüber geben, wie ein Wort tatsächlich gesprochen wird. Und wie 
es immer so ist mit Dingen, die man in der Schule lernt und von denen man dachte, sie 
würden einem nie wieder begegnen – hier sind wir nun und verwenden fast dasselbe 
Konzept.
Der Synthesizer ist nun in der Lage, aus einer Phonemsequenz ein Mel Spectrogram zu 
bilden, das Sie in Bild 3.4 b) sehen. Ein solches ist nicht etwa eine grafische Repräsentation, 
sondern eine numerische, sie wird hier jedoch als Bild dargestellt. Falls Sie im Gebiet der 
Sprachsynthese nicht neu sind, sind Ihnen die Frameworks Tacotron und Tacotron2 sicher 
bereits über den Weg gelaufen, die die bekanntesten Vertreter in der Welt der Synthesizer 
sind. Tacotron2 verwendet ein Sequence-To-Sequence Model with Attention, das uns später 
in Abschnitt 6.2.2 noch einmal begegnen und dort erklärt werden wird. Eine Sequenz von 
Zeichen, dem Text, wird also in eine Sequenz von Phonemen umgewandelt. Die Attention, 
also die Aufmerksamkeit, bezieht sich, wie wir dort erfahren werden, auf den gesamten 
Kontext des aktuell zu übersetzenden Textes. Wie also ein Wort gesprochen wird, hängt 
nicht nur vom Wort selbst ab, sondern auch von den vorangehenden und folgenden Wör tern. Die Besonderheit ist in diesem Fall, dass das Embedding des Encoders in die Atten tion-Schicht mit aufgenommen wird, was gewöhnlich in dieser Architektur nicht der Fall ist.
c) Vocoder: Der Vocoder ist dafür verantwortlich, aus dem Mel Spectrogram eine Waveform 
abzuleiten. Dies geschieht, wie Sie wahrscheinlich richtig vermuten, nicht über eine 
mathematische oder algorithmische Methode, sondern wieder über ein angelerntes 
Netz (etwa MelGAN oder WaveNet), das die statistisch wahrscheinlichste Welle aus dem 
Spectrogram erzeugt. In unserem Fall setzen wir das autoregressive WaveNet ein. Auto regression bedeutet, dass wir Vorhersagen für eine Reihe von Daten treffen, die nur auf 
sich selbst beruhen. Auch hierzu finden Sie später in Abschnitt 6.6.3.3 eine ausführliche 
Erklärung samt Beispiel.
TIPP: Warum nutzt man Mel Spectrograms als Austauschformat zwischen den ver schiedenen Komponenten? Es hat sich gezeigt, dass diese eine hohe Geschwindig keit und hohe Parallelisierbarkeit beim Training neuronaler Netze erlauben. Ihre 
verhältnismäßig geringe Komplexität, die u. a. die 2D-Visualisierung erlaubt, die 
wir in Bild 3.4 b) sehen können, lässt jedoch auch auf ihren Nachteil schließen: Sie 
sind verlustbehaftet (lossy).
Die Herausforderung ist nun, diese drei Komponenten in Einklang zu bringen, Daten für 
das Training der Modelle bereitzustellen und das Training durchzuführen. Die erste Aufgabe 
nimmt uns das Framework, das wir im nächsten Abschnitt einrichten wollen, dankens werterweise ab. Dann kommt jedoch die Bereitstellung der Trainingsdaten. Die geschätzte 
Arbeitsverteilung von Data Engineering und Machine Learning in einem KI-Projekt liegt etwa 
bei 80/20, sprich 80 % der Zeit verbringen Sie mit der Datenaufbereitung und nur 20 % mit 
dem eigentlichen Training. Auch hier ist das so, wie wir sehen werden. Vom Arbeitsaufwand 
ist es sogar noch etwas drastischer, denn für das eigentliche Training müssen wir nur eine 
Hand voll Befehle absetzen, die dann jeweils einige Tage Trainingszeit nach sich ziehen. 



38 3 Erzeugung künstlicher Sprache
Aber alles der Reihe nach. Lassen Sie uns erst mal einen Blick auf die Anwendung der Mo delle werfen, die wir im Nachgang trainieren möchten.
Bild 3.5 zeigt zwei Möglichkeiten, das TTS-Framework zu betreiben.
Zu klonende
Smme
Encoder
Synthesizer Phonem sequenz
Encoder + Aenon Decoder Vocoder
Wave
KǟORȐZǢɏOG
[ ]
Synthesizer Phonem sequenz
Encoder + Aenon Decoder Vocoder
Wave
KǟORȐZǢɏOG
a)
b)
Embedding
Leeres Embedding
Bild 3.5 Anwendung des neuronalen TTS-Frameworks a) mit Stimmimitation einer vorgegebenen 
Sprecherstimme und b) ohne Stimmimitation, zu erkennen am leeren Embedding (Jia, et al., 2018)
a) Der obere Abschnitt zeigt die eigentliche Verwendung, wie es von den Entwicklern vor gesehen war. Von einer zu imitierenden Stimme wird ein kurzes Sample aufgenommen, 
auf Basis dessen ein Embedding über den Encoder erzeugt wird. Ein Embedding ist im 
Prinzip nur eine Liste von 256 Gleitkommazahlen, die die Charakteristika der Stimme 
numerisch abbildet. Der Synthesizer erhält einen Text als Eingabe, der zuvor in Phoneme 
umgewandelt wurde. Auch dieser durchläuft eine Encodierung und wird dann mit dem 
Embedding konkateniert, wobei der Begriff verrechnet es vielleicht etwas genauer erklärt. 
Es folgt der Decoder, der aus der internen Repräsentation des neuronalen Netzes das Mel 
Spectrogram erzeugt. Diese Decodierung erfolgt auf Basis des Attention-Mechanismus, der 
mit dem Paper Attention is all you need (Vaswani, et al., 2017) Bekanntheit erlangte. Sehr 
vereinfacht ausgedrückt bezieht der Decoder bei diesem Prinzip immer die vergangenen 
decodierten Elemente eines Decodierungsprozesses ein und schafft ein Bewusstsein, 
welche Elemente nahe beieinanderliegen und wo sich diese im gesamten Datensatz be finden. Das mit dem Ziel, einen Kontext zwischen dem gesamten finalen Mel Spectrogram
herzustellen. Der Vocoder nimmt dann das fertige Spectrogram entgegen und wandelt es 
über ein GAN (Generative Adversarial Network) zurück in eine Welle, die wir als WAV-Datei 
speichern können.
b) Der untere Prozess entspricht dem oberen aus a), ist aber dahingehend vereinfacht, dass 
auf ein Embedding verzichtet und lediglich eine Liste mit 256 Nullen übergeben wird. 
Dadurch nehmen wir keinen Einfluss auf den Synthesizer und wir müssen für unsere 
Sprachsynthese nur zwei Schritte durchlaufen.
Wir haben also die Wahl, ob wir einen Stimmimitator einbauen, oder ob wir das Beispiel 
simpel halten und uns auf eine Sprachausgabe mit einer vorgegebenen Stimme fokussieren 



3.3 Trainieren einer eigenen TTS-Engine 39
möchten. Die Vorbereitungen treffen wir für beide Varianten; wie es vonstattengeht, sehen 
wir im nächsten Abschnitt.
TIPP: Auch wenn uns die Umwandlung von Text in Phoneme in den meisten Pro jekten bereits abgenommen wird, ist es hier und da sicher ganz interessant, das 
Verhalten eines Phonemizers auszuprobieren. Eines der meist genutzten Module 
dafür ist Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/bootphon/phonemizer.
Legen Sie sich doch mal ein neues Environment an und installieren Sie das Modul 
über pip install phonemizer. Mit dem Befehl phonemize 'Das ist ein 
Beispieltext' | phonemize -l de können Sie beliebige Phoneme erzeugen. 
Stellen Sie vorher jedoch sicher, dass Sie eSpeak installiert haben, das Sie wie derum hier finden Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichsourceforge.net/projects/espeak. Bei der Installation 
ist darauf zu achten, dass de als deutsche Sprache in eine der fünf Textfelder ein getragen ist. Hier ein paar Beispiele, um die Funktionsweise und den Zweck von 
Phonemen zu verdeutlichen.
Text Phoneme
Hallo Welt haloː vɛlt
Ich bin ein Sprachassistent ɪç bɪn aɪn ʃprɑːxasɪstɛnt
Python ist eine fantastische Sprache pʏthoːn ɪst aɪnə fantastɪʃə ʃprɑːxə
3.3.2 Exkurs: Sequence-To-Sequence-Modelle und Attention
Bei der Betrachtung des Synthesizers ist der Begriff Sequence-To-Sequence (Seq2Seq) Model 
gefallen. Da wir später erneut darüber stolpern werden, wenn wir tiefer in den Bereich des 
Natural Language Processing eintauchen und unseren Sprachassistenten Fragen zu Texten 
beantworten lassen, möchte ich diese Modellklasse kurz vorstellen. Dieser Abschnitt gilt 
als Grundlage zur Besprechung der Transformer-Architektur in Abschnitt 6.2.2. Wie der 
Name vermuten lässt, ist ein Seq2Seq-Modell in der Lage, eine Sequenz von Daten auf eine 
andere abzubilden. Diese Konvertierung erlernt es, indem wir es mit Quell- und Zieldaten 
darauf trainieren. Ein häufig verwendetes Beispiel ist eine textuelle Übersetzung von einer 
Sprache in eine andere. So kann ein solches Modell die spezifische Aufgabe übernehmen, 
das italienische Mi piace leggere. in das deutsche Ich lese gerne. zu übersetzen.
TIPP: Moderne Modelle, wie etwa m2m (Many To Many) oder NLLB-200
(No Language Left Behind) von Meta, sind in Lage, beliebige Paare von 100 re spektive 200 Sprachen hin und her zu übersetzen, ohne in einem Zwischenschritt 
ins Englische übersetzen zu müssen. Letzteres können Sie im Meta Demo Lab
unter Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichnllb.metademolab.com experimentell nutzen. Die Autoren betiteln 
die Integration von weniger stark repräsentierten Sprachen wie die pakistanische 
Amtssprache Urdu oder Asturisch, das im Norden Spaniens gesprochen wird.



40 3 Erzeugung künstlicher Sprache
Neben der Textübersetzung sind jedoch noch viele andere Anwendungsfälle für Seq2Seq Modelle zu finden, etwa:
 Textzusammenfassungen
 Generieren von Bildunterschriften
 Chatbots
Im Falle der Textübersetzung wurden in klassischen Seq2Seq-Modellen Recurrent Neural 
Networks (RNN) eingesetzt – eine Architektur, die durch ein internes Gedächtnis in der Lage 
ist, Informationen aus vorherigen Schritten zu bewahren und die Ausgabe auf den Daten 
in diesem Gedächtnis aufzubauen. Voraussetzung ist, dass die Eingabedaten sequenziell 
aufeinander aufbauen. Das gilt für Aktienkurse, Filme, Arbeitsabläufe, aber auch für Texte. 
So wird beispielsweise bei der Übersetzung von Mi piace leggere. beim Wort piace nicht nur 
das Modell bemüht, um eine möglichst genau passende Übersetzung für piace zu finden, 
sondern auch eines, das dem bereits übersetzten Mi folgt, dessen deutsches Pendant ich im 
Gedächtnis des RNNs ist und bei der Wahl des folgenden Wortes (Tokens) konsultiert wird.
HINWEIS: Ein Seq2Seq-Modell kann durch ein RNN abgebildet werden, jedoch 
muss ein RNN nicht zwingend immer ein Seq2Seq-Modell repräsentieren. Es exis tieren noch zwei weitere Vertreter:
 Sequence-To-Vector-Modelle und
 Vector-To-Sequence-Modelle.
Erstere sind denkbar, wenn zum Beispiel ein Sentiment, also eine Stimmung zu 
einem Satz, ermittelt wird. So ergibt sich aus der Sequenz Wir haben den Auftrag 
leider nicht bekommen ein negatives Sentiment in Form einer –1. Das ganze Wo chenende soll die Sonne scheinen würde hingegen zu einem positiven Sentiment, 
also einem einfachen Vektor 1 führen.
Vector-To-Sequence-Modelle finden hingegen Verwendung, wenn aus einem Bild mit 
einer bestimmten Anzahl von Pixeln (die in einem Vektor fester Größe abgebildet wer den) ein Text variabler Länge generiert wird, der das Bild beschreibt (siehe Bild 3.6).
Bild 3.6 Ein Vector-To-Sequence-Modell generiert zu einem Bild eine passende Bild unterschrift (Modell und Demo von Jonas Mouyal).



3.3 Trainieren einer eigenen TTS-Engine 41
Ein Seq2Seq-Modell macht Gebrauch von zwei separaten RNNs (siehe Bild 3.7), die in sich 
retournierend aufgerufen werden. Eines bildet den Encoder, der die Eingabesequenz in eine 
numerische Repräsentation, den Kontext, überführt. Das andere RNN bildet einen Decoder, 
der den Kontext in den Zielsatz im Deutschen transformiert. Der Kontext ist eine Liste von 
Zahlen, oft als Vektor bezeichnet, die alle Informationen beinhaltet, die der Encoder aus dem 
Eingabesatz extrahieren konnte. Da Seq2Seq-Modelle diesen zweigeteilten Aufbau immer 
wieder aufweisen, werden sie auch häufig als Encoder-Decoder-Architekturen beschrieben.
Mi piace leggere. Encoder
(RNN)
Context
(Vektor)
Decoder
(RNN) Ich lese gerne.
1 … n – 1 n
Bild 3.7 Ein Seq2Seq-Modell für eine Übersetzung
Der Prozess der Übersetzung sieht nun so aus, dass der italienische Eingabesatz Wort für 
Wort in den Encoder eingegeben wird (siehe Bild 3.8). Bei der sequenziellen Erzeugung 
des Kontexts wird aber nicht nur das jeweils neue Wort herangezogen, sondern auch die 
vergangenen Wörter, die sich das RNN dank seines Gedächtnisses in Form eines Kontexts 
merken kann.
Mi Encoder RNN
Hidden State 1
Encoder RNN
Hidden State 2
Encoder RNN
Hidden State 3
/ Kontext
piace leggere.
Decoder RNN
Hidden State 4
Decoder RNN
Hidden State 5
Decoder RNN
Ich lese gerne.
Encoder
Decoder
Bild 3.8 Prozess der Textübersetzung anhand eines Seq2Seq-Modells mit Encoder und Decoder. 
Die Anzahl der RNN-Schichten ist hier nur der Anschaulichkeit halber gleich der Anzahl der Wörter 
im Satz. Sie fallen in der Regel höher aus.
TIPP: Ich verwende hier die Begriffe Token und Wort der Einfachheit halber äqui valent. In Abschnitt 6.2.1 werden wir lernen, warum das nicht immer der Fall ist 
und warum es sinnvoll sein kann, Wörter in kleinere Bausteine zu unterteilen. So 
kann etwa das Wort Raupenbagger durch den später eingeführten Tokenizer in die 
Tokens Raupen und Bagger unterteilt werden.
Nach jedem Wort wird der Zustand des Netzes in einem eigenen, sogenannten Hidden State, 
einem verborgenen Zustand, gespeichert. Der letzte Hidden State (in unserem Fall Nummer 
drei, da wir drei Wörter verarbeiten – die Satzzeichen ignorieren wir) wird dann zum Kontext, 
der dem Decoder übergegen wird. Alle anderen Hidden States werden verworfen.



42 3 Erzeugung künstlicher Sprache
Nun ist es so, dass ein Kontext meist eine feste Länge von 256, 512 oder auch 1024 Werten 
hat. Für kurze Sätze reicht das allemal, was aber, wenn wir einen ganzen Zeitungsartikel 
oder das Kapitel eines Buches übersetzen wollen? Sicher geht je nach Anzahl der Wörter der 
Kontext der ersten Sätze irgendwann verloren, in der Kompression würde man von einem 
lossy-Verfahren (verlustbehaftet) sprechen. Wir sind also bei der Verwendung klassischer 
Seq2Seq-Modelle limitiert, was die Länge der zu verarbeitenden Eingabedaten angeht. Wir 
könnten natürlich die Länge des Kontexts verändern, aber auch das ist nur bis zu einem 
bestimmten Limit möglich. Nun gibt es zwei andere, auf RNNs aufbauende Architekturen, 
nämlich GRUs (Gated Recurrent Units) und LSTMs (Long Short-Term Memory), die eine eigene 
Memory-Zelle verwenden, die die Relevanz der Worte jedes Satzes in einem numerischen 
Wert vorhalten können.
Meine Schwester hat früher gerne Trompete gespielt und das tut sie heute noch.
Indem der Begriff Schwester eine hohe Relevanz zugewiesen bekommt, kann sich das Modell 
über die Zelle also auch am Ende des Satzes noch daran erinnern, dass es sich um eine sie
handeln muss, müssten wir den Satz ab der Hälfte vervollständigen.
HINWEIS: Da RNNs sich ebenfalls hervorragend eignen, um Zeitreihen vorherzu sagen, werden uns LSTMs später in Abschnitt 6.6.3.4 erneut begegnen, wenn wir 
unserem Sprachassistenten eine eigene Wettervorhersage ermöglichen.
Doch neben GRUs und LSTMs gibt es einen Ansatz, der wesentlich erfolgreicher ist. Wenn 
Sie sich in den letzten Jahren mit Data Science und NLP beschäftigt haben, wird Ihnen das 
Paper Attention is All You Need bereits geläufig sein, ist es doch maßgeblich dafür verantwort lich, dass bei der Verarbeitung natürlicher Sprache so große Fortschritte erzielt wurden, wie 
schon viele Jahrzehnte nicht mehr.
Die Änderung, die wir am Modell aus Bild 3.8 vornehmen, ist lediglich die, dass wir nun alle 
Hidden States an den Decoder übertragen, statt bloß den letzten. Allerdings steckt da jedoch 
noch etwas mehr dahinter, denn die Zustände aller Encoder-Schritte zu übertragen, würde 
einer Überfrachtung von Informationen für den Decoder gleichkommen. Hier kommt nun die 
besagte Attention (am besten übersetzt als Aufmerksamkeit) ins Spiel, deren Mechanismus 
es dem Decoder erlaubt, in jedem seiner Durchläufe den Hidden States eine Gewichtung (die 
Attention) hinzuzufügen (siehe Bild 3.9).
Mi
Encoder RNN Hidden State 3
piace leggere.
Decoder RNN
Encoder Ich lese gerne.
Decoder
Hidden State 2
Hidden State 1
Aenon Encoder RNN
Encoder RNN
Decoder RNN
Decoder RNN
Bild 3.9 Der Attention-Mechanismus erlaubt es, aus allen erzeugten Hidden States denjenigen aus zuwählen, der für den aktuellen Durchlauf des Decoders am relevantesten ist. Somit gehen die Infor mationen aus den Zuständen früherer Encoder-Durchläufe nicht mehr verloren, sondern sie stehen 
selektiv für den Decoder-Prozess zu Verfügung.



3.3 Trainieren einer eigenen TTS-Engine 43
Der Zustand mit der höchsten Gewichtung wird dann im jeweiligen Schritt herangezogen. 
Die beinahe magisch anmutende Fähigkeit von Attention-Modellen ist nun selbstständig 
feststellen zu können, welche Eingabetokens (und somit welche Zustände des Encoders) 
für das jeweilige Ausgabetoken besonders relevant sind. Betrachten wir exemplarisch den 
Satz Der Hund mag seinen Knochen. Darin bezieht sich das Wort seinen auf Hund. Bei der 
Verarbeitung des Wortes Hund würde voraussichtlich der Hidden State eine hohe Aufmerk samkeit bekommen, der das Token seinen verarbeitete.
Bild 3.10 visualisiert diesen Mechanismus exemplarisch anhand des Satzes Der Hund mag 
seinen Knochen. Die Bibliothek bertviz bildet die Grundlage dafür und liefert eine interaktive 
Darstellung des Satzes samt den Beziehungen der einzelnen Tokens untereinander. Hängen 
zwei Tokens besonders stark zusammen, werden diese durch eine besonders kräftige Farbe 
dargestellt, wie im Beispiel anhand des Paares Der und Hund zu erkennen, das bei selektier tem Wort Der den lilanen Balken sehr stark gesättigt darstellt.
Bild 3.10 Die Gewichtung der Attention auf die jeweilig anderen Tokens wird durch die Sättigung 
der Farbe als besonders stark visualisiert.
HINWEIS: Die Sondertokens [CLS] und [SEP] stehen für Classification respekti ve Separator. Letzterer ist dafür da, um Sätze voneinander zu trennen, um dem 
Modell sagen zu können, wann ein neuer Satz beginnt. Die Existenz von [CLS] hin gegen ist durch die Aufgabe der Satzklassifikation bedingt, die viele der Modelle 
ebenfalls ausführen können. Da der Tokenizer nicht weiß, welche Aufgabe das Mo dell erfüllen wird, hängt er automatisch diese Tokens an den oder die Sätze an.



44 3 Erzeugung künstlicher Sprache
Mit dem Attention-Mechanismus kann man nun also das Problem lösen, dass wichtige Wörter 
in langen Texten keine Beachtung mehr finden (man spricht auch vom Vanishing Gradient 
Problem). Es bleibt jedoch eine weitere Herausforderung bestehen: Das Training von RNNs 
ist sehr zeitintensiv (LSTMs benötigen sogar aufgrund ihrer Komplexität noch mehr Zeit, 
um ausreichend trainiert zu werden).
An diesen Punkt wäre es an der Zeit über Transformers und Self-Attention zu sprechen. 
Da wir diese jedoch erst später in Abschnitt 6.2.2 benötigen, wollen wir hier zunächst mit 
Seq2Seq-Modellen abschließen und uns weiter dem Training unserer TTS-Engine widmen.
3.3.3 Vorgehensweise und Herausforderungen
Encoder, Synthesizer und Decoder müssen separat trainiert werden, dennoch müssen wir 
nur einen Trainingsdatensatz aufbauen, da alle drei neuronalen Netze dieselbe Basis ver wenden und über den Aufruf einiger Skripte die Vorverarbeitung automatisiert vornehmen. 
Einige Besonderheiten, die u. a. auch (Jia, et al., 2018) als Urheber der Forschungsarbeit 
hinter diesem Framework nennen, möchte ich dennoch hervorheben:
 Der Encoder sollte auf einem Datensatz mit möglichst vielen Sprechern trainiert werden. 
Störgeräusche sind laut (Jia, et al., 2018) erwünscht, um das Modell robust zu machen. 
Labels werden hingegen nicht benötigt (zum Beispiel eine Transkription der Trainingsaudiodateien), 
lediglich der Sprechername findet im Training Beachtung.
 Tests mit dem Vocoder haben gezeigt, dass ein Training für die deutsche Sprache nicht 
notwendig ist, wenn man Zugriff auf das englischsprachige Modell hat. Die Übersetzung 
eines Mel Spectrograms in eine Wellenform scheint also zumindest für Englisch und Deutsch 
sprachenunabhängig zu sein.
HINWEIS: Wenn Sie sich die Mühe gemacht und den Titel des Papers Transfer 
Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis ge lesen haben, werden Sie feststellen, dass der Ursprungsgedanke hinter der Arbeit 
mal ein ganz anderer war, nämlich Sprecherstimmen identifizieren und vergleichen 
zu können. Klar, wenn wir mit dem Encoder ein Embedding einer Stimme ermitteln 
und ihre eindeutigen Charakteristika herausstellen können, können wir diese auch 
miteinander vergleichen! Und tatsächlich werden wir diese Funktion auch in den 
Sprachassistenten einbauen, allerdings über ein anderes Framework, wie wir in 
Abschnitt 6.6.1 sehen werden.
Wir werden also mit der Beschaffung der Trainingsdaten beginnen müssen, dabei sehen wir 
uns zwangsweise mit einer Herausforderung konfrontiert, die wir immer dann haben, wenn 
wir nicht mit englischsprachigen Daten zu tun haben. Es gibt nur wenige. Natürlich könnten 
wir selber Audiodaten sammeln oder erstellen, aber der Prozess wäre sehr zeitaufwendig 
und wenn Sie die Daten mit jemandem teilen oder kommerziell nutzen möchten, müssten 
Sie ebenso darauf achten, dass Sie die Rechte für Nutzung und Weitergabe bekommen.



3.3 Trainieren einer eigenen TTS-Engine 45
TIPP: Wenn Sie an einem Punkt in diesem Kapitel angelangt sind, an denen Ihnen 
das Training zu aufwendig ist oder die nötige Zeit oder Hardware fehlt, dann laden 
Sie einfach ein von mir fertig trainiertes Modell herunter. Sie finden diese in den 
Releases des von uns genutzten Repositorys:
Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/padmalcom/Real-Time-Voice-Cloning-German/releases
Im nächsten Abschnitt klonen wir Repositorys. Wenn Sie ein Release runtergela den haben, kopieren Sie einfach die gesamte Struktur in das Repository, sodass 
Sie die Dateien encoder/saved_models/my_run.pt, synthesizer/saved_models/
my_run/my_run.pt und vocoder/saved_models/my_run/my_run.pt vorliegen 
haben. Ist das erledigt, können Sie zu Abschnitt 3.3.9 springen.
Glücklicherweise gibt es für das Training von sprachrelevanten Aufgaben das Speech Data set von M-AILABS, in dem uns Forscher und IT-Spezialisten etwa 20 GB Sprachdaten samt 
textueller Transkription zu Verfügung stellen, die wir fast ohne Anpassung für unseren Zweck 
verwenden können. Während Sie nun weiterlesen, können sie bereits hier den Download 
anstoßen, denn der wird eine Weile benötigen:
Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichwww.caito.de/2019/01/the-m-ailabs-speech-dataset
Suchen Sie nach der Überschrift Statistics & Download Links und klicken Sie neben der 
Sprache GERMAN ganz rechts auf DOWNLOAD.
Neben der Herausforderung der Datenlage sehen wir uns noch mit einer weiteren konfron tiert. Wir werden relativ lange trainieren müssen, bis wir die Güte eines Modells bestimmen, 
und noch länger, um die Qualität aller Modelle im Zusammenspiel bewerten zu können. So 
kann es sein, dass der Encoder nach vier bis fünf Tagen anzeigt, dass das Training keine 
weiteren Verbesserungen mit sich bringt, ob aber das Embedding letztendlich Einfluss auf 
die ausgegebene Stimme nehmen kann, erfahren Sie erst nach etwa zehn weiteren Tagen 
nach dem Training des Synthesizers und des Vocoders. Ein Umstand, der im Kontext von 
Deep Learning häufig für Frustration sorgt und dem nur mit ausreichender Planung und 
Konzeption zu begegnen ist. Wenn Sie (wie ich) eher der experimentelle Mensch sind, der 
Dinge ausprobiert und im Falle des Scheiterns einen anderen Weg geht, bis etwas funktio niert, dann erfordert es hier sicherlich ein Umdenken. Aber derartige Herausforderungen 
halten uns mental fit, nicht wahr?
Zum Vorgehen finden Sie nachfolgend ein kurze Liste:
 Einrichten des Projekts
 Beschaffen der Trainingsdaten
 Formatieren der Trainingsdaten
 (Optional) Training des Encoders
 Training des Synthesizers
 (Optional) Training des Vocoders
 Anwendung unserer Modelle über die UI und per Code
Wir werden also zunächst das Projekt einrichten und die Trainingsdaten entpacken und auf bereiten. Dann beginnen wir mit dem Training des Encoders, das optional ist für den Fall, 
dass Sie keine Stimmen imitieren, sondern die des trainierten Modells verwenden möchten. 



46 3 Erzeugung künstlicher Sprache
Es folgt das obligatorische Training des Synthesizers sowie das optionale Training des Voco ders (wir erinnern uns, der von Corentin Jemine bereits trainierte Vocoder funktioniert für die 
englische, sowie für die deutsche Sprache). Zu guter Letzt wenden wir die Modelle in der UI 
des Frameworks an und ich zeige noch, wie diese per Python-Code aufgerufen werden können.
3.3.4 Einrichten des eigenen TTS-Projekts
Das Ursprungsprojekt finden Sie unter folgender URL: Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/CorentinJ/Real Time-Voice-Cloning.
Ich habe mir allerdings erlaubt, es bereits im Vorfeld für die deutsche Sprache vorzubereiten 
und stelle Ihnen das Projekt unter dieser URL bereit: Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/padmalcom/Real Time-Voice-Cloning-German.
Klonen Sie das Repository über den Befehl git clone Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/padmalcom/
Real-Time-Voice-Cloning-German und legen Sie ein entsprechendes Environment mit 
Anaconda an. Der Befehl könnte so lauten, wenn Sie mit der Abkürzung rtvc für Real Time 
Voice Cloning einverstanden sind: conda create -n rtvc python=3.8. Aktivieren Sie das 
Environment entsprechend über conda activate rtvc.
Damit sind wir schon fast bereit loszulegen. Sie müssen lediglich noch die Abhängigkeiten 
installieren. Das geschieht in diesem Fall über die Datei environments.yml. Deren Zweck und 
Verwendung werden wir später noch besser verstehen lernen, im Moment bitte ich Sie ein fach folgenden Befehl im Anaconda Prompt auszuführen, wenn Sie sich im gerade geklonten 
Ordner befinden:
Conda env update -n rtvc --file environment.yml
Bestätigen Sie das Update des Environments. Anaconda lädt nun mit Ausnahme von PyTorch
alle benötigten Pakete herunter. Das Modul PyTorch hat die Besonderheit, dass es von NVI DIA’s CUDA (Compute Unified Device Architecture) abhängt, das es wiederum in mehreren 
Versionen gibt. Die Version hat Einfluss auf die Auswahl des PyTorch-Moduls. Das ist auch 
der Grund, warum ich in der environments.yml keine Vorgaben für eine bestimmte PyTorch Version machen kann. Aber der Reihe nach.
Zunächst müssen Sie herausfinden, ob Sie über eine Grafikkarte verfügen, die darauf aus gelegt ist, Machine-Learning-Aufgaben darauf auszuführen. In der Regel sind das NVIDIA Karten mit der Bezeichnung Titan, Tesla und GeForce. In letzterem Fall sollte der numerische 
Bezeichner der GeForce mindestens 1080, bzw. für die spätere Generation mindestens 2080, 
bzw. 3080 betragen.
Auswahl und Installation von CUDA
Es folgt die Installation von CUDA. Schauen Sie zunächst, welche CUDA-Version von PyTorch 
aktuell unterstützt wird. Diese Angabe finden Sie hier Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichpytorch.org/get-started/locally. 
In Bild 3.11 sehen Sie etwa, dass für die aktuelle Version 1.9.0 CUDA 10.2 und CUDA 11.1 
verwendet werden können. Alternativ können Sie auch immer eine Version für Berechnungen 
auf der CPU herunterladen, jedoch ist davon abzuraten, da unser Training sonst Wochen oder 
gar Monate dauern würde.



3.3 Trainieren einer eigenen TTS-Engine 47
Bild 3.11 Die Auswahl der richtigen Version von PyTorch hängt von mehreren Faktoren ab.
Nun haben wir drei Möglichkeiten: Sie haben CUDA bereits in einer der unterstützten Ver sionen installiert. Damit wären Sie hier erst mal fertig und könnten zum nächsten Abschnitt 
springen. Wenn Sie CUDA noch nicht installiert haben, dann sollten Sie das jetzt tun.
TIPP: Falls Sie vergessen haben, welche CUDA-Version Sie ursprünglich installiert 
hatten, hilft folgender Befehl:
nvcc --version
In der letzten Zeile finden Sie hinter Cuda compilation tools das installierte Release.
Leider hat NVIDIA keine Übersichtsseite über vergangene Releases. Eine kurze Internetsuche 
hilft jedoch und zeigt, dass zum Beispiel CUDA 10.2 hier gefunden werden kann: Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichdeveloper.
nvidia.com/cuda-10.2-download-archive.
Laden Sie das Paket entsprechend Ihres Betriebssystems und der Prozessorarchitektur 
herunter und installieren Sie es.
Im dritten Fall haben Sie eine CUDA-Version auf Ihrem PC eingerichtet und möchten nun eine 
PyTorch-Version installieren, die mit dieser CUDA-Version kompatibel ist. Dafür gibt es eine 
spezielle Seite, die ich Ihnen hier verlinke: Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichpytorch.org/get-started/previous-versions.
Auswahl und Installation von PyTorch
Verfügen Sie über eine CUDA-Version, die das aktuelle Release von PyTorch unterstützt, 
wählen Sie im in Bild 3.11 gezeigten Auswahldialog Ihr Betriebssystem, bei Package Pip, bei 
Language Python und bei Compute Platform Ihre Version von CUDA. Unten erscheint nun ein 
Befehl, den Sie so im Anaconda Prompt ausführen können, sodass PyTorch installiert wird. 
Die Größe des Pakets ist beeindruckend und sicherlich auch der Grund für die Entwickler, 
nicht einfach alle CUDA-Versionen in einer einzigen PyTorch-Version zu unterstützen.
Sollten Sie eine ältere Version von CUDA besitzen, dann gehen Sie auf den Link zu den Previous 
Versions und suchen Sie dort nach dem letzten PyTorch-Paket, dass Ihre CUDA-Installation 
unterstützt. Für 10.1 wäre das zum Beispiel die Version 1.7.1. Suchen Sie dort den Installationsbefehl 
für Pip oder Conda und führen Sie ihn aus.



48 3 Erzeugung künstlicher Sprache
Mit diesem letzten Schritt haben wir nun unsere Umgebung eingerichtet und können an fangen, die Trainingsdaten bereitzustellen.
3.3.5 Beschaffen und Bereitstellen der Trainingsdaten
Entpacken Sie die Trainingsdaten, wenn Sie diese wie in Abschnitt 3.3.3 beschrieben her untergeladen haben. Ich habe auf meinem Laufwerk E: einen Ordner Datasets angelegt, der 
wiederum die Trainingsdaten enthält. Diese liegen im Unterordner de_DE vor (siehe Struktur 
in Bild 3.12). In diesem wiederum liegt ein Ordner by_book, der als Unterordner female, male
und mix enthält. Darin organisiert finden Sie einzelne Bücher oder im Falle von Angela Merkel 
ein großes, gemischtes Verzeichnis mit mehreren Reden. Jedes dieser Verzeichnisse verfügt 
über eine Datei metadata.csv, die eine Referenz auf verschiedene WAV-Dateien beinhaltet 
sowie ein Transkript des darin gesprochenen Textes.
Bild 3.12 Die Ordnerstruktur der Trainingsdaten in gekürzter Form
Ein Beispieleintrag in der metadata.csv sieht so aus:
Die_Kanzlerin_direkt_04_13_f000003|Es geht um die verfügbaren Finanzmittel für die 
Jahre 2014 bis 2020.|Es geht um die verfügbaren Finanzmittel für die Jahre 
zweitausendvierzehn bis zweitausendzwanzig.
Das CSV ist durch Pipes (|) getrennt und beinhaltet neben dem Verweis auf die entsprechende 
WAV-Datei Die_Kanzlerin_direkt_04_13_f000003 (ohne Endung) noch das Transkript in nicht 



3.3 Trainieren einer eigenen TTS-Engine 49
normalisierter und in normalisierter Form. Wie eine solche Normalisierung aussieht, ist hier 
schön zu sehen. Es werden darin beispielsweise alle Zahlen ausgeschrieben, ebenso wie alle 
Abkürzungen, Währungszeichen etc.
Nun erwartet das Framework für das Training keine metadata.csv, sondern jeweils für jede 
einzelne WAV-Datei eine Textdatei mit demselben Namen, die den gesprochenen Text ent hält. Das müssen wir nicht händisch machen. Ich habe dafür ein Skript vorbereitet, dass Sie 
einfach ausführen. Das Verzeichnis des Datasets können Sie parametrisieren. Es muss ja 
nicht sein, dass Sie Ihre Daten ebenfalls in E:\Datasets liegen haben.
python mailab_normalize_text.py --datasets_root E:\Datasets\de_DE
Schauen Sie sich mailab_normalize_text.py gerne mal an. Hier ist allerdings nichts zu finden 
als gute, alte Handwerkskunst, die nacheinander die Ordner nach Dateien mit dem Namen 
metadata.csv durchsucht, diese öffnet und die Textdateien speichert. Exemplarisch sollte 
Ihr Verzeichnis mit den Reden Angela Merkels nun eine Struktur wie in Bild 3.13 zeigen.
Bild 3.13 Die Datenvorbereitung erzeugt Paare aus Text- und WAV-Dateien aus der vorliegenden 
metadata.csv.
Damit haben wir unsere Trainingsdaten in eine Ausgangsform gebracht, mit denen nun im 
Nachhinein alle Trainingsskripte arbeiten können.
TIPP: Sie merken jetzt schon, dass der Platzbedarf der Trainingsdaten hoch ist, 
das steigert sich aber noch. Vergewissern Sie sich, dass Sie in Ihrem Datasets Ordner etwa 150 GB Platz haben, denn wir erzeugen in jedem Trainingsschritt 
noch weitere Dateien, die darin abgelegt werden.
3.3.6 Preprocessing, Training und Evaluation des Encoders
Im ersten Schritt wandeln wir in der Vorverarbeitung die WAV-Dateien aus den Trainings daten in Mel-Spektrogramme, die in Form eines NumPy-Arrays abgelegt werden (Ending 
npy). Das erreichen Sie, indem Sie encoder_preprocess.py ausführen:
python encoder_preprocess.py E:\Datasets\
Der Prozess benötigt bei mir etwa zwei Stunden. Es kann sein, dass Fehler auftreten, die 
die Exception audioread.exceptions.NoBackendError hervorrufen. Im derzeitigen Release der 
Daten tritt dieser Fehler etwa bei ramona_deininger\\tom_sawyer\\wavs\\._tom_sawyer_09_
f000196.wav auf. Ganz konkret ist das bei den WAV-Dateien der Fall, die nicht eingelesen 



50 3 Erzeugung künstlicher Sprache
werden können. Löschen Sie diese Dateien einfach aus den Trainingsdaten und starten Sie 
das Preprocessing erneut.
Sie werden sehen, dass in E:\Datasets ein neuer Ordner SV2TTS (Speaker verification to text 
to speech, also das Transfer Learning, das im zugrunde liegenden Paper beschrieben wurde) 
erstellt wurde, der wiederum einen Unterordner encoder und darunter Ordner pro Sprecher 
enthält, die die Mel-Spektrogramme enthalten.
Nun können wir schon zum Training schreiten. Das wollen wir aber so gestalten, dass wir den 
Trainingsprozess über das Visualisierungsframework visdom beobachten können. Öffnen Sie 
dazu einen zweiten Prompt, aktivieren Sie das rtvc-Environment und setzen Sie den Befehl 
visdom ab. Sie sollten nun sehen, dass ein lokaler Server gestartet wird (siehe Bild 3.14).
Bild 3.14 Der Visdom-Server wird auf Port 8097 gestartet.
Nun können Sie in dem anderen Fenster das eigentliche Training mit folgendem Befehl starten:
python encoder_train.py my_run E:\Datasets\SV2TTS\encoder
Wir übergeben also den Pfad der erstellen Mel-Spektrogramme als Parameter. my_run titu liert unser Modell, das wir mit genau diesem Namen auch später referenzieren würden. Ein 
besserer Name wäre vielleicht german_encoder, Sie sind frei in der Namenswahl.
Nach einer Weile, wenn die Initialisierung abgeschlossen ist und das Training beginnt, 
können Sie den Fortschritt in visdom, wie in Bild 3.15 zu sehen, verfolgen. Im oberen linken 
Fenster sind die Statistiken des Trainingsdatensatzes zu sehen, gleich rechts daneben die 
Modell- und Datenparameter, wobei sich letztere teilweise mit dem linken Fenster überschnei den. Bei den Modellparametern treffen wir auf die model_embedding_size, die schon kurz 
in Abschnitt 3.3.1 angesprochen wurde, als die Optionen eines Embeddings einer anderen 
Sprecherstimme oder eines leeren Embeddings vorgestellt wurde. Hier ist zu sehen, dass die 
leere Liste 256 Nullen beinhalten muss, später erfahren Sie aber dazu mehr. Neben einigen 
Statistiken zum Device, auf dem trainiert wird (in meinem Falle eine GeForce GTX 1080 TI), 
sehen Sie auch den Mittelwert (mean) und die Standardabweichung (std) eines einzelnen, 
durchschnittlichen Trainingsschrittes. Diese sind hier sehr hoch, was wahrscheinlich daran 
liegt, dass auf meinem PC während des Trainings auch gearbeitet und Ressourcen in An spruch genommen wurden.
Oben rechts ist das Loss zu sehen, das im Deutschen als Verlustrate bezeichnet wird. Generell 
gilt: Je niedriger das Loss, desto präziser das Modell. Unser Ziel ist also klar: Trainieren, bis 
der Wert nicht mehr sinkt. Wie aber wird das Loss überhaupt bestimmt? Dafür nimmt sich 
der Trainingsprozess einen Teil unserer Daten her und verwendet ihn für das Training des 
Modells. Ein anderer, separater Teil (in der Regel der Rest der Daten) wird für die Validierung 
des Modells benutzt.



3.3 Trainieren einer eigenen TTS-Engine 51
Bild 3.15 Der Fortschritt des Trainings wird in visdom dargestellt.
TIPP: In der Regel spricht man von einer Trennung des Training Datasets und des 
Validation Datasets in der Relation 75/25, also 75 % der Daten werden für das 
Training reserviert, 25 % für die Validierung. Diese Werte können natürlich von 
Anwendungsfall zu Anwendungsfall variieren. Wichtig ist nur, dass Daten nicht 
doppelt verwendet werden, denn kennt ein Modell die Daten aus dem Validation 
Dataset, so ist das Modell darauf trainiert und kann mit hoher Wahrscheinlichkeit 
klarere Aussagen dazu treffen. Das Validation Dataset soll jedoch dazu dienen, die 
Performanz des Modells nach jeder Iteration des Trainings für unbekannte Daten sätze zu prüfen und helfen, die Gewichte entsprechend einzustellen. Manche 
Quellen sprechen weiterhin von einem Test Dataset (oder Holdout Dataset), das 
eine Evaluation des fertigen Modells erlaubt.
Beim Training des Modells werden die Gewichte innerhalb des neuronalen Netzes nach und 
nach angepasst, indem es einen Optimierungsprozess durchläuft. Einer der bekanntesten 
ist sicherlich Back Propagation.
Nach jedem Trainingsschritt wird für einen dem Netz unbekannten Datensatz der Fehler über 
eine sogenannte Loss Function berechnet, beispielsweise über den Mean Squared Error. Wenn 
man es sich einfach machen möchte, könnte man sagen, dass der Fehler in der Abweichung 
des von einem neuronalen Netz vorhergesagten Wertes von dem tatsächlich zu erwartenden 
Wert besteht. Das Loss ist eine Aufsummierung der entstandenen Fehler – und diese wollen 
wir so weit, wie es geht, minimieren.



52 3 Erzeugung künstlicher Sprache
Nun kommen wir wieder zum Anpassen der Gewichte im neuronalen Netz. Da der Fehler 
nun bekannt ist, kann dieser zu den einzelnen Neuronen im neuronalen Netz zurückgeführt 
und die Gewichte dazwischen angepasst werden. Dieser zweiteilige Prozess wird als Back 
Propagation bezeichnet und sorgt dafür, dass sich die Kurve in Bild 3.15 immer weiter der 0 
annähert. In der Regel wird die 0 jedoch nicht erreicht. Wir werden später erfahren, wie 
zu beurteilen ist, ob das Training an einer bestimmten Stelle unterbrochen werden kann.
TIPP: Warum hat das Loss zu Beginn einen Wert von etwa 4,2? Da es zu Beginn 
eines Trainings keinen konkreten Anhaltspunkt gibt, wie die Gewichte zu verteilen 
sind, werden dafür Zufallswerte gesetzt. Diese sind meistens so unzutreffend, 
dass sich das Loss zu Anfang rapide senkt, dann aber irgendwann ähnlich eines 
negativen Logarithmus optisch stagniert.
Werfen wir zuletzt noch einen Blick auf das Diagramm mit dem Titel UMAP Projection. Sie 
sehen oben in der ersten Box, dass unser Trainingsdatensatz über sechs Sprecher verfügt. 
Da das Encoder-Modell ursprünglich aus der Domäne Speaker Verification kommt, wird hier 
versucht, die Bücher einzelner Sprecher in einem Cluster anzuordnen. Alle Bücher, die Ra mona Deiniger eingesprochen hat, sollten sich in Form eines bunten Punktes an einer Stelle 
sammeln. Tun sie das nicht, können wir davon ausgehen, dass das Modell noch nicht lange 
genug trainiert wurde.
Damit sind wir auch schon in den letzten Teil dieses Abschnitts vorgedrungen, der Modell evaluierung. Wir haben bereits erfahren, dass die Kurve des Loss gegen Null konvergiert (der 
Fachbegriff lautet Convergence) und wir hier einen Anhaltspunkt haben, um zu erkennen, 
dass das Modell nicht mehr besser wird. Erkennen Sie also nach einem Tag keine Verbesse rung des Loss, können Sie davon ausgehen, dass die Potenziale, die Ihnen die Trainingsdaten 
bieten, vollständig genutzt sind. Da sich in den letzten zwei Intervallen von 5000 Schritten 
noch eine Verbesserung des Loss zeigt, können wir davon ausgehen, dass wir das Modell 
noch etwas weiter trainieren sollten. Ein weiterer Anhaltspunkt ist im Falle des Encoders die 
UMAP Projection. Bilden die bunten Punkte kleine, nahe beieinanderliegende Cluster und 
sind nicht mehr auf dem Diagramm verstreut, ist auch zu erkennen, dass das Modell eine 
gewisse Güte erreicht hat. Blicken wir auf die (in der E-Book-Version) türkisen, braunen oder 
lilafarbenen Punkte im Diagramm unten links in Bild 3.15, so sehen wir, dass diese noch 
etwas weiter verstreut und nicht klar abzugrenzen sind. Auch das deutet darauf hin, dass 
das Training fortgesetzt werden sollte.
Sollte das Training einmal unterbrochen worden sein, kann es jederzeit fortgesetzt werden. 
Wiederholen Sie dazu einfach den Befehl python encoder_train.py my_run E:\Datasets\
SV2TTS\encoder und das Skript lädt den zuletzt gespeicherten Punkt, um das Training von 
dort wieder aufzunehmen. Wo aber wird das Modell gespeichert? Schauen Sie einmal in den 
Ordner encoder/saved_models. Dort finden Sie die Datei my_run.pt, natürlich entsprechend 
des Namens, den Sie beim Aufruf des Skripts übergeben haben. Das etwa 16 MB große 
Modell ist das Resultat, das wir nun schon so zur Erstellung von Sprecher-Embeddings ver wenden können.



3.3 Trainieren einer eigenen TTS-Engine 53
3.3.7 Preprocessing, Training und Evaluation des Synthesizers
Der nächste und wichtigste Schritt ist nun, den Synthesizer zu trainieren. Die bereits ange sprochenen Anpassungen für die deutsche Sprache habe ich bereits für Sie vorgenommen, 
wobei sich die Hauptaufgabe als Erweiterung des Zeichensatzes um die Character Ä, Ü, Ö, ä, 
ü, ö, ß entpuppte, denn der Synthesizer muss wissen, welche Zeichen ihn in einem Eingabe text erwarten können und diese waren bisher nur auf das Alphabet der englischen Sprache 
gesetzt. Schauen Sie sich bei Interesse die Datei synthesizer/utils/symbols.py an und werfen 
Sie einen Blick in die Zeichenkette _characters.
Wir sind jetzt so weit, dass die Vorverarbeitung über folgenden Befehl initiiert werden kann:
python synthesizer_preprocess_audio.py E:\Datasets\ --subfolders 
de_DE\by_book\female,de_DE\by_book\male,de_DE\by_book\mix --dataset "" --
no_alignments --wav_dir
Der erste Parameter ist das Hauptverzeichnis unseres Trainingsdatensatzes, in dem die 
Ordner de_DE und SV2TTS liegen. Wir übergeben weiterhin eine Liste von Ordnern unter 
dem Parameternamen subfolders, in dem die verschiedenen Bücher zu finden sind, die die 
einzelnen Sprecher lesen. Da wir eine Kategorisierung nach Geschlecht im M-AILABS Datensatz vorliegen haben, was das Skript so nicht vorsieht, müssen wir die Unterordner 
explizit benennen. Durch die Nennung dieser Unterordner können wir im Parameter dataset
einen leeren String übergeben, da das Skript nicht abhängig von einer bestimmten Daten satzstruktur, wie sie etwa LibriSpeech aufweist, nach den Daten suchen muss. Weiterhin 
spezifizieren wir, dass unser Datensatz nicht über Alignments verfügt und die WAV-Dateien 
in einem extra wav-Ordner liegen.
HINWEIS: Alignments finden sich für manche Audiotrainingsdaten. Sie zeichnen 
sich dadurch aus, dass sie für jedes Wort eines Datensatzes den zeitlichen Beginn 
und dessen Ende in einer kommaseparierten Liste notieren.
Ist die Vorverarbeitung der Audiodateien abgeschlossenen, finden wir im Verzeichnis 
E:\Datasets\SV2TTS einen neuen Ordner synthesizer, der wiederum zwei Ordner enthält. 
Dabei handelt es sich um den Ordner audio, der für jede Datei unseres Trainingsdatensatzes 
eine numerische Repräsentation des Audio-Streams in Form eines NumPy-Arrays (Endung 
npy) enthält, sowie die Mel-Spektrogramme im Ordner mels, ebenfalls in Form einer npy Datei.
TIPP: Sollten Sie ein tiefergehendes Interesse verspüren sich den Inhalt der 
Arrays anzuschauen, zögern Sie nicht und erstellen Sie eine neue Python-Datei mit 
folgendem Inhalt, wobei Sie den Pfad von my_file.npy entsprechend anpassen.
import numpy as np
data = np.load(r'E:\Datasets\SV2TTS\synthesizer\mels\my_file.npy')
print(data)



54 3 Erzeugung künstlicher Sprache
Das r vor der Zeichenkette sagt aus, dass der String nicht formatiert wird, 
etwa durch sogenannte Escape Sequences wie \n oder \t, die sonst einen Zeilen umbruch oder einen Tab hervorrufen würden. Sie stellen also dadurch sicher, 
dass der String nicht verändert oder Schrägstriche darin interpretiert werden.
Im zweiten Schritt der Vorverarbeitung des Synthesizers erzeugen wir die Embeddings:
python synthesizer_preprocess_embeds.py E:\Datasets\SV2TTS\synthesizer --
encoder_model_fpath encoder/saved_models/my_run.pt
Sehen Sie, dass wir hier unseren Encoder verwenden? Klar, wenn wir Embeddings ohne den 
Encoder erzeugen könnten, hätten wir ihn ja überhaupt nicht trainieren müssen. Denken Sie 
daran, dass Sie den Namen des Modells anpassen, falls Sie einen anderen gewählt haben. 
Etwa 30 Minuten später (die Laufzeit kann sich natürlich je nach Ressourcen ändern) sollte 
auch der zweite Schritt abgeschlossen und die Embeddings für die jeweiligen Audiodateien 
erzeugt worden sein, sodass anschließend mit dem Training begonnen werden kann:
python synthesizer_train.py my_run E:\Datasets\SV2TTS\synthesizer
Auch hier ist die Ausführung nicht sehr kompliziert, haben wir doch zuvor das Data 
Engineering sauber abgeschlossen. Überlegen Sie sich, ob Sie dem Modell einen anderen 
Namen geben möchten, etwa german_synthesizer und ändern Sie my_run bei Bedarf. Der 
Name des Modells muss hier nicht kongruent zum Namen des Encoder-Modells sein.
TIPP: Das Training kann durch eine kleine Stellschraube beschleunigt werden, die 
jedoch bei mir zu einem eindeutigen, aber schwer zu ergründenden Fehler führt 
(EOFError: Ran out of input). In Zeile 146 der Datei synthesizer\train.py wird data_
loader initialisiert (eine Klasse des Frameworks pytorch) und dabei der Parameter 
num_workers auf 2 gesetzt. Das bedeutet, dass das Zusammentragen der Daten 
über zwei weitere Prozesse erledigt wird, sodass Datenverarbeitung und Daten beschaffung sich nicht blockieren. Ein ehrenhaftes Ziel, doch funktioniert es eben 
nicht überall, weswegen ich num_workers auf 0 gesetzt habe. Das bedeutet nicht, 
dass überhaupt keine Daten geschaufelt werden, sondern dass das Zusammen tragen eben im Hauptprozess der Python-Anwendung, also des Trainingsskripts, 
geschieht.
Geben Sie dem Training wieder einige Tage Zeit und unterbrechen Sie es dann mit STRG+C
bzw. durch Schließen des Anaconda Prompts, falls dieser nicht reagieren sollte. Im Ordner 
synthesizer\saved_models ist nun ein Ordner mit dem Namen Ihres Trainings angelegt wor den, in meinem Fall my_run. Darin befindet sich wiederum eine Datei my_run.pt, die eine 
Größe von etwa 361 MB aufweisen sollte und unser fertig trainiertes Modell repräsentiert.
Daneben sind im selben Ordner noch weitere Metadaten zu finden, ganz besonders interessant 
darunter das Verzeichnis wavs, in dem die vom Synthesizer erzeugten Mel-Spektrogramme 
über einen Standard-Vocoder (in diesem Fall Griffin-Lim, einem Algorithmus, keinem Mo dell) in WAV-Dateien umgewandelt werden und das für jeden 500. Schritt des Trainings. 



3.3 Trainieren einer eigenen TTS-Engine 55
Es lässt sich also dadurch nachvollziehen, wie die Güte des Modells steigt, ohne dass wir 
diese anhand abstrakter Zahlen wie etwa dem Loss einschätzen müssten. Vergleichen Sie 
doch mal das WAV, das nach 500 Schritten und eines, das nach etwa 30.000 Schritten erzeugt 
wurde. Hören Sie den Unterschied?
HINWEIS: Moment mal, wir können Mel-Spektrogramme auch ohne Machine 
Learning in Audio umwandeln? Genau, können wir. In diesem Fall über die simple 
Funktion librosa.feature.inverse.mel_to_audio der Bibliothek Librosa, wo bei die Methode in diesem Framework in synthesizer\audio.py ausimplementiert 
wurde. Ein wichtiger Punkt, den ich unbedingt ansprechen möchte, ist aber: Es 
muss nicht immer alles KI sein. Erlaubt ist, was funktioniert und den Zweck erfüllt, 
auch wenn das heißt, dass Unternehmen und Einrichtungen dann kein KI-Fleiß-
stempelchen bekommen. Manchmal ist es eben ein Algorithmus von 1984, der am 
besten passt. 
Jetzt habe ich das Loss, dass wir beim Encoder zur Begutachtung der Güte betrachtet haben, 
vielleicht etwas schlechter gemacht, als es wirklich ist. Denn in Nuancen herauszuhören, 
ob eine Sprachsynthese noch besser wird oder eben nicht mehr, ist mit bloßen Ohren eine 
schwere Aufgabe. Schauen Sie also weiter auf die Verlustrate und beschließen Sie erst, das 
Training zu beenden, wenn Sie merken, dass diese nicht mehr merklich näher an die Null 
heranrückt.
3.3.8 Preprocessing, Training und Evaluation des Vocoders
Beginnen Sie damit, im Ordner E:\Datasets\SV2TTS einen Ordner vocoder anzulegen. Die 
Vorverarbeitung der Daten geschieht in gewohnter Manier über ein Skript.
python vocoder_preprocess.py E:\Datasets\ --model_dir=synthesizer/saved_models/my_run/
Auch hier macht der Vocoder Gebrauch vom Synthesizer-Modell, was abermals darauf 
schließen lässt, dass der Trainingsprozess sequenziell und nicht parallelisierbar ist. Nach 
einer Laufzeit von etwa zwei Stunden sollte der Prozess abgeschlossen sein. Nun kann auch 
schon das Training gestartet werden.
python vocoder_train.py my_run E:\Datasets\
Wie auch die letzten zwei Male kann das Modell gerne auch german_vocoder genannt werden, 
eine Übereinstimmung zu früheren Modellnamen ist nicht erforderlich. Führen Sie den Befehl 
aus und unterbrechen Sie ihn, wenn Sie keine merkliche Verbesserung des Loss beobachten.
Ein Blick in den Ordner vocoder\saved_models\my_run offenbart nun eine entsprechend 
der Trainingsdauer große Liste an WAV-Dateien, die einmal mit gen_batched und target ge kennzeichnet sind. Wie Sie wahrscheinlich richtig vermuten, sind die mit target markierten 
Dateien die, die der Vocoder im Optimalfall generierten sollte und die mit gen_batched die, 
die er tatsächlich generiert. Die Zahl am Anfang kennzeichnet den Schritt des Trainings. 



56 3 Erzeugung künstlicher Sprache
Es empfiehlt sich also ein Vergleich des Paares von 1000 (1 k) und des Paares von etwa 
90.000 (90 k). Hören Sie den Fortschritt?
Fast unbemerkt haben sich in diesen Ordner auch die Modelle eingeschlichen, die sich wie 
gewohnt unter dem Namen my_run.pt finden lassen. Damit ist auch das Training des Vocoders 
abgeschlossen. Wir haben demnach unsere drei Modelle beisammen und können endlich 
schauen, wie sie sich in der Anwendung verhalten.
3.3.9 Anwendung der Modelle
Für die Anwendung haben wir zwei Möglichkeiten. Einmal bietet uns das Framework ein User 
Interface, die sogenannte Toolbox. Diese ist zwar etwas verwirrend, da eher wissenschaft licher Natur. Dafür kann man sie aber anwenden, ohne entwickeln zu müssen. Die zweite 
Möglichkeit ist der programmatische Ansatz, den wir auf jeden Fall evaluieren wollen, da 
wir ja unsere Text-To-Speech-Klasse um die eben erlernten Funktionen erweitern möchten.
Beginnen wir dennoch mit der Verwendung der Toolbox. Mit einem Aufruf von python 
demo_toolbox.py -d E:\Datasets\ starten wir die Anwendung, die in Bild 3.16 gezeigt wird.
Bild 3.16 Die UI des Voice Cloning Frameworks gliedert sich in Dataset und Modelle (oben links), 
Sprachsynthese (oben rechts), UMAP Projection (unten links), Embeddings und Mel-Spektrogramme 
(unten rechts).
Was, wie gesagt, etwas unstrukturiert wirkt, wird sich uns in den nächsten Abschnitten 
erschließen. Oben links finden Sie den Trainingsdatensatz, sortiert nach Geschlechtern 
(Dataset), Sprechern (Speaker) und Sprach-Samples (Utterances). Wir können nun einzel ne Sprach-Samples per Load laden, sodass deren Mel Spectrogram sowie ein Embedding 
unten rechts generiert und visualisiert wird. Damit lassen sich Texte mit der Stimme dieses 



3.3 Trainieren einer eigenen TTS-Engine 57
Sprechers generieren. Viel spannender ist es aber sicherlich, für die meisten Texte mit der 
eigenen Stimme sprechen zu lassen. Dafür existiert in der vierten Zeile eine Schaltfläche 
Record. Drücken Sie diese, werden Sie aufgefordert, fünf Sekunden lang einen beliebigen Text 
zu sprechen. Infolgedessen wird statt mit einer der Sprecherstimmen aus den Trainingsdaten 
ein Embedding und ein Mel Spectrogram mit Ihrer eigenen Stimme erzeugt.
Bevor Sie sich nun für eine dieser Funktionen entscheiden, lassen Sie uns einen Blick auf 
die Modelle am unteren Ende des Quadranten oben links werfen. Hier finden Sie jeweils für 
Encoder, Synthesizer und Vocoder den Namen des geladenen Modells. Sie werden hier die 
Namen wiederfinden, die Sie beim Training des jeweiligen Modells eingegeben haben, in 
meinem Fall immer my_run. Ist eines der Felder leer, sind in dem Ordner saved_models von 
Encoder, Synthesizer oder Vocoder keine Modelle zu finden. Ist hingegen an jeder Position 
ein Modell ausgewählt, müssen Sie entweder per Record ein Sample Ihrer Stimme aufnehmen 
oder per Load ein Sample einer der vorliegenden Sprecherstimmen laden. Das Embedding 
dieser Aufnahme wird im Anschluss erzeugt und angezeigt.
Es folgt die Eingabe eines deutschen Textes in das Feld oben rechts und ein Klick auf Synthe size and vocode, um diesen Text mit der von Ihnen gewählten Stimme vorlesen zu lassen. Es 
zeigt sich, dass zunächst ein Mel Spectrogram unter dem ersten unten rechts generiert wird, 
woraus der Vocoder in nur wenigen Sekunden, aber doch nicht in Echtzeit, eine Wellenform 
generiert. Das unter Audio Output gelistete Device spielt diese nun automatisch ab.
Bevor wir das Ergebnis besprechen, möchte ich noch das letzte Feld der UI erläutern, die UMAP 
Projections unten links. Klicken Sie doch dreimal auf Load, um nacheinander drei verschiedene 
Samples des gewählten Sprechers zu laden. Es sollte nach einem kurzen Augenblick ein Dia gramm gezeichnet werden, dass die jeweiligen Samples in einem Cluster visuell gruppiert. 
Nehmen Sie nun selber ein Sample auf, falls Sie das noch nicht getan haben, oder wählen 
Sie einen anderen Sprecher, um zu sehen, wie sich die Cluster voneinander distanzieren.
Bild 3.17 zeigt die Embeddings von Angela Merkel und Herrn Karlsson, die sich in roten 
und grünen Pünktchen gruppieren. Dieses Schaubild hat zwar mit der Domäne Voice Cloning
nicht direkt zu tun, es macht aber dennoch klar, dass das Embedding eines Sprechers immer 
ähnlich ist und dass die angesprochene Konkatenation mit einer generischen Stimme einen 
Einfluss auf das synthetisierte Sample nehmen kann.
Bild 3.17 Die Embeddings zweier zufällig ausgewählter Sprecher grenzen sich klar gegeneinander ab.



58 3 Erzeugung künstlicher Sprache
Und damit sind wir auch schon bei der Evaluation. Konnten Sie eine merkliche Einflussnahme 
Ihrer Stimme auf das generierte Audio vernehmen? Ich leider nicht, allerdings ist das stark 
abhängig von der jeweiligen Stimme. Und da wird es interessant: Corentine Jemine, als Ent wickler des Frameworks, hat diese Tatsache auch bei sich festgestellt. Schaut man sich die 
Firma resemble.ai an, für die er nun arbeitet, sieht man, dass diese Firma Stimmimitation über 
neuronale Netze anbietet. Schaut man genauer hin, bemerkt man schnell, dass es bei resemble.
ai mit einer Fünfsekundenaufnahme nicht getan ist, denn deren Anwendung fordert Sie auf, 
mehrere Minuten lang verschiedene Sätze vorzulesen, mit denen dann ein Modell trainiert 
wird. Genau gesagt, wird das bestehende Modell einem Re-Training unterzogen, sodass es mit 
den Stimmaufnahmen einer Person ein sogenanntes Fine Tuning erfährt – ein generisches 
Modell wird also auf einen konkreten Datensatz zugeschnitten. Und genau das wollen wir 
gleich versuchen! Zuerst schließen wir jedoch dieses Kapitel ab, indem wir uns anschauen, 
wie unsere Sprachsynthese aus Python-Code heraus aufgerufen werden kann. Schauen wir 
uns dazu die Datei demo_simple.py an, die Sie auch in gekürzter Form in Listing 3.4 finden.
Listing 3.4 Programmatischer Aufruf der Synthese
1. from encoder.params_model import model_embedding_size as speaker_embedding_size
2. from utils.argutils import print_args
3. from utils.modelutils import check_model_paths
4. from synthesizer.inference import Synthesizer
5. from encoder import inference as encoder
6. from vocoder import inference as vocoder
7. from pathlib import Path
8. import numpy as np
9. import soundfile as sf
10. import librosa, argparse, torch, sys, os
11. from audioread.exceptions import NoBackendError
12.
13. if __name__ == '__main__':
14.
15. enc_model_fpath = Path("encoder/saved_models/my_run.pt")
16. syn_model_fpath = Path("synthesizer/saved_models/my_run/my_run.pt")
17. voc_model_fpath = Path("vocoder/saved_models/my_run/my_run.pt")
18.
19. print("Lade Encoder, Synthesizer und Vocoder ...")
20. encoder.load_model(enc_model_fpath)
21. synthesizer = Synthesizer(syn_model_fpath)
22. vocoder.load_model(voc_model_fpath)
23.
24. in_path = './test.wav'
25. in_text = 'Das ist ein Test mit meiner eigenen Stimme.'
26. out_path = './gen.wav'
27.
28. original_wav, sampling_rate = librosa.load(str(in_path))
29. preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)
30.
31. embed = encoder.embed_utterance(preprocessed_wav)
32. print("Erzeuge das Embedding...")
33.
34. texts = [in_text]
35. embeds = [embed]



3.3 Trainieren einer eigenen TTS-Engine 59
36. #embeds = [[0] * 256]
37.
38. specs = synthesizer.synthesize_spectrograms(texts, embeds)
39. spec = specs[0]
40. print("Mel Spectrogram erfolgreich erzeugt")
41.
42. generated_wav = vocoder.infer_waveform(spec)
43. generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate),
44. mode="constant")
45. generated_wav = encoder.preprocess_wav(generated_wav)
46.
47. sf.write(out_path, generated_wav.astype(np.float32),
48. round(synthesizer.sample_rate / 1.0))
49.
50. print("Audiodatei wurde geschrieben.")
Listing 3.4 ist zum Glück nicht sehr komplex, sodass wir es in Gänze durchgehen können. In 
Zeile 13 beginnt die Anwendung zuerst Encoder, Synthesizer und Vocoder zu initialisieren, 
indem die von uns in mühevoller Arbeit trainierten Modelle geladen werden. Ab Zeile 24 
werden Ein- und Ausgabeparameter festgelegt, einmal in_path, das eine etwa fünfsekündi ge Aufnahme der Stimme enthält, die imitiert werden soll. An nächster Stelle steht der zu 
synthestisierende Text in in_text gefolgt von out_path, einer Pfadangabe für die Ausgabe 
der finalen Audiodatei.
Die Audioeingabe wird nun geladen und getrimmt, sodass stille Bereiche am Anfang und 
Ende weggeschnitten werden. Diese verfälschen nämlich nur das Ergebnis, da sie falsche 
Informationen über die Sprecherstimme enthalten, die nachgeahmt werden soll. In Zeile 31 
erfolgt die Erzeugung des Embeddings. Nun haben wir zwei Möglichkeiten weiterzumachen:
a) Wir verwenden das Embedding, dass wir aus der Datei in in_path erzeugt haben und 
versuchen, die darin verwendete Stimme zu imitieren.
b) Wir verwenden kein bzw. ein neutrales Embedding und greifen auf die neutrale Stimme 
des trainierten Modells zurück.
Wenn wir, wie in Zeile 35 gezeigt, das Embedding (als einziges Element einer Liste) über geben, wird dieses verwendet. Wenn wir andernfalls Zeile 36 einkommentieren, dann wird 
als Embedding lediglich eine Liste mit einer Liste aus 256 Nullen übergeben, die keine 
Charakteristika einer zu imitierenden Stimme enthält. Probieren Sie beides gerne mal aus. 
Warum müssen die Elemente in eine Liste gepackt werden? Ganz einfach, weil der Aufruf des 
Synthesizers in Zeile 38 eine Liste erwartet, da die Funktion synthesize_spectrograms()
darauf ausgelegt ist, mehrere Dateien als Batch zu verarbeiten. Dementsprechend wird auch 
eine Liste mehrerer Spektrogramme zurückgeliefert, von der wir uns in Zeile 39 das erste 
und einzige Element holen.
Nun kommt der Vocoder zum Einsatz, der über infer_waveform() in Zeile 42 eine Wellen form generiert, die im folgenden so gepadded (an den richtigen Stellen mit Nullen versehen) 
wird, dass sie in Zeile 47 als WAV-Datei geschrieben werden kann. Sehen Sie die Division 
durch 1.0 in Zeile 48? Indem Sie diese Gleitkommazahl leicht nach oben oder unten abändern, 
können Sie die Geschwindigkeit des Sprechers variieren.



60 3 Erzeugung künstlicher Sprache
TIPP: Falls Sie den NumPy-Array direkt abspielen möchten, statt ihn zu schreiben, 
ist das Vorgehen in folgendem Listing zu finden.
1. import sounddevice as sd
2. import time
3. audio_length = librosa.get_duration(generated_wav, sr = 16000)
4. sd.play(generated_wav.astype(np.float32), 
 round(synthesizer.sample_rate / 1.0))
5. time.sleep(audio_length)
6. print("Done")
Sie können den Code entsprechend einrücken und direkt an Listing 3.4 anhängen. 
Dass die Imports mitten im Text zu finden sind, stört den Interpreter nicht. Hier 
holen wir uns zunächst die Länge des Audioarrays in Zeile drei, formatieren die 
Daten so, dass sie dem Datentyp float32 entsprechen und passen die Sample Rate
an. Hier muss man ein wenig aufpassen, da diese Anpassung auch eine Änderung 
der Dauer des Audios zur Folge hat. Wählt man einen Divisor größer als 1.0, so 
erhöht sich die Dauer um den Faktor. Sie müssten also in Zeile 3 eine Anpassung 
vornehmen bzw. am besten direkt die korrekte Sample Rate übergeben (16000 
durch 1.1 wären dann beispielsweise 14545). Das time.sleep() in Zeile 5 ist 
wichtig, da Python nicht auf das Abspielen des Audio-Streams wartet, sondern die 
Anwendung beenden würde. Genau für dieses Warten geschieht überhaupt die Be rechnung der Abspieldauer.
Da wir nun den Prozess ein paar Mal durchgekaut haben und ich wahrscheinlich schon Ge fahr laufe, Sie zu langweilen, machen wir nun erst mit dem Fine Tuning des Synthesizers 
weiter, bevor wir Listing 3.4 in Initialisierung und Logik trennen und in einer alternativen 
Text-To-Speech-Klasse verarbeiten.
3.3.10 Fine Tuning des Synthesizers mit der eigenen Stimme
Haben Sie ein Glas Wasser am Tisch? Wenn nicht, besorgen Sie sich eines, denn Sie werden 
gleich etwa eine Viertelstunde Text vorlesen müssen. Zwar habe ich Ihnen schon Sätze vor bereitet, sodass Sie sich keine Aufnahmen zusammensuchen müssen, aber das Sprechen mit 
Ihrer Stimme kann ich Ihnen nicht abnehmen.
Im Repository finden Sie jedoch das Archiv de_DE.zip, das – einmal entpackt – knapp über 
200 Audiodateien enthält, die ich eingesprochen habe. Sollten Sie also Ihre eigene Stimme 
klonen wollen, löschen Sie alle WAV-Dateien aus dem Ordner by_book\male\freiknecht\
undefined\wavs und sprechen Sie diese in einem Aufnahmewerkzeug Ihrer Wahl ein. Aus 
technologischer Sicht kann ich dafür Audacity empfehlen, da Sie hier auch relativ einfach stille 
Bereiche per Ausschneidebefehl entfernen können. Auch wenn die exemplarisch in Bild 3.18
gezeigte Audiodatei am Anfang eine kurze Nulllinie (das ist vielleicht nicht der passendste 
Begriff, aber ich nehme an, Sie wissen, was ich meine) aufweist, beginnt das 00000065.wav
sofort beim Abspielen mit dem eingesprochenem Text.



3.3 Trainieren einer eigenen TTS-Engine 61
Bild 3.18 Eine beispielhafte Audiodatei, an deren Beginn und Ende keine längeren Stillephasen zu 
finden sind.
Neben Audacity können Sie meine Anwendung ttsdatasetcreator verwenden, die Sie ebenfalls 
auf Github unter der URL Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/padmalcom/ttsdatasetcreator finden. Sie bietet 
eine einfache Konsolenanwendung, die Ihnen nach und nach die Sätze aus der metadata.csv
zeigt, die Aufnahme koordiniert und getrimmt schreibt, sodass Sie sich nur noch auf das 
Sprechen konzentrieren müssen.
Gehen wir nun davon aus, dass Ihnen die Ordnerstruktur aus dem de_DE.zip mit Ihren eige nen Samples vorliegt und Ihr Glas Wasser sicherlich leer ist. Benennen Sie nun den Ordner 
de_DE aus unserem ursprünglichen Trainingsordner E:\Datasets um, zum Beispiel in de_DE_origi nal und kopieren Sie den neuen, kleineren Ordner de_DE an dessen Position. Für das Fine 
Tuning benötigen wir nämlich die Urprungstrainingsdaten nicht, wohl aber die von uns 
trainierten Modelle, die ja noch in saved_models in den Verzeichnissen encoder, synthesizer
und vocoder liegen.
TIPP: Achtung, wir werden nun die Modelle verändern. Sie sollten sie unbedingt 
wegspeichern, bevor Sie fortfahren, ganz konkret alle Dateien, die in den drei 
Komponenten unter saved_models zu finden sind.
Nun gilt es ebenso den Ordner E:\Datasets\SV2TTS zu sichern (zum Beispiel als SV2TTS_original) und 
einen neuen Ordner mit diesem Namen anzulegen. Wir gehen zurück zu Abschnitt 3.3.7 und 
starten das Preprocessing für den Synthesizer erneut. Denken Sie daran, dass Sie die Liste 
der Unterordner anpassen müssen, da wir im Moment nur einen Ordner male im Datensatz 
vorhalten.
python synthesizer_preprocess_audio.py E:\Datasets\ --subfolders de_DE\by_book\male 
--dataset "" --no_alignments --wav_dir
Führen Sie sequenziell die folgenden Befehle aus:
python synthesizer_preprocess_embeds.py E:\Datasets\SV2TTS\synthesizer --
encoder_model_fpath encoder/saved_models/my_run.pt
python synthesizer_train.py my_run E:\Datasets\SV2TTS\synthesizer



62 3 Erzeugung künstlicher Sprache
Das Training nimmt den Prozess aus dem bestehenden Modell my_run auf und trainiert die ses nun konkret auf unsere Samples. Sie können den Vorgang nach einigen 1000 Schritten 
abbrechen. Es gibt kaum einen Grund, länger als eine Stunde zu trainieren.
Führen Sie jetzt noch mal python demo_simple.py aus. Was sagen Sie zu dem Ergebnis? 
Beeindruckend oder? Die synthetisierte Sprache klingt nun perfekt nach unserer eigenen 
Stimme. Die Quintessenz aus unserem Experiment ist also, dass das Klonen einer Stimme 
nur in manchen Fällen mit einer kurzen Aufnahme klappt, bessere Resultate erzielt man mit 
einem kurzen Fine Tuning. Leider nimmt die Generierung über den Vocoder noch viel Zeit in 
Anspruch, was unseren Sprachassistenten langsam macht. Ob Sie das nun in Kauf nehmen 
oder bei der regelbasierten TTS-Engine aus Abschnitt 3.1 bleiben, bleibt Ihnen überlassen. 
Aber Moment, wir haben ja noch die Option, den Griffin-Lim-Algorithmus für die Konvertie rung des Mel Spectrograms zum WAV einzusetzen! Ersetzen Sie doch mal Zeile 41 bis 45 in 
Listing 3.4 durch die folgende Zeile:
generated_wav = Synthesizer.griffin_lim(spec)
Und? Damit erreichen wir fast eine Echtzeitgenerierung, wenn wir das Laden der Modelle 
außer Acht lassen! Dafür leidet aber die Audio-Qualität. Es ist also eine Abwägungssache, 
für was Sie sich entscheiden. Ich werde aufgrund der Geschwindigkeit im Sinne von better 
done than perfect zu Griffin-Lim greifen, jedoch beeinträchtigt diese Entscheidung unsere 
Weiterentwicklung nur marginal.
Wir betten nun im folgenden Teil den Deep-Learning-Ansatz in unsere TTS-Klasse ein und 
beenden das Kapitel damit. Zuvor wollen wir jedoch noch einen kurzen Ausflug in Richtung 
der Installation und Verwaltung mehrerer Abhängigkeiten machen.
3.3.11 Exkurs: Handhabung aller Modulabhängigkeiten
Wir werden in unseren kommenden Projekten mit immer mehr Modulen jonglieren müs sen, denn wir fügen ja von Mal zu Mal Funktionen hinzu, die in der Regel auf bestehenden 
Bibliotheken aufbauen, die Entwickler überall auf der Welt dankenswerter Weise für uns 
implementiert haben. Die Verwaltung der Abhängigkeiten ist allerdings sehr mühselig, ebenso 
die Installation, sollten wir sie alle nacheinander via pip installieren.
Glücklicherweise gibt es dafür mehrere Lösungen. Eine haben Sie bereits in Abschnitt 3.3.4
kennengelernt. Vielleicht entsinnen Sie sich, dass wir per conda env update und eine 
environment.yml eine Umgebung aufgebaut haben, in der das Voice Cloning bedingungslos 
lief, ohne dass wir etwas manuell nachinstallieren mussten. Die environement.yml enthält 
dabei alle Referenzen auf die benötigten Module samt ihrer Version, sodass Anaconda diese 
nach und nach für uns installieren kann. Eine Alternative, die ich in diesem Buch auch 
primär verwenden werde, da sie unabhängig von Anaconda ist, ist die Verwendung der 
sogenannten requirements.txt. Sie finden eine solche bereits im Beispiel 02_text_to_speech. 
Ein Blick hinein zeigt eine Liste aller installierter Module sowie deren genaue Version. Eine 
Installation erfolgt einfach über pip install -r requirements.txt. Probieren Sie es ruhig mal aus.
Eine requirements.txt zu erstellen ist ebenso kein Hexenwerk. Der Befehl, um die installierten 
Module auszugeben, ist pip freeze. Damit wird die Liste in der Konsole ausgegeben, Bild 3.19
zeigt ein exemplarisches Subset.



3.3 Trainieren einer eigenen TTS-Engine 63
Bild 3.19 Ausgabe der ersten sieben Module aus der aktivierten Umgebung
Möchten Sie die Module in eine Datei schreiben, genügt es, die Ausgabe über pip freeze > 
requirements.txt umzuleiten. Die Datei können Sie dann Ihren Projekten beilegen, sodass jeder 
eine identische Umgebung schaffen kann. Möchten Sie Module entfernen, können Sie das 
manuell in der geschriebenen Datei in einem beliebigen Texteditor tun, falls Sie beispiels weise Testmodule installiert haben, die Sie in Produktion nicht benötigen.
Und wie sieht es aus, wenn wir ein einzelnes Paket in einer bestimmten Version instal lieren möchten? Auch das ist kein Problem, so lässt sich loguru zum Beispiel über pip install 
loguru==0.5.2 installieren. Führt man diesen Befehl aus, wird loguru in der aktuellen 
Version deinstalliert (derzeit 0.5.3) und in 0.5.2 neu installiert.
Der Vollständigkeit halber sei noch pip uninstall loguru genannt, um das Modul zu 
deinstallieren. Diesen Befehl müssen Sie mit y bestätigen oder mit n ablehnen.
HINWEIS: Noch ein Hinweis zur Verwendung von PyTorch. Hier macht es meistens 
keinen Sinn, die Versionen in der requirements.txt anzugeben, solange PyTorch
oder pip nicht in der Lage sind, die richtige CUDA-Version zu ermitteln, die auf der 
jeweiligen Zielumgebung vorhanden ist. Ich werde im Weiteren davon ausgehen, 
dass Sie die Installation selbstständig vornehmen. Wir benötigen sie nämlich nur 
für den Text-To-Speech-Teil, der in diesem Kapitel implementiert wurde. Alle ande ren Abhängigkeiten werde ich jedoch gewissenhaft in der jeweiligen requirements.
txt der Kapitel pflegen.
3.3.12 Einbinden der Logik in die Text-To-Speech-Klasse
Da wir nun schon recht viele Themen in kurzer Zeit behandelt haben, möchte ich noch 
mal auf das Repository zu diesem Buch verweisen, in dem Sie auch im Ordner code/ 02_b_
text_to_speech_deep_learning die fertige Implementierung finden, die wir eben mühevoll 
erarbeitet haben.
Sie sollten dafür eine neue Umgebung anlegen, da wir viele Abhängigkeiten zusätzlich 
installieren, die wir für den Rest des Sprachassistenten nicht brauchen. Führen Sie also bei spielsweise conda create -n 02_b_text_to_speech_deep_learning python=3.8 aus, 
um dem nachzukommen, und installieren Sie alle Requirements wie eben beschrieben über 
pip install -r requirements.txt und schieben Sie die PyTorch-Version nach, die Ihrer 
CUDA-Version entspricht. Wir erinnern uns, dass diese auf Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichpytorch.org/get-started/
previous-versions zu finden ist und wir Version 1.7.1 installiert haben.



64 3 Erzeugung künstlicher Sprache
Kommen wir direkt zur Bereitstellung der Modelle. Ich habe in dem Beispiel 02_b lediglich 
die Ordner encoder, synthesizer, utils und vocoder aus dem Voice Cloning-Repository herüber kopiert. Falls Sie über ein eigenes Modell verfügen, müssen Sie dieses in den bekannten 
saved_models-Ordnern platzieren. Damit sind die Vorbereitungen schon abgeschlossen und 
wir können uns die neue TTS.py in Listing 3.5 (in gekürzter Form) anschauen.
Listing 3.5 Die Voice-Klasse auf Basis des Deep-Learning-Ansatzes
1. class Voice:
2.
3. def __init__(self, fast_vocoding=True):
4. self.process = None
5. self.griffin_lim = fast_vocoding
6.
7. self.enc_model_fpath = Path("encoder/saved_models/my_run.pt")
8. self.syn_model_fpath = Path("synthesizer/saved_models/my_run/my_run.pt")
9. self.voc_model_fpath = Path("vocoder/saved_models/my_run/my_run.pt")
10.
11. if self.enc_model_fpath.exists():
12. encoder.load_model(self.enc_model_fpath)
13. else:
14. logger.warning("Encoder-Modell existiert nicht.")
15.
16. if self.syn_model_fpath.exists():
17. self.synthesizer = Synthesizer(self.syn_model_fpath)
18. else:
19. logger.error("Synthesizer-Modell existiert nicht.")
20.
21. if self.voc_model_fpath.exists():
22. vocoder.load_model(self.voc_model_fpath)
23. else:
24. logger.warning("Vocoder-Modell existiert nicht.")
25.
26. def __speak__(self, text):
27. texts = [text]
28. embeds = [[0] * 256]
29.
30. if not self.syn_model_fpath.exists():
31. logger.error("Synthesizer-Modell ist nicht geladen. TTS fehlgeschlagen.")
32. return
33.
34. specs = self.synthesizer.synthesize_spectrograms(texts, embeds)
35. spec = specs[0]
36.
37. if not self.griffin_lim and self.voc_model_fpath.exists():
38. generated_wav = vocoder.infer_waveform(spec)
39. generated_wav = np.pad(generated_wav, (0, self.synthesizer.sample_rate),
40. mode="constant")
41.
42. if self.enc_model_fpath.exists():
43. generated_wav = encoder.preprocess_wav(generated_wav)
44. else:
45. logger.warning("Kann Ausgabe nicht bereinigen.")



3.3 Trainieren einer eigenen TTS-Engine 65
46. else:
47. if not self.voc_model_fpath.exists():
48. logger.warning("Vocoder-Modell existiert nicht. Verwende Griffin-Lim.")
49. generated_wav = Synthesizer.griffin_lim(spec)
50.
51. audio_length = librosa.get_duration(generated_wav, sr =
52. self.synthesizer.sample_rate)
53. sd.play(generated_wav.astype(np.float32), self.synthesizer.sample_rate)
54. time.sleep(round(audio_length))
55.
56. def say(self, text):
57. if self.process:
58. self.stop()
59. p = multiprocessing.Process(target=self.__speak__, args=(text,))
60. p.start()
61. self.process = p
62.
63. def set_voice(self, voiceId):
64. pass
65.
66. def stop(self):
67. if self.process:
68. self.process.terminate()
69.
70. def get_voice_keys_by_language(self, language=''):
71. return []
Sie erkennen wahrscheinlich einen Großteil aus der demo_simple.py-Anwendung wieder. 
In der Initialisierungsmethode laden wir die Modelle, sofern sie denn existieren. Achten 
Sie auf eine saubere Fehlerbehandlung, da viel schiefgehen kann, wenn die Modelle nicht 
geladen werden. Wenn Vocoder und Encoder nicht geladen werden können, wird das Modul 
dennoch funktionieren, weswegen ich nur eine Warnung ausgebe. Existiert jedoch kein 
Synthesizer, funktioniert die komplette Sprachwiedergabe nicht. In diesem Fall kommt es 
zu einer Fehlermeldung.
Schauen wir uns als Nächstes die Methode say() an, die, wie auch bei der Verwendung von 
pyttsx3, einen neuen Prozess startet. Sehen Sie hier jedoch die Besonderheit. Ich habe dies mal, des Lerneffekts wegen, die aufzurufende Methode __speak__() als Klassenmethode 
deklariert. Der Aufruf über multiprocessing.Process() ist aber identisch, außer dass vor 
dem Methodennamen ein self steht.
__speak__() selber durchläuft nun den bekannten Prozess: Es wird ein neutrales Embedding 
angelegt, der Synthesizer erstellt ein Mel Spectrogram für einen vorgegebenen Text und 
der Vocoder, entweder auf Basis unseres Modells oder des Griffin-Lim-Algorithmus, erzeugt 
daraus eine Wellenform. Diese wird dann über sounddevice.play() wiedergegeben.
Da wir set_voice() und get_voice_keys_by_language() zuvor implementiert haben, 
habe ich diese Methoden auch hier hinzugefügt, jedoch lediglich als Fassade stehengelassen. 
stop() unterbricht in gewohnter Manier die Wiedergabe, indem es den Prozess terminiert.
Damit haben wir unser Ziel erreicht und können den Sprachassistenten mit unserer eigenen 
Stimme sprechen lassen. Wenden wir uns nun dem Sprachverständnis zu.






4 Spracherkennung
Spracherkennung ist der Prozess, ein Sprachsignal in eine möglichst genaue Sequenz von 
Wörtern zu transformieren. Wikipedia fügt hinzu, dass sich Spracherkennung in Sprecher und Stimmerkennung untergliedert (Wikipedia, 2021). Das deutet darauf hin, dass das Feld 
nicht eindeutig definiert ist, weswegen ich gerne eine eigene Definition für drei Teilbereiche 
geben möchte, die von Bezeichnung und Inhalt nahe beieinanderliegen:
 Spracherkennung bezeichnet den Prozess, gesprochene Sprache in Text umzuwandeln.
 Sprechererkennung ist das Vorgehen, einen Sprecher eindeutig aus einer Sprachaufnahme 
anhand ihrer oder seiner Stimme zu identifizieren.
 Sprachenerkennung beschäftigt sich mit der automatisierten Erkennung einer gesprochenen 
Sprache aus Text oder Audioaufnahmen.
Wir werden uns im praktischen Teil vorrangig mit der Spracherkennung und der Sprecher erkennung beschäftigen. Die Sprachenerkennung ist auch ein spannendes Feld, bei dem wir 
kommerziellen Sprachassistenten einen Schritt voraus wären (die meisten beherrschen nur 
eine Sprache zur selben Zeit), jedoch wäre das eher der Kür statt der Pflicht zuzuordnen.
Für die Spracherkennung gibt es seit dem letzten KI-Sommer unzählige neue Ansätze und 
Möglichkeiten, um eine immer höhere Präzision bei dieser Transformation von Sprache zu 
Text zu erzielen, denn Sprache ist immer noch das bevorzugte Kommunikationsmedium des 
Menschen und jede Technologie und Methodik, diese zu verbessern, wird von Forschung 
und Industrie dankbar aufgenommen. Aber Hand aufs Herz, wir nutzen Sprache bei der 
Interaktion mit dem Computer immer noch sehr wenig. Für Alexa und Co. gilt das natürlich 
nicht, auch beim Umgang mit dem Smartphone kommt Sprache viel häufiger zum Einsatz. 
So spreche ich selber lieber mit meinem Navigationsgerät, als dass ich mich umständlich 
über die Armatur des Autos beuge und auf viel zu kleinen Tasten lange Straßennamen tippe, 
von denen ich bei der Hälfte nicht weiß, wie man sie schreibt.
HINWEIS: In der Geschichte der künstlichen Intelligenz gibt es immer mal wieder 
Perioden, in denen die Entwicklung große Fortschritte macht. Als ich damals 2006 
mein Informatikstudium begonnen hatte, war es in der KI-Vorlesung undenkbar, 
dass es mal einen Chatbot gibt, von dem man nicht sagen kann, ob er menschlich 
ist oder ob man sich mit einem Computer unterhält. Für genau diesen Fall entwarf 
Alan Turing 1950 sogar einen Test, den Turing-Test (der eigentlich Imitation Game
hieß, so wie die gleichnamige Filmbiographie mit Benedict Cumberbatch). Hier 
wurde eine Person vor zwei Computer gesetzt und sollte mit dem nicht sichtbaren 
Gegenüber via Tastatur kommunizieren. Sollte diese Person nach einiger Zeit



68 4 Spracherkennung
nicht sagen können, an welchem anderen Ende der Mensch saß und an welchem 
der Computer, galt der Test als bestanden. Selbst die bekanntesten Chatbots wie 
Joseph Weizenbaums ELIZA konnten dem Test nur kurze Zeit standhalten. Nach 
einigen periodischen Fortschritten in den Folgejahren ebbte jedes Mal der Hype 
um die KI ab, meistens dann, wenn Fortschritte stagnierten und neue Forschungs felder entstanden und mehr Aufmerksamkeit bekamen. Dann sprach man von 
einem KI-Winter, wie er in Bild 4.1 im Jahre 1973 und 1988 zu sehen ist. Besagte 
Grafik hat beileibe keinen Anspruch auf Vollständigkeit und ist sicher eher als 
populärwissenschaftlich zu bezeichnen, da es für wirkliche KI-Wissenschaftler 
wesentlich interessantere, vielfältigere und bahnbrechendere Entwicklungen gibt 
(alleine die verschiedenen Architekturen von neuronalen Netzen der letzten Jahre 
verdienten eine eigene Grafik); dennoch stellt es die Perioden der KI-Sommer und 
-Winter dar, was die eigentliche Intention war.
Turing Test (1950)
1950 1970 1980 1990 2010
Deep Learning
Revoluon (2012)
Terminus „Arficial
Intelligence“
entsteht (1956)
1. KI-Winter 
(1973)
Fokusänderung:
generelle KI  modulare 
Expertensysteme (1981)
Erstes selbsahrendes
Auto (1986)
Schachcomputer Deep Blue
gegen Kasparov (1997)
1960 2000
2. KI-Winter 
(1988)
Weizenbaums ELIZA (1964)
Boston Dynamics
Roboterentwicklung
(2005)
Release von 
Alexa und
Turing Test
bestanden 
(2014) Erster Staubsaugerroboter
Roomba (2002)
Bild 4.1 Einige Meilensteine in der Entwicklung der künstlichen Intelligenz von 1950 bis 
2015 und die Kennzeichnung von KI-Sommer- und -Winterperioden (Amritendu, 2020), 
(Marsden, 2017), (Schuchmann, 2019)
Klar ist, dass wir uns derzeit noch in einer Sommerperiode befinden, auch wenn 
manche Medien bereits von einem KI-Herbst sprechen. Denn wir sind immer noch 
dabei, auf Basis der in den letzten Monaten entstandenen Fortschritten, zum Beispiel in 
den Architekturen neuronaler Netze oder der Parallelisierung von Trainings, neue 
und altbekannte Probleme zu lösen und die Lösungsfindung zu optimieren. Ein 
Beispiel für eine neue Architektur sind Stand 2021 etwa die Transformer, die be sonders im Bereich NLP (Natural Language Processing) zu bahnbrechenden Fort schritten, etwa in Übersetzung, Textgenerierung oder der Textzusammenfassung, 
beitragen. Ein Beispiel für Optimierungsmethoden hingegen ist die Destillation von 
Modellen, in denen ein kleineres Modell auf Basis eines größeren Modells nach trainiert wird, um einen Transfer des wesentlichen Wissens von groß nach klein zu 
ermöglichen. Das wiederum ermöglicht, Modelle auf kleineren Devices, wie dem 
Raspberry Pi oder einem Smartphone, anzuwenden.



4 Spracherkennung 69
Wir kommen also in der Wissenschaft und experimentellen Umsetzung weiter 
voran. Jedoch sind Implementierungen in der Industrie tatsächlich noch nicht so 
umfassend vorhanden, wie meist behauptet wird. Die Transferleistung von der For schung zum wirklichen, praktischen und robusten Einsatz von KI-Modellen muss 
noch erbracht werden. Bisher schaffen das nur die wenigsten, und von denen sind 
es vor allem die Big Player am Markt: Google, Amazon und Microsoft.
Kein Wunder, liegen ihnen doch Daten aus verschiedensten Domänen wie dem 
Finanzwesen, Social Media oder Forschung und Entwicklung in allen Sprachen der 
Welt vor. Diese können sie hervorragend nutzen, um ihre Modelle stetig zu verbes sern. Und statt die streng gehüteten Fähigkeiten wie früher in Form von Beratung 
oder Software zu vertreiben, werden KI-Services abgeschottet in der eigenen 
Cloud gehostet, sodass kleine Unternehmen mit sehr generischen Usecases kaum 
einen Bedarf haben, selber Kompetenz in der Entwicklung und im Betrieb von 
Machine-Learning-Anwendungen aufzubauen.
Natürlich ist das aus Sicht jedes Experten grob fahrlässig, bedenkt man, dass man 
schon in einfachen Modellen kaum nachvollziehen kann, wie es zu einer Entschei dung kommt, von großen Netzen ganz zu schweigen. Es wird damit die so häufig 
geforderte digitale Souveränität aus der Hand gegeben: Wir wissen nicht mehr, 
wie Technologien funktionieren, auf welcher Basis Entscheidungen fallen und wir 
können diese Entscheidungen kaum beeinflussen. Kein schönes Szenario, aber 
eines, dem wir theoretisch leicht begegnen könnten. Wir müssen es nur tun, zum 
Beispiel indem wir selber Wissen aufbauen und Technologien schaffen.
Um die Schwarzmalerei zu beenden, sei gesagt, dass es natürlich auch rühmliche 
Ausnahmen gibt. Besonders Start-Ups stellen häufig Modelle und Code offen 
zu Verfügung und lehren, wie diese zu nutzen und auf die eigenen Bedürfnisse 
anzupassen sind. Besonders hervorzuheben ist hier Hugging Face, eine in Paris 
und New York angesiedelte Firma, die sich selbst als AI Community versteht und 
Hunderte von Modellen aus dem NLP-, Audio- und Computer-Vision-Bereich sam melt, hostet und mittels einfacher APIs (Application Programming Interfaces) on premises nutzbar macht. Natürlich sind auch all die individuellen Entwickler und 
Entwicklerinnen zu erwähnen, die ihre Projekte auf Plattformen wie github.com
oder modelzoo.co frei zu Verfügung stellen.
Warum diese ausschweifende Diskussion? Ich bin der festen Überzeugung, dass 
besonders offene Entwicklungen einen Einfluss darauf haben, ob der KI-Sommer 
noch anhält oder bald schon endet; ganz im Gegensatz zu den letzten Perioden, 
die große Unternehmen und Universitäten bestimmt haben. Fortschritt wird mitt lerweile an allen Ecken und Enden der Welt erzielt, denn der Zugang zu Wissen 
und Technologie ist gegeben. Und die Menschen sind bereit zu teilen, sei es aus 
einem Gemeinschaftsgedanken heraus, aus Gründen der Selbstpromotion oder 
um Feedback und Unterstützung von anderen zu erhalten. Leben wir nicht in 
einem tollen Zeitalter?
Eine Dimension, die in Bild 4.1 nicht berücksichtigt, aber dennoch erwähnenswert 
ist, ist die räumliche Verteilung der Forschung, sprich es sagt nichts darüber aus, 
wo auf der Welt Fortschritte erzielt werden. Häufig wird KI-Entwicklung mit China, 



70 4 Spracherkennung
den USA oder Kanada in Verbindung gebracht. Lassen Sie uns aber mal ganz nüch tern die Zahlen betrachten, die Gleb Chuvpilo für eine Statistik mühevoll zusam mengetragen hat (Chuvpilo, 2020), indem er Publikationen, speziell deren Autoren, 
der International Conference on Machine Learning (ICML 2020) und der Neural 
Information Processing Systems (NeurIPS 2020) evaluiert hat (siehe Bild 4.2).
0 500 1000 1500 2000
Summe der Publikaonen
Israel
Schweiz
Deutschland
Frankreich
Kanada
Großbritanien
China
USA
0246 8 10 12
Publikaonen nach
Einwohnerzahl
Israel
Schweiz
Deutschland
Frankreich
Kanada
Großbritanien
China
USA
Bild 4.2 Publikationen im Bereich KI auf ICML 2020 und NeurIPS 2020 nach Ländern
Hier ist zu sehen, dass deutsche Forscherinnen und Forscher insgesamt gar nicht 
so schlecht dastehen, sind sie doch auf Platz 6 der Top-50 aller Länder zu finden. 
Vor uns liegen mit sehr großem Vorsprung die USA sowie China, dann Großbritan nien, Kanada und Frankreich. Deutschland folgt die Schweiz, Südkorea, Japan und 
Israel. Eine weitere interessante Betrachtung stellt Chuvpilo in der unteren Grafik 
an, in der die Publikationen nach Einwohnerzahl ermittelt werden. Hier führen die 
Schweiz und Israel die Spitze an, gefolgt von Singapur und den USA. Deutschland 
ist erst auf dem 14. Platz zu finden. Werfen wir noch einen Blick auf die Univer sitäten und Unternehmen, unter deren Schirmherrschaft die Veröffentlichungen 
stattfinden (siehe Bild 4.3).
Hier gehen die ersten fünf Plätze in die USA. Besonders Google sticht heraus, 
gefolgt von den Universitäten in Stanford, Massachusetts und Berkeley. Auch hier 
ist wieder die Schweiz, die mit der ETH in Zürich und der EPFL in Lausanne auf 
Platz 12 und 13 zu finden ist, hervorzuheben. Die Universität Tübingen ist der erste 
Vertreter aus Deutschland auf Platz 57, die nächste die TU München auf Platz 84.
Eine Klassifizierung dieser Platzierungen mit Blick auf Deutschland und Europa 
vorzunehmen, möchte ich mir hier ersparen, haben Sie doch nun ein Mittel, dies 
selbst zu tun. Wichtig ist mir jedoch zu hinterfragen, was wir tun könnten, um uns



4 Spracherkennung 71
0 50 100 150 200 250
Publikaonen der Unternehmen und 
Universitäten
TU München
Universität Tübingen (D)
INRIA (Frankreich)
Harvard University (USA)
EPFL (Schweiz)
ETH Switzerland
Tsinghua University (China)
Facebook
Microso
UC Berkeley
MIT
Stanford University
Google
Bild 4.3 KI-Publikationen nach Unternehmen und Universitäten weltweit
besser zu positionieren. Unsere Regierung hat in den letzten Jahren ein KI-Stra tegiepapier1
 veröffentlicht, das das Ziel kommuniziert, KI Made in Germany an 
die Weltspitze zu bringen. Ein schöner Plan, denn dann gäbe es mehr Austausch, 
mehr Expertise in den Unternehmen und hoffentlich mehr Verwendung von künst licher Intelligenz in den Produkten, die wir jeden Tag nutzen. Diese Umsetzung 
lediglich zu fordern, reicht aber nicht. Aus meiner Sicht müssen dazu im Wesent lichen folgende Maßnahmen umgesetzt werden:
 Offen zugängliche und qualitätsgesicherte Datensätze müssen bereitgestellt 
werden, um Modelle darauf zu trainieren. Prädestiniert wäre dafür das statis tische Bundesamt, jedoch müssten die Daten kostenlos, in größerem Umfang, 
nicht aggregiert und maschinenlesbar bereitgestellt werden.
 Forschung muss zugelassen und verstanden werden. Das Schaffen von Regu larien und Ethikregeln ist wichtig, es darf jedoch die KI-Forschung nicht von 
vornerein behindern oder gar verteufeln. Dafür ist es wichtig, Experten an den 
Stellen zu platzieren, an denen Entscheidungen getroffen werden, die das Feld 
voranbringen.
 Es muss Grundlagenforschung betrieben werden. Nur eine High Level Innovation
nach der anderen anzustreben (siehe Anwendungsfälle im KI-Strategiepapier) ist 
nicht der Weg, um im Bereich KI führend zu werden, denn dann wenden wir nur 
an, was andere erforschen, und verpacken es nett.
Zugegeben, die letzten Punkte driften stark ins Politische ab. Es ist mir jedoch ein 
persönliches Anliegen, dass wir als Experten die Chance haben, das kostbare Wis sen, das wir uns über Jahre erarbeitet haben, auch umsetzen können und dürfen. 
Ein weiser Mann (mein Elektriker) sagte mir einst: Wir wollen Teil der Lösung sein, 
nicht des Problems. Lassen Sie uns also Teil der Lösung sein und überall da für 
das Verständnis und die Nöte der KI-Forschung werben und sorgen, wo sich uns 
die Möglichkeit bietet.
1 www.ki-strategie-deutschland.de



72 4 Spracherkennung
Lassen Sie uns im nächsten Abschnitt kurz anschauen, wie der Prozess der Spracherkennung 
exemplarisch aussehen kann. Warum exemplarisch? Na ja, es gibt viele verschiedene An sätze und Herangehensweisen, um analoge Sprache in digitalen Text umzuwandeln. Einige 
Schritte in diesem Prozess sind aber häufig sehr ähnlich.
Der erste Schritt ist, ein analoges Signal in ein digitales umzuwandeln, mit dem Ziel, ein 
Spektrogramm zu erzeugen. Dazu wird, wie in Bild 4.4 a) gezeigt, eine Wellenform geladen, 
die als Amplitude über die Dauer einer Aufnahme visualisiert werden kann. Diese Amplitude 
wird nun in mehrere kleine Blöcke mit einer Länge von einigen Millisekunden eingeteilt, 
sodass jeder Block einem exakten numerischen Wert zugeordnet werden kann.
a) b) c)
Amplitude (db)
Magnitude
Frequenzband
Zeit (s) Frequenz Zei enster
Bild 4.4 Darstellung einer Audiodatei als Amplitude über die Zeit (a), als Fast Fourier Transformation 
(b) und als Spektrogramm (c).
Um eine Spracherkennung durchführen zu können, benötigen wir allerdings nicht konkret 
die Amplitude, sondern diese drei Kennzahlen:
 Frequenz,
 Intensität,
 und die Zeit, um diese beiden Werte zu erzeugen.
Dabei hilft uns die Fast Fourier Transformation (FFT), die die Amplitude in ein Set dieser drei 
Werte wandelt. Eine häufige Darstellung des Resultats einer FFT sehen Sie in Bild 4.4 b), in 
dem für dieselbe Audiodatei die Frequenz über der Magnitude dargestellt wird.
TIPP: Eine schöne Analogie zur Funktionsweise und Verwendung der FFT, die ich 
mehrmals gelesen habe, lautet wie folgt:
Was tut eine Fourier Transformation? Nehmen wir an, Sie hätten einen Smoothie 
vor sich. Die Fourier Transformation ermittelt dessen Rezeptur.
Wie? Der Smoothie wird durch verschiedene Filter gegeben, um jede Zutat einzeln 
herauszufiltern.
Warum? Rezepte sind einfacher zu analysieren, zu vergleichen und zu modifizieren 
als der Smoothie selbst.
Wie können wir unseren Smoothie zurückbekommen? Wir vermischen einfach wieder 
die einzelnen Inhaltsstoffe.



4 Spracherkennung 73
Im letzten Schritt wird das Spektrogramm erzeugt, das wir ja schon zur Genüge im vorigen 
Kapitel besprochen haben. Auf der y-Achse von Bild 4.4 c) ist die Frequenz zu sehen, die 
x-Achse zeigt die Zeitfenster, die betrachtet werden. Diese können unterschiedliche Längen 
haben und werden mitunter als Acoustic Frame bezeichnet. Die farbliche Ausprägung stellt 
die Amplitude dar.
Nun hätte ich ein schlechtes Gewissen, wenn ich hier nur trockene Theorie vermitteln würde. 
Also schauen wir uns kurz an, wie man die ersten drei Schritte in Python umsetzen würde. 
Listing 4.1 zeigt die einzelnen Schritte, Sie müssen aber zuvor librosa, matplotlib und scipy
über pip installieren.
Listing 4.1 Zeichnen der Amplitude, der Magnitude der Frequenzen und des Spektrogramms
1. import librosa
2. from librosa import display
3. import scipy
4. import matplotlib.pyplot as plt
5. import numpy as np
6. from scipy import signal
7.
8. if __name__ == "__main__":
9.
10. # Laden der Audiodatei
11. file_path = "test.wav"
12. samples, sampling_rate = librosa.load(file_path)
13. print("# Samples: " + str(len(samples)) + ", Sampling Rate: " +
14. str(sampling_rate))
15.
16. # Berechnen der Audiolänge
17. duration = len(samples) / sampling_rate
18. print("Audiolänge: " + str(duration))
19.
20. # Darstellung der Amplitude über die Zeit
21. plt.figure()
22. librosa.display.waveplot(y = samples, sr = sampling_rate)
23. plt.xlabel("Zeit (s)")
24. plt.ylabel("Amplitude (db)")
25. plt.show()
26.
27. # FFT mit Hilfe von Scipy
28. n = len(samples)
29. T = 1 / sampling_rate
30. yf = scipy.fft.fft(samples)
31. xf = np.linspace(0.0, 1.0 / (2.0*T), n // 2)
32. fig, ax = plt.subplots()
33. ax.plot(xf, 2.0/n * np.abs(yf[:n//2]))
34. plt.grid()
35. plt.xlabel("Frequenz")
36. plt.ylabel("Magnitude")
37. plt.show()
38.
39. # Ableiten des Spektrogramms über Scipy
40. frequencies, times, spectrogram = signal.spectrogram(samples, sampling_rate)



74 4 Spracherkennung
41. plt.specgram(samples,Fs=sampling_rate)
42. plt.title('Spectrogram')
43. plt.ylabel('Frequenzband')
44. plt.xlabel('Zeitfenster')
45. plt.show()
Nehmen Sie eine kurze Audiodatei auf, speichern Sie sie als test.wav im selben Ordner, in 
der auch die Python-Datei liegt, und rufen Sie diese auf. Sie sehen schnell, dass die Biblio theken uns den Großteil der Arbeit abnehmen und dass die FFT und das Erzeugen eines 
Spektrogramms ihren Weg in Scipy, eine der bekanntesten wissenschaftlichen Bibliotheken 
Pythons, gefunden haben, zeigt, dass wir einen Weg gehen, der weitestgehend standardisiert 
ist. Die Anwendung in Listing 4.1 zeigt Ihnen nacheinander drei Grafiken. Sie müssen jedes 
Fenster schließen, um das nächste angezeigt zu bekommen. Sie werden sehen, dass diese 
Bild 4.4 ähneln.
Nun halten wir das Spektrogramm in Händen, Zeit einen weiteren alten Bekannten zu treffen: 
Das Phonem, das als ein Baustein eines gesprochenen Wortes gilt, ist in etwa so lange wie ein 
Acoustic Frame. Die Aufgabe besteht nun darin, diese Phoneme in die richtige Reihenfolge 
und den richtigen Kontext zu bringen, um daraus Worte zu formen. Hier gibt es zwei An sätze, die ich Ihnen vorstellen möchte: Hidden Markov Modelle (HMM) und neuronale Netze.
Das HMM ist in der Lage, die aufeinanderfolgenden Worte auf Basis statistischer Wahrschein lichkeiten zu ermitteln. Nehmen wir uns einen Beispielsatz Goldfische sind pflegeleicht. Der 
Prozess, die dazugehörigen Phoneme zu identifizieren und richtig anzuordnen, ist in drei 
Schritte, auch Ebenen genannt, eingeteilt:
1. Es wird gemäß einer einfachen Wahrscheinlichkeitstabelle der erste Buchstabe gewählt, 
den das Phonem abbildet. Das kann hier ein G oder ein K sein. Nehmen wir einmal an, 
dass die Wahrscheinlichkeit, dass ein Wort mit G beginnt bei 0,6 liegt und bei K bei 0,4, 
dann wird entsprechend das G gewählt.
2. In einem zweiten Schritt prüft das HMM die Wahrscheinlichkeit der Benachbarung zweier 
Phoneme. So wird anhand einer Tabelle geprüft, welche Buchstaben, die dem folgenden 
Phonem am ähnlichsten sind, höchstwahrscheinlich einem G folgen. Wir nehmen an, dass 
die Wahrscheinlichkeit des Buchstaben o am höchsten ist. Für die nächsten zwei Schritte 
werden die Buchstaben l und d als statistisch am wahrscheinlichsten betrachtet. Da das 
entstandene Wort Gold im Sprachgebrauch vorhanden ist, wird nun die nächste Ebene 
aktiviert.
3. In der dritten Ebene des HMM prüft das Modell auf der Wortebene, wie wahrscheinlich 
es ist, dass zwei Wörter nebeneinanderstehen. Nehmen wir an, dass wir auch das zweite 
Wort Fische durch Ebene 1 und 2 ermittelt haben, so beginnt unserer Satz mit Gold und 
Fische, wobei Fische natürlich auch ein konjungiertes Verb sein kann. Also geht der Pro zess einen Schritt zurück und evaluiert erneut, ob die Wahrscheinlichkeit hoch ist, dass 
das Wort Gold noch nicht final ist und stellt fest, dass es offensichtlicher ist, dass Gold fische gemeint sind. Einer identischen Situation sehen wir uns bei dem Wort pflegeleicht
ausgesetzt. Auch hier wird zunächst Pflege als Nomen oder Verb identifiziert und dann 
festgestellt, dass dem Verb wahrscheinlich ein Adjektiv wie pflegeleicht folgen muss, das 
denselben Phonemen entspricht wie pflege und leicht.



4 Spracherkennung 75
Somit sind wir in der Lage, das Spektrogramm Phonem für Phonem zu traversieren, um die 
korrekte Wortfolge des gesprochenen Satzes zu ermitteln. Allerdings hat das HMM einen 
Nachteil: Die Vielfältigkeit der Phoneme, die durch neue Worte oder zum Beispiel durch Anglizismen 
entsteht, macht die Erkennung häufig ungenau, Phoneme müssen umständlich nachgetragen 
und indiziert und Wahrscheinlichkeiten aktualisiert werden.
Schauen wir uns doch mal den zweiten Ansatz an, der auf neuronale Netze aufbaut. In der 
Disziplin können wir uns ja schon als alte Hasen betrachten, haben wir ja im vorigen Kapitel 
bereits drei davon trainiert. Dennoch ein paar Worte dazu. Wir haben ja gesehen, dass die 
Sprachsynthese sehr datenintensiv in der Trainingsphase ist. Das Gleiche gilt auch für die 
Spracherkennung, möchte man doch alle möglichen Laute abdecken und möglichst viele 
Variationen in das Zielmodell einfließen lassen. Denken Sie einmal an all die Dialekte, die 
wir alleine in Deutschland haben. Ein neuronales Netz, wie es exemplarisch in Bild 4.5 zu 
sehen ist, besteht aus einer Eingangs- und einer Ausgangsschicht und einer verschiedenen 
Anzahl von sogenannten Hidden Layers dazwischen. Wie in Kapitel 3 beschrieben, werden 
nun die Gewichte zwischen den einzelnen Neuronen nach und nach angepasst, bis der Fehler 
am Ende so weit minimiert ist, dass das Ergebnis unseren Erwartungen entspricht.
Input 
Layer
Output
Layer
Hidden 
Layer 1
0,1
0,4
0,6
… …
0,5
0,9
Hidden
Layer 2
Hidden 
Layer 3
Main
Training
Data
Fine Tuning
Training
Data
0,2
0,6
0,7
0,3
0,6
„Ja, grad se
läd!“ „Ja,
Krautsalat“
„Ja, mit 
Absicht!“
Bild 4.5 Ein neuronales Netz für die Erkennung von Phonemen kann auf Basis neuer Audiodateien 
korrigiert werden, um etwa Dialekte zu erkennen.
Warum kauen wir das ganze nun erneut durch? Erinnern Sie sich daran, wie einfach wir 
unser Synthesizer-Modell für unsere eigene Stimme nachtrainieren konnten? Das Gleiche gilt 
auch hier! Bekommen wir die Rückmeldung, dass zum Beispiel ein Dialekt in der Erkennung keine 
ausreichend hohe Präzision erzielt, so beschaffen wir uns einige hundert Sprachdateien 
und trainieren das Netz nach. Das Beispiel, das Bild 4.5 zeigt, beschreibt, wie der Satz Ja, 
grad se läd! von einem Modell nicht erkannt wird. Klar, ist es ja pfälzische Mundart und für 
einen Laien nicht ohne Wörterbuch zu verstehen. Das Modell erkennt den Satz nicht und 
wählt als Rückgabe die Zeichenkette, die dem eingegebenen Klang am ähnlichsten ist und 
das ist erfahrungsgemäß Krautsalat. Um das Modell zu optimieren, ist es nötig, für das Fine 



76 4 Spracherkennung
Tuning Daten hinzuzuführen, die den Dialekt und die dazugehörigen, korrekten Phoneme 
aufweisen. Damit wird beim nächsten Aufruf des Modells womöglich eine richtige Erkennung 
durchgeführt und aus Ja, Krautsalat! wird die Vorhersage Ja, mit Absicht!.
Damit sind wir viel flexibler als das HMM, denn das Retraining oder Fine Tuning ist kaum 
aufwendiger, als ein paar Befehle im Terminal abzusetzen. Keine Angst, wir werden jetzt 
kein neues Modell trainieren (vielleicht sind Sie ja auch enttäuscht, dass wir es nicht tun), 
denn es gibt bereits einige hervorragende Modelle, die wir nutzen können.
Lassen Sie uns stattdessen einen Blick darauf werfen, wie ermittelt wird, ob eine Wort folge statistisch wahrscheinlich ist, denn das haben wir zuvor nicht in der Tiefe betrachtet. 
Listing 4.2 zeigt eine Implementierung mit PyTorch, Hugging Face Transformers und einem 
GPT-2 (Generative Pre-trained Transformer)-Modell, das die statistischen Zusammenhänge 
vieler verschiedener Texte beinhaltet. Installieren Sie vor der Verwendung torch und trans formers über pip.
Listing 4.2 Berechnung der Perplexität eines Satzes über Language Models
1. import torch
2. from transformers import AutoTokenizer, AutoModelWithLMHead
3.
4. def sent_scoring(model, tokenizer, text, cuda):
5. input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)
6. if cuda:
7. input_ids = input_ids.to('cuda')
8. with torch.no_grad():
9. outputs = model(input_ids, labels=input_ids)
10. loss, _ = outputs[:2]
11. perplexity = loss.item()
12. return perplexity
13.
14.
15. if __name__ == '__main__':
16. # Initialisiere Modell
17. tokenizer = AutoTokenizer.from_pretrained("dbmdz/german-gpt2")
18. model = AutoModelWithLMHead.from_pretrained("dbmdz/german-gpt2")
19. model.eval()
20. cuda = torch.cuda.is_available()
21. if cuda:
22. model.to('cuda')
23.
24. # Berechne Score
25. print(sent_scoring(model, tokenizer, "Goldfische sind pflegeleicht.", cuda))
26. print(sent_scoring(model, tokenizer, "Gold Fische sind pflegeleicht.", cuda))
Als Beispielsatz nehmen wir wieder einmal Goldfische sind pflegeleicht. und berechnen das 
Scoring über das Loss. Die Ausgabe, die Sie bekommen, ist keinesfalls die Wahrscheinlich keit, sondern die Perplexität, also die Unsicherheit oder Verwirrung, die beim Modell bei der 
Betrachtung dieses Satzes entsteht. Ein niedrigerer Wert entspricht also einer niedrigeren 
Verwirrung, was positiv zu bewerten ist.



4 Spracherkennung 77
HINWEIS: In Zeile 10 von Listing 4.2 sehen Sie ein Konstrukt, dass ich kurz erklä-
ren möchte. Ein wunderbares Feature der Sprache Python ist, dass eine Funktion 
mehrere Rückgabewerte haben kann. Nun kann es aber der Fall sein, dass man 
einen oder mehrere gar nicht benötigt. Ist dem so, kann der Rückgabewert, der 
nicht benötigt wird, durch einen Unterstrich statt durch einen Variablennamen 
repräsentiert werden. Es erfolgt dementsprechend keine Zuweisung.
Natürlich sind die tatsächlich eingesetzten Methoden noch etwas komplexer und betrachten 
auch die Satzzusammenhänge, etwa über das sogenannte POS Tagging (Part-of-speech Tagging), 
über das sich ermitteln lässt, welche Art Wort an welcher Position eines Textes vorkommt. 
Das klingt schwerer, als es ist. Die Bibliotheken SpaCy und NLTK bieten dafür vorgefertigte 
Funktionen für viele verschiedene Sprachen.
Listing 4.3 POS Tagging und Visualisierung der Wortbeziehungen eines Satzes
1. import spacy
2. from spacy import displacy
3.
4. nlp = spacy.load("de_core_news_sm")
5. doc = nlp("Goldfische sind pflegeleicht.")
6. displacy.serve(doc, style="dep")
Um Listing 4.3 auszuführen, müssen sie spacy per pip installieren, gefolgt von dem Befehl 
python -m spacy download de_core_news_sm, um das deutschsprachige Modell für die 
Analyse herunterzuladen. displacy.serve startet dann einen kleinen, lokalen Webserver 
auf Port 5000, sodass Sie in Ihrem Browser localhost:5000 aufrufen können, um die Visua lisierung angezeigt zu bekommen (siehe Bild 4.6).
Goldfische
NOUN
sind
AUX
pflegeleicht.
ADV
sb pd
Bild 4.6 Darstellung der Beziehungen und Wortarten im Beispielsatz
Auch hier findet keinesfalls eine regelbasierte Analyse des Satzes statt. Es kommt stattdessen 
wieder ein statistisches Modell zum Einsatz, das den Satz nach der Tokenisierung (also dem 
Zerlegen in einzelne Tokens, meist Wörter) parst und mit Tags versieht. Diese Tags entspre chen dem Universal POS Tags-Standard2
 und kennzeichnen die einzelnen Tags in diesem 
Fall als Nomen, Hilfsverb und Adjektiv. Die als Pfeile gezeichneten Relationen sagen aus, 
dass Goldfische das Subjekt zum Verb sind ist und pflegeleicht das dazugehörige Prädikat.
2 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichuniversaldependencies.org/docs/u/pos



78 4 Spracherkennung
TIPP: Wenn Sie sich intensiver mit SpaCy beschäftigen möchten (was ich 
nachvollziehen kann, weil es eine faszinierende Sammlung von Funktionen 
ist), dann werden Sie häufiger auf unbekannte Abkürzungen stoßen. Ein Trick, 
um ohne lange Recherche eine Antwort zu erhalten, ist die Verwendung von 
print(spacy.explain('sb')). Damit erhalten Sie eine Erklärung, was sich 
beispielsweise hinter sb verbirgt.
Am Ende dieses Abschnitts möchte ich Ihnen noch folgenden Hinweis geben: Seit einiger Zeit 
werden sogenannte Recurrent Neural Networks eingesetzt, um Sequence-to-Sequence-Modelle 
zu trainieren, die den statistisch wahrscheinlichsten Text zu einem digitalisierten Audiostrom 
ermitteln können. Sollte Sie das Thema interessieren, kann ich Ihnen das Unterrichtsmaterial 
von Andrew Ng sehr empfehlen, das RNNs in all ihren Facetten ausführlich erklärt.
Sie sehen, dass das Feld sehr umfassend ist, haben wir doch eben in kürzester Zeit die 
Digitalisierung von Audio, Hidden Markov Models, (Recurrent) Neural Networks, Language 
Models und POS-Tagging angesprochen. Wenn Sie in den nächsten zwei Abschnitten sehen 
werden, wie einfach wir doch Aktivierungswörter, Spracherkennung und Sprechererkennung 
umsetzen, werden Sie sicher überrascht sein.
■ 4.1 Aktivierungswörter
Wir beginnen die Umsetzung in diesem Kapitel mit den Aktivierungswörtern, die unse ren Sprachassistenten darauf aufmerksam machen sollen, dass nun ein Befehl folgt. Die 
bekanntesten Beispiele für solche sind sicher Siri, Alexa oder Okay Google. Warum werden 
Aktivierungswörter benötigt? Klar, einerseits wollen wir nicht, dass ein Computer all unsere 
privaten Gespräche mithört. Dieses Argument zieht bei uns allerdings nicht, da wir ja selber 
dafür sorgen werden, dass sprichwörtlich kein Wort diesen Raum verlässt. Jedoch kann es 
auch vorkommen, dass Gespräche als Befehl interpretiert werden und der Assistent fälsch licherweise Funktionen ausführt, ohne dass es von den Benutzern gewollt ist.
Bevor wir jedoch mit der Umsetzung beginnen, müssen wir uns noch kurz dem Thema Kon figurationsmanagement widmen, denn unser Sprachassistent soll zu einem gewissen Grad 
konfigurierbar sein und die Werte der Konfiguration sollen persistent gespeichert werden, 
sodass diese beim Starten automatisch geladen werden können.
4.1.1 Exkurs: Konfigurationsmanagement
Wir werden uns zunächst kurz das YAML-Format anschauen. Natürlich könnten wir auch 
Properties-, JSON- oder XML-Dateien verwenden, jedoch finde ich persönlich YAML immer 
etwas schlanker und besser lesbar als XML und gleichzeitig kann es wesentlich mehr In formationen vorhalten, als eine einfache Properties-Datei mit ihren Schlüssel-Wert-Paaren.



4.1 Aktivierungswörter 79
Haben wir uns einmal mit dem Format vertraut gemacht, schauen wir uns an, wie wir eine 
solche Datei einlesen, wie wir Werte darin speichern und daraus lesen. Ganz konkret begin nen wir mit der Sprache als erste, konfigurierbare Eigenschaft.
YAML steht für YAML Ain’t Markup Language (in der ersten Version stand es noch für Yet 
Another Markup Language) und ist eine Sprache für die Datenserialisierung. Ziel war es, eine 
möglichst lesbare Sprache ohne viele Strukturelemente zu schaffen, was dadurch umgesetzt 
wurde, dass sie aus hierarchischen Schlüssel-Wert-Paaren zusammengesetzt ist.
YAML-Parser verfügen häufig über eine automatische Datentyperkennung, sodass wir in 
der Konfiguration Boolean, String, Integer oder Float verwenden können und diese in den 
gängigen Frameworks auch als solche erkannt werden (siehe Bild 4.7). Der Standard bietet 
sogar die Möglichkeit, Datentypen zu forcieren, so weit werden wir aber nicht gehen, da 
wir die Logik implementieren und schon dort drauf achten werden, dass die Konfiguration 
korrekt geschrieben wird.
Strings
name: bumbleblee
name: ‚bumblebee‘
Mehrzeilig
name: |
mein
name
ist bumblebee
Zahlen
alter: 20
größe: 1,85
Boolean
berechgt: True
berechgt: true
berechgt: TRUE
Null-Werte
beziehung:
beziehung: null
beziehung: Null
Li beziehung: NULL sten
freunde: [Max, Marcel, 
Marcel]
oder
freunde:
- Max
- Marcel
- Simon
Verschachtelungen
muer:
name: Ulla
alter: 65
beruf: Lehrerin
Kommentare
# Bie prüfen
Bild 4.7 Beispiele für Datentypen des YAML-Formats
Bild 4.7 birgt kaum Überraschungen. Zeichenketten können mit oder ohne Hochkommata 
verwendet werden, auch mehrzeilige Zeichenketten sind möglich, wenn diese mit einer 
Pipe, also einem senkrechten Strich, beginnen. Booleans können in verschiedener Groß- 
und Kleinschreibung verwendet werden, Zahlen als Gleitkommawert oder als Ganzzahl. 
Listen hingegen lassen sich über eine eckige Klammer zusammenfassen oder einfach mit 
Spiegelstrichen untereinanderschreiben. Ein großer Vorteil ist, dass auch Null-Werte (als 
Variablen, denen kein Wert zugeordnet ist) gesetzt werden können. Ebenso sind auch Ver schachtelungen, wie man sie aus der objektorientierten Programmierung kennt, zulässig. 
So kann etwa das Objekt mutter mehrere Eigenschaften wie Name, Alter oder Beruf haben. 
Zu guter Letzt deutet die Raute an, dass ein Kommentar bis zum Zeilenende folgt, was be sonders für Entwickler, die mit Python vertraut sind, eine Erleichterung beim Lesen der 
Konfiguration darstellt.
Kopieren Sie nun den letzten Projektordner aus dem vorherigen Kapitel über Text-To-Speech 
und nennen Sie diesen 003_confman. Dem sollte die Anlage eines passenden Environments 
über conda create -n 003_confman python=3.8 folgen sowie die Installation aller Pakete 
aus 002_ttsx. Wie war das doch gleich? Richtig, pip freeze > requirements.txt im Ordner 
002_ttsx bei aktivierter Umgebung 002_ttsx und dann pip install -r requirements.txt



80 4 Spracherkennung
bei aktivierter Umgebung 003_confman. Die Aktivierung geschieht über conda activate 
003_confman. Im Prinzip haben wir damit nichts anderes getan, als Projektordner und Um gebung aus dem Beispiel 002_ttsx zu duplizieren. Installieren Sie nun noch pyyaml über pip, 
um einen Parser zum Lesen der Konfiguration im Projekt zu haben.
Beginnen wir damit, dass wir die Konfiguration anlegen. Erstellen Sie dazu im Projektordner 
eine Datei mit dem Namen config.yml und fügen Sie folgenden Inhalt ein.
assistant:
 language: "de"
Ich möchte gerne die Eigenschaften des Sprachassistenten unter dem Schlüssel assistant
gruppieren. Als erstes Element legen wir einen String mit dem Schlüssel language und dem 
Wert de an. Achten Sie wie beim Python-Code auch auf eine ordentliche Einrückung, um die 
Lesbarkeit zu gewährleisten.
Schauen wir uns nun die wesentliche Änderung in der main Punkt Pei an, in der wir diese Konfigu rationsdatei nun einlesen werden. Das komplette Beispiel (oder besser das MVP, wir arbeiten 
ja inkrementell) finden Sie wie gewohnt im Github-Repository.
Wir erstellen zu Beginn in Listing 4.4 eine globale Variable Namens CONFIG_FILE. Die Groß- 
und Kleinschreibung spielt keine Rolle, ich habe mir nur angewöhnt, Konstanten groß und 
mit Unterstrich getrennt zu schreiben. Die Bezeichnung global rührt daher, dass die Variable 
außerhalb jeder Klasse definiert ist. Dieser Variablen weisen wir nun einfach den Namen 
unserer Konfigurationsdatei zu.
Listing 4.4 Auslesen der Konfiguration in der main Punkt Pei
1. from loguru import logger
2. import yaml
3. import sys
4.
5. from TTS import Voice
6. import multiprocessing
7.
8. CONFIG_FILE = "config.yml"
9.
10. class VoiceAssistant():
11.
12. def __init__(self):
13. logger.info("Initialisiere VoiceAssistant...")
14.
15. # Lese Konfigurationsdatei
16. logger.debug("Lese Konfiguration...")
17.
18. # Verweise lokal auf den globalen Kontext und hole die Variable CONFIG_FILE
19. global CONFIG_FILE
20. with open(CONFIG_FILE, "r", encoding='utf-8') as ymlfile:
21. # Lade die Konfiguration im YAML-Format
22. self.cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
23. if self.cfg:
24. logger.debug("Konfiguration gelesen.")



4.1 Aktivierungswörter 81
25. else:
26. # Konnte keine Konfiguration gefunden werden? Dann beende die Anwendung
27. logger.debug("Konfiguration konnte nicht gelesen werden.")
28. sys.exit(1)
29. language = self.cfg['assistant']['language']
30. if not language:
31. language = "de"
32. logger.info("Verwende Sprache {}", language)
In der Methode __init__() wird die Konfiguration gelesen. Das beginnt damit, dass wir 
CONFIG_FILE aus dem globalen Kontext in unsere Klasse holen. Das tun wir in Python, indem 
wir, wie in Zeile 19 zu sehen, dem Variablennamen ein global voranstellen.
Im Folgenden können wir deren Wert lesen, was in Zeile 20 geschieht. Dort öffnen wir die 
Datei über die Methode open(), sodass sie gelesen werden kann (das "r" im zweiten Para meter steht für read), und wir geben noch das Encoding an, um zu bestimmen, wie die Datei 
zu interpretieren ist.
Sehen Sie den neuen Import yaml? Diesen machen wir uns nun in Zeile 22 zunutze und rufen 
yaml.load auf, um die gesamte Konfiguration einzulesen. Dann können wir schon auf unsere 
Eigenschaft language zugreifen. Dieser Zugriff geschieht hierarchisch, sodass wir, wie in 
Zeile 29 zu sehen, erst das Element assistent referenzieren. Dieses Element kann verschie dene Elemente als Unterelement haben, wir verweisen auf das einzige Element language und 
bekommen damit den Wert de zurückgeliefert, den wir in der Variablen language speichern.
In Zeile 30 und 31 behandeln wir den möglichen Fehler, dass die Variable nicht gesetzt ist 
und setzen sie dann per Default auf de. Zu guter Letzt geben wir die Konfiguration per Logger 
aus. Der Platzhalter {} im ersten Argument des Aufrufs von logger.info() wird durch die 
folgenden Parameter des Aufrufs ersetzt, in diesem Fall durch language.
Wenn Sie die Anwendung ausführen, werden Sie sehen, dass die Konfiguration bereits gelesen 
wird. Dadurch, dass wir nun in der Konfiguration ein Kürzel wie de statt des ausgeschriebe nen Namens wie etwa German verwenden, müssen wir die Methode get_voice_keys_by_
language() in der Klasse Voice leicht modifizieren. Statt voice.name nach German oder 
English zu durchsuchen, durchsuchen wir nun die voice.id nach dem Sprachkürzel, gefolgt 
von einem Bindestrich (siehe Zeile 8 in Listing 4.5). Wir erinnern uns, dass die voice.id wie 
folgt aussieht und das Sprachkürzel aufweist: HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\
Speech\Voices\Tokens\TTS_MS_DE-DE_HEDDA_11.0.
Listing 4.5 Die Methode get_voice_keys_by_language() verwendet nun Kürzel.
1. def get_voice_keys_by_language(self, language=''):
2. result = []
3. engine = pyttsx3.init()
4. voices = engine.getProperty('voices')
5.
6. # Wir hängen ein "-" an die Sprache in Großschrift an,
7. # damit sie in der ID gefunden wird
8. lang_search_str = language.upper()+"-"
9.



82 4 Spracherkennung
10. for voice in voices:
11. # Die ID einer Sprache ist beispielsweise:
12. # HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\...\TTS_MS_DE-DE_HEDDA_11.0
13. if language == '':
14. result.append(voice.id)
15. elif lang_search_str in voice.id:
16. result.append(voice.id)
17. return result
Sollten Sie sich entschieden haben, Ihre eigene Stimme für den Assistenten zu verwenden 
und nicht auf pyttsx3 zu setzen, dann ist dieser Schritt obsolet, da wir ja sowieso nur eine 
Stimme in einer Sprache trainiert haben.
Nun müssen Sie noch in der main Punkt Pei bei der Sprachauswahl den Parameter language berück sichtigen und die Zeichenkette "German" durch diesen ersetzen, also modifizieren wir die 
Zuweisung so: voices = self.tts.get_voice_keys_by_language(language).
Damit haben wir unseren seichten Einstieg in die Verwendung einer Konfiguration geschafft. 
Wir werden diese immer, wenn benötigt, erweitern und uns später auch anschauen, wie 
wir die config.yml schreiben, falls wir Änderungen per Sprachbefehl vorgenommen haben. 
Schauen wir uns nun die Aktivierung per Wake Word an.
4.1.2 Die Hummel und das Stachelschwein
Roy Black lebte zwar eindeutig vor meiner Zeit, trotzdem konnte ich mir den Abschnittstitel 
nicht verkneifen, da die Ähnlichkeit zu den zwei Protagonisten aus Schön ist es, auf der Welt 
zu sein einfach zu groß ist. Sie werden gleich verstehen, warum. Unser Ziel ist jetzt, das 
Verständnis von Aktivierungswörtern in unserer Anwendung zu verankern. Auf dem Weg 
dorthin werden Sie lernen, Bibliotheken manuell über Wheel-Dateien zu installieren und 
eine Audioeingabe und -verarbeitung über PyAudio umzusetzen.
Für die Erkennung der Aktivierungswörter verwenden wir die Bibliothek porcupine und lösen 
damit schon mal das Rätsel um das Stachelschwein auf. Duplizieren Sie den Projektordner 
003_confman aus dem letzten Abschnitt und nennen Sie ihn 004_wake_words. Legen Sie auch 
das entsprechende Environment an und installieren Sie alle Pakete, die Sie im Environment 
003_confman installiert haben, am besten über eine requirements.txt. Diese finden Sie wie 
immer auch im Github Repostitory.
Nun installieren wir PyAudio über ein Wheel, das Sie von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichwww.lfd.uci.edu/~gohlke/
pythonlibs herunterladen können.
Der Autor der Seite, Christoph Gohlke, macht sich eine Menge Arbeit, um die Pakete hier 
zu kompilieren und zu pflegen, um fehlende Distributionen bereitzustellen, die auf dem 
Python Package Index (Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichpypi.org) nicht zur Verfügung stehen – so auch PyAudio in 
Version 0.2.11 für Python 3.8 und ein 64-Bit-Windows. Suchen Sie also auf der Seite nach 
dem Link PyAudio-0.2.11-cp38-cp38-win_amd64.whl und laden Sie die Datei herunter.



4.1 Aktivierungswörter 83
TIPP: Wenn Sie versuchen, PyAudio manuell über pip zu installieren, werden Sie 
eine Fehlermeldung bekommen. Zum Herunterladen des Wheels gibt es jedoch 
noch die Alternative pipwin. Diese Anwendung wird ganz normal über pip instal liert und kann dann selber wiederum wie pip aufgerufen werden. Ein Beispiel sorgt 
sicher für Klarheit:
pip install pipwin
pipwin install pyaudio
Da wir jedoch eine requirements.txt pflegen wollen, die plattformunabhängig ist, 
tue ich mich mit der Verwendung von pipwin etwas schwer und bevorzuge die Ins tallation des Wheels.
Legen Sie die Datei nun in den Projektordner und führen Sie pip install PyAudio-0.2.11-
cp38-cp38-win_amd64.whl aus, um das Modul PyAudio zu installieren. Wenn Sie nun im 
aktivierten Environment ein pip freeze ausführen, werden Sie sehen, dass das Wheel dort 
ebenfalls zu finden ist, allerdings mit einem absoluten Pfad. Diesen sollten Sie später korri gieren, wenn Sie die Pakete in die requirements.txt ausgeben, da der Projektordner nicht auf 
jedem PC am selben Ort zu finden ist und somit der Pfad nicht stimmt.
Gut, PyAudio ist installiert und damit haben wir die Möglichkeit, Audio-Streams in unserer 
Anwendung einzulesen. Fahren wir damit fort festzulegen, auf welche Aktivierungswörter 
unser Assistent reagieren soll. Die Bibliothek porcupine bietet vortrainierte Modelle3
 für 
Aktivierungswörter. Einige davon seien hier genannt:
 Alexa
 Americano
 Bumblebee
 Jarvis
 Terminator
 Computer
Folgen Sie dem Link in der Fußnote, bekommen Sie eine vollumfängliche Liste zu sehen. Wie 
der Titel dieses Abschnitts vermuten lässt, habe ich mich für Bumblebee, also die Hummel, 
entschieden. Wählen Sie aber gerne auch einen anderen Begriff. Auch mehrere Begriffe 
sind erlaubt.
HINWEIS: Die Verwendung mehrerer Aktivierungswörter kann zum Beispiel dann 
interessant sein, wenn Sie Funktionalität im Sprachassistenten logisch trennen 
möchten. So kann Terminator verwendet werden, um das Licht im ganzen Haus 
auszuschalten, wobei Computer für alle anderen Aufgaben eingesetzt wird. Wir 
werden jedoch in den kommenden Kapiteln keine Unterscheidung zwischen einzel nen Wake Words vornehmen.
3 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/Picovoice/porcupine/tree/master/resources/keyword_files/windows



84 4 Spracherkennung
Installieren Sie nun pvporcupine4
 in Version 1.9.5 über pip install pvporcupine==1.9.5 und 
öffnen Sie unsere config.yml. Tragen Sie dann als Unterelement von assistant die Eigenschaft 
wakewords: ["bumblebee", "terminator"] ein. Gemäß Bild 4.7 haben wir es hier mit 
einer Liste zu tun, die zwei Zeichenketten beinhaltet, die als Wake Words dienen sollen. In 
der main Punkt Pei sorgen wir dafür, dass folgende Imports gegeben sind:
from loguru import logger
import yaml, time, sys, pvporcupine, pyaudio, struct, os
Die kommaseparierte Schreibweise ist in Python zwar geduldet, aber nicht so üblich. Ich 
verwende sie aber, um Papier und Druckerfarbe zu sparen, sehen Sie es mir nach.
In der Methode __init__() in der main Punkt Pei werden wir nun direkt im Anschluss an das Aus lesen der Sprache die Aktivierungsworterkennung und die Audioeingabe über Ihr Mikrophon 
initialisieren. Listing 4.6 zeigt die dazugehörigen Befehle. Zu Beginn lesen wir in Zeile 4 die 
Konfiguration aus und speichern die Aktivierungswörter in der der Klasse VoiceAssistant
zugeordneten Variablen wake_words.
Listing 4.6 Initialisierung der Aktivierungsworterkennung und Erzeugen eines Audioeingabe-Streams
1. logger.debug("Initialisiere Wake Word Erkennung...")
2.
3. # Lies alle wake words aus der Konfigurationsdatei
4. self.wake_words = self.cfg['assistant']['wakewords']
5.
6. # Wird keines gefunden, nimm 'bumblebee'
7. if not self.wake_words:
8. self.wake_words = ['bumblebee']
9. logger.debug("Wake Words sind {}", ','.join(self.wake_words))
10.
11. self.porcupine = pvporcupine.create(keywords=self.wake_words)
12. # Sensitivities (sensitivities=[0.6, 0.35]) erweitert oder schränkt
13. # den Spielraum bei der Intepretation der Wake Words ein
14. logger.debug("Wake Word Erkennung wurde initialisiert.")
15.
16. # Initialisiere Audio stream
17. logger.debug("Initialisiere Audioeingabe...")
18. self.pa = pyaudio.PyAudio()
19.
20. # Liste alle Audio Devices auf
21. for i in range(self.pa.get_device_count()):
22. logger.debug('id: {}, name: {}',
23. self.pa.get_device_info_by_index(i).get('index'),
24. self.pa.get_device_info_by_index(i).get('name'))
25.
26. # Wir öffnen einen (mono) Audio-Stream, der Audiodaten einer bestimmten Länge
27. # von einem bestimmten Device einliest.
4 Picovoice, die Entwickler von pvporcupine, haben sich entschieden, ab Version 2.x ihrer Software einen Lizenz schlüssel (Access Key) einzuführen, was aus meiner Sicht absolut verständlich ist, denn sie müssen ja Geld 
verdienen. Für Lernende ist ein solcher Schlüssel kostenfrei. Möchten Sie pvporcupine allerdings ohne einen 
solchen einsetzen, achten Sie darauf, die Version 1.9.5 zu installieren.



4.1 Aktivierungswörter 85
28. self.audio_stream = self.pa.open(rate=self.porcupine.sample_rate, channels=1,
29. format=pyaudio.paInt16, input=True,
30. frames_per_buffer=self.porcupine.frame_length, input_device_index=0)
31.
32. logger.debug("Audiostream geöffnet.")
Darauf initialisieren wir porcupine in Zeile 11 und übergeben der Methode create() die Liste 
von Aktivierungswörtern aus unserer Konfiguration. Sollten Sie feststellen, dass porcupine
Schwierigkeiten hat, ein bestimmtes Schlüsselwort zu verstehen, können Sie in create()
den optionalen Parameter sensitivities, wie in Zeile 12 gezeigt, mitgeben, um die Überprüfung 
auf die übergebenen Wörter weniger strikt durchzuführen. Die Elemente dieser Liste dieses 
Parameters beziehen sich jeweils auf die Sensitivität der Liste der Schlüsselwörter.
In Zeile 18 erfolgt die Initialisierung der Audioeingabe über PyAudio. Mit der Instanz pa
können wir in den Zeilen 21 bis 24 alle Audiogeräte ausgeben, über die Ihr PC verfügt. 
Häufig ist das Standardgerät mit der ID 0 versehen. Sollten Sie jedoch die Aufnahme über 
ein anderes wünschen, merken Sie sich die ID, die die For-Schleife ausgibt und ersetzen Sie 
die 0 von input_device_index in Zeile 30.
HINWEIS: For-Schleifen funktionieren in Python etwas anders als in Java oder C#. 
In diesen Sprachen zählt ein meist temporärer Zähler von einem definierten Start wert zu einem Endwert hoch- bzw. herunter, was häufig so aussieht: for int i=0 
to 10. Stattdessen iteriert die For-Schleife in Python über eine Sequenz, was eine 
Liste, ein Dictionary oder auch eine Zeichenkette sein kann. Um etwa von 0 bis 9 
zu zählen, müsste die Schleife so aussehen: for i in range(10):. Die Funktion 
range() dient dazu, die benötigte Sequenz zu erzeugen.
In den Zeilen 28 bis 30 öffnen wir einen Ausgabestream auf dem gewünschten Gerät. Die 
Eigenschaften rate und frames_per_buffer sind von porcupine vorgegeben. Ein Monostream 
genügt uns, weswegen wir channels auf 1 setzen, denn bei der Audioeingabe geht es nicht 
um Aufnahmequalität, sondern um die Einfachheit der Analyse. Des Weiteren können wir 
keine zusätzlichen Informationen aus dem Stream schöpfen, denn das Gesprochene bleibt das 
Gleiche. Dass es sich um einen Eingabe-Stream handelt, spezifizieren wir über input=True
und über format=pyaudio.paInt16 legen wir lediglich das Datenformat fest, in dem der 
Stream gelesen werden soll.
Nun ist der Stream geöffnet und wir können die ankommenden Daten verarbeiten. Da dies 
ein kontinuierlicher Prozess ist, werden wir das jedoch nicht in der Initialisierungsphase 
tun, sondern in der bisher recht unspektakulären Methode run(). Diese implementieren 
wir aus, wie in Listing 4.7 gezeigt. In Zeile 2 eröffnen wir ein Try-Except-Finally-Konstrukt, 
welches versucht, den Code von Zeile 3 bis 9 auszuführen. Tritt dabei eine Exception auf, 
springt die Anwendung in die Zeile 11. Allerdings schränken wir die Exception auf einen 
KeyboardInterrupt ein, der auftritt, wenn der Benutzer STRG+C drückt, um das Programm zu 
beenden. Somit schaffen wir eine elegante Lösung, um die eigentlich endlose Verarbeitung 
des Audio-Streams, die in Zeile 3 begonnen wird, zu beenden.



86 4 Spracherkennung
Listing 4.7 Implementierung der Methode run() zur Erkennung von Aktivierungswörtern
1. def run(self):
2. try:
3. while True:
4. pcm = self.audio_stream.read(self.porcupine.frame_length)
5. pcm_unpacked = struct.unpack_from("h" * self.porcupine.frame_length, pcm)
6. keyword_index = self.porcupine.process(pcm_unpacked)
7. if keyword_index >= 0:
8. logger.info("Wake Word {} wurde verstanden.",
9. self.wake_words[keyword_index])
10.
11. except KeyboardInterrupt:
12. logger.debug("Per Keyboard beendet")
13. finally:
14. logger.debug('Beginne Aufräumarbeiten...')
15. if self.porcupine:
16. self.porcupine.delete()
17.
18. if self.audio_stream is not None:
19. self.audio_stream.close()
20.
21. if self.pa is not None:
22. self.pa.terminate()
In Zeile 4 lesen wir Frame für Frame den Stream ein. Ein Frame ist immer ein bestimmter 
Ausschnitt der Audiodaten. Bei einem zu kleinen Frame ist es wahrscheinlich, dass das Akti vierungswort abgeschnitten wird, bei einem zu großen Frame kann es zu längeren Verarbei tungszeiten kommen und die Aktivierung des Sprachassistenten wird eventuell verzögert. 
Auch hier gibt uns porcupine dankenswerterweise einen passenden Wert vor, sodass wir uns 
nicht darum kümmern müssen (siehe Zeile 4). Zeile 5 nimmt die Daten aus dem Buffer pcm
und formatiert sie so um, dass sie von porcupine gelesen werden können, in diesem Fall als 
eine Folge von Short-Werten, was das "h" spezifiziert. Multipliziert man einen String mit 
einer Ganzzahl, wird der String um dessen Wert verlängert, "h" * 5 ergibt also "hhhhh".
Nun sind wir so weit, dass wir porcupine.process() ausführen können. Sollte ein Akti vierungswort erkannt worden sein, gibt die Methode den Index des Wortes in der Liste aller 
von uns selektierten Aktivierungswörter zurück.
Es folgt noch der Finally-Block, der in jedem Fall nach dem Try-Except aufgerufen wird, egal 
ob das Try erfolgreich ausgeführt wurde oder nicht. Hier haben wir die Chance, unser Pro gramm sauber zu beenden, was wir auch gerne in Anspruch nehmen. Wir beenden porcupine
in Zeile 15, schließen den Audio-Stream in Zeile 19 und unsere PyAudio-Instanz in Zeile 22.
Damit ist die Implementierung schon abgeschlossen, Sie können den Assistenten nun 
starten und sollten sehen, dass er in unserem Fall auf bumblebee oder terminator mit einer 
Log-Meldung reagiert.



4.2 Implementierung einer Spracherkennung 87
■ 4.2 Implementierung einer Spracherkennung
Nun haben wir ja mehr oder weniger schon eine Spracherkennung im vorigen Abschnitt 
implementiert, allerdings stark eingeschränkt, nämlich auf die zwei Aktivierungswörter, 
die wir uns ausgesucht haben. Was wir nun zusätzlich erreichen möchten, ist, eine unein geschränkte Spracherkennung durchzuführen zu können, nachdem ein Aktivierungswort 
gesprochen wurde.
Dazu werden wir die Bibliothek VOSK verwenden, die für viele verschiedene Programmier sprachen und Plattformen zu Verfügung steht. Dies ist insofern sehr hilfreich, da ich Ihnen 
ja eine größtmögliche Freiheit bei der Wahl der Zielplattform des Assistenten versprochen 
hatte. Ebenso existieren für VOSK viele vortrainierte Modelle in verschiedenen Sprachen, 
derzeit etwa 19. Diese unterteilen sich in portable und komplexere Modelle, die entweder 
auf Geräten mit weniger Rechenleistung lauffähig sind oder aber eine höhere Genauigkeit 
bei der Erkennung aufweisen.
Ebenso befinden wir uns in diesem Kapitel in der Situation, dass wir zwischen mehreren 
Zuständen wechseln müssen, nämlich:
 Warten auf ein Aktivierungswort
 Lauschen auf Befehle der Benutzer
 Beenden des Sprachassistenten
Also müssen wir eine einfache Finite State Machine (FSM) implementieren, die auf die ver schiedenen Zustände des Sprachassistenten richtig reagiert. Klingt alles schwieriger, als es 
ist. Beginnen wir einfach damit, dass wir das letzte Projekt und dessen Anaconda-Umgebung 
duplizieren und in 05_text_to_speech umbenennen.
Bevor wir nun mit der Implementierung beginnen, stoßen wir den Download des Text-To Speech-Modells an, das Sie auf der Seite Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichalphacephei.com/vosk/models finden.
Dort unter der Sprache German ist das vosk-model-de-0.6 zu finden. Sie können auch gerne die 
deutschsprachigen Modelle mit dem Tag small ausprobieren, jedoch habe ich die Erfahrung 
gemacht, dass die Qualität der Erkennung doch recht reduziert ist. Sie sollten diese Modelle 
nur verwenden, wenn es wirklich einen Ressourcenmangel auf Ihrem Gerät gibt.
Laden Sie im gleichen Zuge noch das unter Speaker identification model zu findende Modell 
vosk-model-spk-0.4 herunter, das wir für den nächsten Abschnitt dieses Kapitels benötigen. 
Entpacken Sie nun beide Archive in das Projektverzeichnis, sodass darin ein Ordner vosk model-de-0.6 und ein Ordner model-spk-0.4 zu finden ist.
Fügen Sie am Anfang der main Punkt Pei die nötigen zwei Imports hinzu, die wir für die Sprach erkennung benötigen:
from vosk import Model, SpkModel, KaldiRecognizer
import json
Nun können wir VOSK initialisieren. Der Prozess ist denkbar einfach, wie in Listing 4.8
zu sehen ist. Zu Beginn laden wir das Modell zur Spracherkennung in Zeile 2, gefolgt vom 
Modell für die Sprechererkennung in Zeile 3. In der Instanz von SpeechAssistant legen wir 
eine neue Variable rec an, die den Recognizer selbst beinhaltet.



88 4 Spracherkennung
Listing 4.8 Initialisierung der Spracherkennung mit VOSK
1. logger.info("Initialisiere Spracherkennung...")
2. stt_model = Model('./vosk-model-de-0.6')
3. speaker_model = SpkModel('./vosk-model-spk-0.4')
4. self.rec = KaldiRecognizer(stt_model, speaker_model, 16000)
5. # Hört der Assistent gerade auf einen Befehl oder wartet er auf ein Wake Word?
6. self.is_listening = False
In Zeile 6 legen wir zuletzt eine weitere Variable is_listening an, die vorhalten wird, ob der 
Sprachassistent derzeit einem Befehl lauscht oder auf ein Aktivierungswort wartet.
Jetzt sind wir bereit, Sprache zu erkennen. Da dies wieder ein kontinuierlicher Prozess 
ist, platzieren wir den notwendigen Code in der Methode run() direkt im Anschluss an 
die Überprüfung auf die Aktivierungswörter. Dem bestehenden If-Statement in Zeile 1 von 
Listing 4.9 fügen wir eine Zuweisung hinzu und setzen self.is_listening auf True, denn wenn 
ein Aktivierungswort gesagt wird, soll ja die Spracherkennung aktiviert werden.
Das prüfen wir auch sogleich in Zeile 6. Wenn wir also lauschen und VOSK etwas mit dem 
Buffer pcm, den wir ja schon vorher über PyAudio eingelesen haben, anfangen kann, ana lysieren wir das Ergebnis. Dieses kommt als JSON-formatierter String daher, weswegen wir 
diesen über die Bibliothek json parsen müssen. Das Resultat speichern wir in recResult, auf 
dessen Eigenschaft text wir zugreifen, um den erkannten Text auszugeben.
Listing 4.9 Implementieren der kontinuierlichen Spracherkennung
1. if keyword_index >= 0:
2. logger.info("Wake Word {} wurde verstanden.", self.wake_words[keyword_index])
3. self.is_listening = True
4.
5. # Spracherkennung
6. if self.is_listening:
7. if self.rec.AcceptWaveform(pcm):
8. recResult = json.loads(self.rec.Result())
9.
10. # Hole das Resultat aus dem JSON Objekt
11. sentence = recResult['text']
12. logger.debug('Ich habe verstanden "{}"', sentence)
13.
14. self.is_listening = False
Zuletzt wird die Spracherkennung in Zeile 14 wieder deaktiviert. Es fehlt nur noch ein kurzes 
pip install vosk, um die Bibliothek zu installieren, denn wir haben vorher lediglich die 
Modelle heruntergeladen, nicht aber die Logik. Führen Sie nun die Anwendung über python 
main Punkt Pei aus, so werden Sie sehen, dass Sie den Sprachassistenten aktivieren und Befehle 
einsprechen können, die in der Konsole ausgegeben werden.



4.3 Fingerabdruck der Stimme 89
■ 4.3 Fingerabdruck der Stimme
Nun soll es noch darum gehen, dass wir einzelne Sprecher identifizieren können, um sie 
unterschiedlich zu behandeln. So können wir etwa individuell Nachrichten abspielen, für 
Benutzer X die Tagesschau, für Benutzer Y die Heise News. Ebenso wie das Embedding einer 
Sprecherstimme, das wir in Kapitel 3 kennengelernt haben, können wir den Fingerabdruck 
einer Stimme über den KaldiRecognizer in Erfahrung bringen. Dazu müssen wir nicht mal 
große Aufwände betreiben, denn der Fingerabdruck wird sowieso in dem resultierenden 
JSON-Objekt mitgegeben. Damit es also in diesem Abschnitt nicht zu langweilig wird, werfen 
wir noch einen Blick auf TinyDB, eine Bibliothek, mit der wir die Fingerabdrücke samt ein 
paar weiterer Benutzerdaten lokal in einer JSON-basierten Datenbank speichern und daraus 
lesen können.
Beginnen wir damit, dass wir unserer config.yml das Unterelement allow_only_known_
speakers: True unter dem Schlüssel assistant hinzufügen. Wie Sie wahrscheinlich richtig 
vermuten, wollen wir dadurch steuern, dass der Assistent nur Befehle von einem ihm be kannten Sprecher entgegennimmt.
Es folgt die Anlage der neuen Datei usermgmt.py im Hauptverzeichnis unseres Projekts. In 
Listing 4.10 importieren wir zusätzlich zu loguru TinyDB und Query aus dem Modul tinydb, 
um konkret in Zeile 16 eine Tabelle aus der JSON-Datei users.json zu öffnen und gleich darauf 
die Tabelle speakers daraus zu referenzieren.
Da wir zu Beginn wahrscheinlich eine leere Datei vorfinden, fügen wir eine Option hinzu, 
um einen Dummy-Benutzer über __add_dummies__() hinzuzufügen. Dies veranschaulicht 
auch gleich, wie Sie prüfen können, ob in der Tabelle speaker_table ein Eintrag mit einem 
bestimmten Wert in einem bestimmten Feld gefunden wird. Hier prüfen wir in Zeile 10, ob 
ein Eintrag existiert, dessen Eigenschaft name auf jonas gesetzt ist. Ist das nicht der Fall, 
wird dieser in Zeile 11 angelegt. Der Wert für den Fingerabdruck der Stimme habe ich durch 
drei Punkte ersetzt, da die Liste von Floats das Listing unnötig aufblähen würden. Wir füllen 
diesen aber gleich, und Sie können gerne dessen Namen ändern.
Listing 4.10 Die Klasse UserMgmt verwaltet die Datenbank und die Tabelle für die Benutzerverwaltung.
1. from loguru import logger
2. from tinydb import TinyDB, Query
3.
4. class UserMgmt:
5.
6. def __add_dummies__(self):
7.
8. # Füge zwei Sprecher hinzu
9. Speaker = Query()
10. if (not self.speaker_table.contains(Speaker.name == 'jonas')):
11. self.speaker_table.insert({'name': 'jonas', 'voice': [...]})
12.
13.
14. def __init__(self, init_dummies=False):
15. # initialize a db to store speaker data
16. self.db = TinyDB('./users.json')



90 4 Spracherkennung
17. self.speaker_table = self.db.table('speakers')
18. if init_dummies:
19. self.__add_dummies__()
Legen Sie nun noch die Datei users.json an und befüllen Sie diese initial mit folgender De finition.
Listing 4.11 Anlage der JSON-Datei für die TinyDB-Datenbank und die Tabelle speakers
{
 "speakers": {
 "1": {
 "name": "",
 "voice": [],
 },
 }
}
Wir erstellen in Listing 4.11 einen ersten, anonymen Sprecher, den Sie hinsichtlich des 
Namens gerne schon mit Ihren Daten füllen dürfen. Dann geht es zurück in die main Punkt Pei, in 
der wir zunächst einige neue Imports definieren müssen.
import tinydb
import numpy as np
from usermgmt import UserMgmt
Die Benutzerverwaltung findet hier also gleich Verwendung. Zusätzlich brauchen wir NumPy, 
eine der bekanntesten Bibliotheken Pythons, die uns mathematische Funktionen auf Basis 
von Vektoren und Matrizen anbietet. In unserem Fall kommt NumPy zum Einsatz, wenn wir 
zwei Sprecherstimmen vergleichen.
Die Initialisierungsmethode reichern wir nun um die Initialisierung unserer Benutzerver waltung an.
Listing 4.12 Initialisierung der Benutzerverwaltung
1. logger.info("Initialisiere Benutzerverwaltung...")
2. self.user_management = UserMgmt(init_dummies=True)
3. self.allow_only_known_speakers =
4. self.cfg["assistant"]["allow_only_known_speakers"]
5. logger.info("Benutzerverwaltung initialisiert")
Zeile 2 in Listing 4.12 bewirkt die Instanziierung der Klasse UserMgmt und gibt per Parameter 
an, dass ein Dummy-Sprecher erstellt werden soll, falls dieser noch nicht existiert. Direkt 
darauf liest die Anwendung die Eigenschaft allow_only_known_speakers in eine Klassen variable ein, sodass wir diese im späteren Verlauf dazu benutzen können, um zu prüfen, ob 
nur bekannte Benutzer zugelassen werden sollen oder nicht. Nun begeben wir uns in die 
Methode run() und editieren den Block ab if self.is_listening.



4.3 Fingerabdruck der Stimme 91
Listing 4.13 Erkennen des Sprechers zur Laufzeit des Sprachassistenten
1. if self.is_listening:
2. if self.rec.AcceptWaveform(pcm):
3. recResult = json.loads(self.rec.Result())
4.
5. # Hole den Namen des Sprechers falls bekannt.
6. speaker = self.__detectSpeaker__(recResult['spk'])
7.
8. # Zeige den "Fingerabdruck" deiner Stimme. Speichere diesen und füge
9. # ihn mit einer neuen ID in users.json ein, die nach dem ersten Aufruf
10. # im Projektverzeichnis erstellt wird.
11. logger.debug('Deine Stimme sieht so aus {}', recResult['spk'])
12.
13. # Sind nur bekannte sprecher erlaubt?
14. if (speaker == None) and (self.allow_only_known_speakers == True):
15. logger.info("Ich kenne deine Stimme nicht und darf damit keine Befehle von
16. dir entgegen nehmen.")
17. self.current_speaker = None
18. else:
19. if speaker:
20. logger.debug("Sprecher ist {}", speaker)
21. self.current_speaker = speaker
22. self.current_speaker_fingerprint = recResult['spk']
23. sentence = recResult['text']
24. logger.debug('Ich habe verstanden "{}"', sentence)
25. self.is_listening = False
26. self.current_speaker = None
Listing 4.13 analysiert die Aufnahme über VOSK wie gehabt. Die erste Neuerung finden wir in 
Zeile 6, in der die Methode __detectSpeaker__() aufgerufen wird, die aus dem JSON-Objekt, 
das wir, wie schon besprochen, vom KaldiRecognizer bekommen, den Fingerabdruck der spre chenden Person bekommt. Nur dass wir diesmal nicht auf das Feld text sondern auf das Feld 
spk zugreifen. Diesen Abdruck in Form einer Liste von Floats prüft __detectSpeaker__()
nun gegen die Datenbank und schaut, ob ein passendes Embedding für einen der bekannten 
Sprecher hinterlegt ist.
Die Logausgabe in Zeile 11 ist für uns sehr wichtig, denn die ausgegebene Liste können Sie 
nach dem Ausführen herauskopieren und in die users.json einfügen, um Ihr Embedding von 
nun an referenzieren und überprüfen zu können.
In Zeile 14 wird auf die Eigenschaft allow_only_known_speakers zugegriffen. Wenn diese auf 
True gesetzt ist und kein uns bekannter Sprecher über __detectSpeaker__() identifiziert 
wurde, verweigert der Sprachassistent den Dienst. Haben wir die Eigenschaft auf False gesetzt 
oder ist ein Sprecher bekannt, so speichern wir Sprecher und Embedding zwischen und geben 
dann den Text aus. Warum setzen wir den Sprecher in Zeile 26 direkt wieder zurück? Nun, 
nach Zeile 24 werden wir noch die komplette Verarbeitung der Befehle einfügen, womit wir 
im folgenden Kapitel beginnen.
Um die Anwendung auszuführen, ist es erforderlich, dass Sie numpy, vosk und tinydb über 
pip installieren. Der Befehl python main Punkt Pei, eine kurze Aktivierung per Wake Word und 
ein beliebiger gesprochener Satz führen dazu, dass Sie den Fingerabdruck Ihrer Stimme 
ausgegeben bekommen. Falls Sie diesen noch nicht in die users.json kopiert haben, holen 
Sie das jetzt nach, sodass der Sprachassistent Sie als validen Benutzer erkennt.






5 Dialoge 
und Intentionen
Unser Sprachassistent hat jetzt schon einige Fähigkeiten erlangt, die es theoretisch ermögli chen, mit einem Menschen zu kommunizieren. Versuchen Sie doch mal mit ein paar einfachen 
If-Statements eine Antwort auf Fragen wie Wie spät ist es? oder Wie heißt du? zu bekommen. 
Spätestens nach der fünften manuell geschriebenen Frage-Antwort-Kombination werden Sie 
merken, dass der Code immer komplexer wird. Weiterhin werden Sie wahrscheinlich fest stellen, dass eine Frage auf viele verschiedene Arten formuliert werden kann. Alleine zur 
Frage nach der Uhrzeit fallen mir direkt folgende Variationen ein:
 Wie spät ist es?
 Wie viel Uhr ist es?
 Uhrzeit
 Wie spät haben wir es?
 Wie spät ist es jetzt?
 Ich wüsste gerne, wie spät es ist.
Dazu kommt, dass sich eine Frage auch noch parametrisieren lässt, zum Beispiel Wie spät ist es in 
Tokyo? Andere lassen sogar mehrere Parameter zu, die alle einzeln verarbeitet werden müssen. 
Aber alles der Reihe nach. In diesem Kapitel werden wir uns erst einmal anschauen, wie wir 
eine Intention eines Benutzers deuten. Dabei verfolgen wir zwei Ansätze:
1. Einen auf Basis fest vorgegebener Muster, die zwar teilweise flexibel in der Erkennung 
sind, aber dennoch explizit angegeben werden müssen.
2. Einen zweiten auf Basis maschinellen Lernens, in dem wir einige Sätze und Entitäten 
vorgeben und mit Annotationen versehen und daraus ein Modell trainiert wird.
Dadurch bieten wir einerseits den zahlreichen Entwicklern, die für unseren herausragenden 
Open-Source-Sprachassistenten später weitere Intents entwickeln, zwei Wege, die Eingaben 
des Benutzers zu interpretieren. Andererseits, und dieses Ziel ist wohl im Moment noch eher 
das naheliegende, lernen Sie auch auf praktische Art und Weise den Unterschied zwischen 
regelbasierter und lernender KI kennen. Sie werden lernen zu entscheiden, wann maschi nelles Lernen sinnvoll ist und wann Sie pragmatisch und zielorientiert zu einem klassischen 
Ansatz greifen sollten.



94 5 Dialoge und Intentionen
■ 5.1 Intent Recognition – Die menschliche 
Intention verstehen lernen
Wir Menschen können uns über viele Kanäle äußern, akustisch durch Worte und Tonfall, durch 
Gesten und Mimik oder auch indem wir … nichts tun. Der Kommunikationswissenschaftler 
Paul Watzlawick hat einmal fünf Axiome zur menschlichen Kommunikation formuliert 
(Watzlawik, Beavin, & Don, 2000):
 Man kann nicht nicht kommunizieren.
 Jede Kommunikation hat einen Inhalts- und einen Beziehungsaspekt.
 Kommunikation ist immer Ursache und Wirkung.
 Menschliche Kommunikation bedient sich analoger und digitaler Modalitäten.
 Kommunikation ist symmetrisch oder komplementär.
Auch wenn ich mich tatsächlich freue, etwas aus dem theoretischen Deutschunterricht aus der 
Oberstufe heute im Berufsleben tatsächlich noch im Kopf zu haben, kommt die ernüchternde 
Feststellung doch recht schnell, dass für die Kommunikation von Mensch und Maschine ganz 
andere Regeln gelten. Anders formuliert: Wir müssten einem Computer erst lehren, diese 
fünf Grundregeln zu beachten, sofern das in unserem Sinne wäre. Aber ist es das? Möchten 
wir, dass ein Sprachassistent gelangweilt wird, wenn wir länger nicht mit ihm sprechen? 
Oder soll er auf den gestressten Ton nach einem langen Arbeitstag damit reagieren, dass er 
eher beruhigenden Jazz spielt, statt, wie von uns gewünscht, die Nachrichten vorzulesen? 
Wahrscheinlich eher nicht. Unsere Erwartungshaltung an eine Maschine ist, dass sie tut, 
wozu wir sie auffordern. Die Gesetzmäßigkeiten der menschlichen Kommunikation gelten 
also nicht, auch wenn ich zugebe, dass es sicher einen gewissen Unterhaltungswert hätte, 
sie zu simulieren.
Wie aber sehen die Herausforderungen bei der Mensch-Maschine-Kommunikation aus? 
Tatsächlich werden wir uns nur mit einem kleinen Feld dieser doch recht umfangreichen 
Disziplin beschäftigen, nämlich dem Computer beizubringen, unsere Intentionen einzu ordnen und zu interpretieren. Die Disziplin der Intent Recognition teilt sich also auf, wie in 
Bild 5.1 zu sehen ist.
Intent Recognion
Intent Classificaon Intent Parsing
Wie spät ist es?
Wie spät ist es in New York?
Wann beginnt das Hockeytraining?
Uhrzeit in Berlin?
Wann muss ich zum Zahnarzt?
Um wie viel Uhr kommt Sarah?
| Uhrzeit
| Uhrzeit
| Kalender
| Uhrzeit
| Kalender
| Kalender
LOCATION
ACTIVITY
LOCATION
ACTIVITY
ACTIVITY
Wie spät ist es?
Wie spät ist es in New York?
Wann beginnt das Hockeytraining?
Uhrzeit in Berlin?
Wann muss ich zum Zahnarzt?
Um wie viel Uhr kommt Sarah?
Bild 5.1 Die Erkennung von Intents gliedert sich auf in die Klassifizierung von Intentionen und in das 
syntaktische Verständnis des Befehls und seiner Parameter.



5.1 Intent Recognition – Die menschliche Intention verstehen lernen 95
Unter vielen verschiedenen Funktionen, die unser Sprachassistent anbietet, muss per Klassi fikation zuerst herausgefunden werden, welche am besten dafür geeignet ist, den Wunsch 
des Benutzers zu erfüllen. Im Beispiel in Bild 5.1 unterscheiden wir zwischen dem Intent 
Uhrzeit und dem Intent Kalender. Ersterer gibt die Uhrzeit in einer bestimmten Zeitzone 
aus, letzterer nennt den Zeitpunkt eines bestimmten Ereignisses im Kalender. Der Classifier 
muss nun mithilfe von einem passenden Set an Trainingsdaten so trainiert werden, dass er 
zwischen den beiden Intents entscheiden kann.
Ist dieses Ziel erreicht, können wir zum zweiten Schritt übergehen und den Satz über einen 
Parser analysieren, sodass wir etwaige Parameter extrahieren können. Im Falle der Urzeit 
wäre es zum Beispiel ein Ort oder eine Aktivität, in Bild 5.1 als LOCATION und ACTIVITY
gekennzeichnet.
5.1.1 Intent Classifier am Beispiel
Da das alles sehr abstrakt klingt und das hier ja ein Praxisbuch ist, wollen wir uns kurz 
zur Untermauerung des Gelesenen eine Anwendung schreiben, die in der Lage ist, Texte, 
darunter auch Intents, zu klassifizieren.
Listing 5.1 Textklassifikation – Laden der Trainings- und Testdaten
1. import spacy
2. from spacy.util import minibatch, compounding
3. import time, random, os
4. from sklearn.metrics import classification_report
5. import pandas as pd
6. from sklearn.utils import shuffle
7. import numpy as np
8. def load():
9. df_data = pd.read_csv(os.path.join(os.path.dirname(os.path.abspath(__file__)), 
 "data.csv"))
10.
11. df_data = shuffle(df_data)
12. df_data.reset_index(inplace=True, drop=True)
13.
14. train, test = np.split(df_data, [int(len(df_data)*0.8)])
15.
16. train_set = {'sentences': train.text_de.tolist(), 
 'intents': train.intent_index.tolist()}
17. test_set = {'sentences': test.text_de.tolist(), 'intents': test.intent.tolist()}
18. return train_set, test_set
Sie sehen an den Imports, dass wir SpaCy und dessen Textklassifikations-Pipeline verwenden, 
um unsere Intentionserkennung zu trainieren. Weiterhin setzen wir scikit-learn, pandas und 
NumPy ein – drei Bibliotheken, auf die Sie immer wieder stoßen werden, wenn Sie sich mit 
maschinellem Lernen und der Manipulation und Analyse von Daten beschäftigen.
Listing 5.1 beginnt mit der Funktion load, die eine CSV-Datei (Comma-separated Values) ein liest. Diese ist wie in Tabelle 5.1 zu sehen aufgebaut.



96 5 Dialoge und Intentionen
Tabelle 5.1 Trainings- und Testdaten für die Textklassifikation
text_en text_de text_fr text_es intent intent_index
… Buche Restaurant in Ulm … … BookRestaurant 1
… Füge Pink zu meiner Playlist 
hinzu
… … AddToPlaylist 0
… Wie ist das Wetter in Landau … … GetWeather 2
… Spiele Musik von Queen … … PlayMusic 3
Die Daten entstammen einem NLU (Natural Language Understanding) Benchmark von Sonos1
. 
Ich habe sie jedoch bereits ein wenig umformatiert, sodass sie leichter auf unser Training 
anzuwenden sind. Im Beispiel 100_extras/100_04_intentclassifier finden Sie das Skript 
preprocessing.py, das für diese Vorverarbeitung verantwortlich ist. Es fügt die einzelnen, mit 
Tags versehenen Satzteile zu ganzen Sätzen zusammen und übersetzt sie aus dem Englischen 
(text_en) ins Deutsche (text_de) und für alle Interessenten auch noch ins Spanische (text_es) 
und Französische (text_fr). Wir werden jedoch alleine mit der deutschen Übersetzung arbei ten. Dazu kommt noch das Label, das den eigentlichen Intent angibt, sowie eine numerische 
Repräsentation dieses Intents in intent_index.
TIPP: Wenn Sie sich schon ein wenig länger mit KI beschäftigen, werden Sie ge merkt haben, dass Fortschritte meist erst in Beispielen in englischer Sprache prä-
sentiert werden. Das ist aus meiner Sicht in Ordnung, sprechen wir diese Sprache 
doch fast alle und haben somit eine gemeinsame Grundlage, um die Entwicklung 
kollaborativ voranzutreiben. Doch in diesem Fall macht es uns natürlich die Arbeit 
etwas schwer, denn wir wollen ja deutsche Texte klassifizieren können und nicht 
nur englische. Bei Modellen auf Basis numerischer Werte müssen wir vielleicht 
höchstens Metriken anpassen, aber nichts übersetzen. Zum Glück ist es aber 
heutzutage gar nicht mehr so schwer, Texte von einer Sprache in die andere zu 
übersetzen. Das Beispiel aus der Vorverarbeitung für das Beispiel der Textklassi fikation möchte ich Ihnen nicht vorenthalten.
Listing 5.2 Maschinelle Übersetzung mit Transformers
1. from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
2. import torch
3.
4. device = „cuda:0“ if torch.cuda.is_available() else „cpu“
5. tokenizer_en_de = AutoTokenizer.from_pretrained
 („Helsinki-NLP/opus-mt-en-de“)
6. model_en_de = AutoModelForSeq2SeqLM.from_pretrained
 („Helsinki-NLP/opus-mt-en-de“).to(device)
7.
8. text = „This example has to be translated.“
9.
10. with tokenizer_en_de.as_target_tokenizer():
1 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/sonos/nlu-benchmark



5.1 Intent Recognition – Die menschliche Intention verstehen lernen 97
11. tokenized_text = tokenizer_en_de(text, return_tensors=‘pt‘).to(device)
12. translation = model_en_de.generate(**tokenized_text)
 return tokenizer_en_de.batch_decode(translation, 
 skip_special_tokens=True)[0]
Listing 5.2 lädt jeweils ein Modell für einen Tokenizer, der die zu übersetzende 
Zeichenkette in ihre Einzelteile zerlegt, sowie ein Sequence-To-Sequence-Modell, 
das, wie der Name vermuten lässt, Datenstrukturen einer Sequenz in eine andere 
Sequenz transformieren kann.
Die bereits angesprochene Bibliothek transformers von Hugging Face ist hier eine 
große Erleichterung, denn neben der Klasse, die uns Initialisierung und Benutzung 
einfach machen, hostet das Unternehmen auch die entsprechenden Modelle. In 
unserem Fall ist das opus-mt-en-de der Universität Helsinki. Indem wir dessen 
Namen bei der Initialisierung von AutoTokenizer und AutoModelForSeq2SeqLM an geben, werden diese vom Hugging Face Model Hub2
 heruntergeladen. Dort finden 
Sie zahlreiche vortrainierte Modelle für verschiedenste Aufgaben aus dem Speech und NLP-Bereich. Die Universität Helsinki bietet dort kostenfrei 1334 Modelle an, 
die jeweils von einer in eine andere Sprache übersetzen (alleine für die Überset zung ins Deutsche bzw. aus dem Deutschen sind dort 52 Modelle zu finden). Die 
Qualität der Übersetzung ist bis auf einige wenige Ausnahmen, die vorrangig durch 
Eigennamen hervorgerufen werden, hervorragend. Sie können sich davon aber ein 
eigenes Bild machen, indem Sie sich die Trainingsdaten in data.csv anschauen.
In Listing 5.2 wird nun der Text in Zeile 11 in einzelne Tokens zerlegt und in 
Batches decodiert, also aus der internen Repräsentation zurück in Text umgewan delt. Ein besonderes Augenmerk soll noch auf Zeile 3 und die Aufrufe to(device)
gelegt werden. Wir prüfen hier, ob CUDA auf dem System installiert ist, sprich ob 
wir die Inference der Modelle auf einer GPU rechnen können. Wenn device also 
ein CUDA-Device referenziert, kann das Sequence-to-Sequence-Modell auf die 
GPU übertragen werden. Um es anzuwenden, müssen sich auch die Tokens im 
GPU-Speicher befinden. Dieser Transfer geschieht in Zeile 11. Sollte keine GPU 
zu Verfügung stehen, wird das Device cpu verwendet und alle Operationen finden 
eben auf der CPU statt.
Sind die Daten in ein sogenanntes Pandas DataFrame geladen, mischen wir sie in Listing 5.1
in Zeile 11 durch. Das hat den Sinn und Zweck, dass wir beim Aufteilen der Daten in Zeile 14 
nicht nur Texte eines Intents in den Testdaten haben. Das wäre nämlich sehr wahrschein lich, da die Daten nach Intent geordnet in der CSV-Datei abgespeichert sind. Wir erstellen 
in Zeile 12 einen neuen Index für das DataFrame und nutzen dann numpy.split(), um 
80 % der Daten für das Training und den Rest für die Tests zurückzulegen. Haben Sie be reits das ein oder andere Machine-Learning-Buch gelesen, werden Sie wohl die Methode 
train_test_split() von scikit-learn verwendet haben, die eben auch diese Aufgabe erfüllt. 
Mit numpy.split können wir einen Datensatz aber auch in mehrere Gruppen aufteilen, falls 
wir zum Beispiel noch einen Validierungsdatensatz benötigen. train_test_split() ist jedoch auf 
zwei Gruppen begrenzt.
2 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichhuggingface.co/models



98 5 Dialoge und Intentionen
In Zeile 16 und 17 konstruieren wir ein Dictionary (zu erkennen an den geschweiften Klam mern) und fügen einen Eintrag mit dem Index sentences und einen mit dem Index intents
hinzu. Nun ziehen wir einzelne Spalten aus dem DataFrame und konvertieren sie in eine 
Liste. Für den Trainingsdatensatz benutzen wir die deutschen Sätze und die numerischen 
Indizes der Intents. Für den Testdatensatz benutzen wir ebenfalls die deutschen Sätze, jedoch 
die Namen der Intents. Am Ende liefern wir beide Dictionaries per Return zurück.
Es folgt eine Methode zur Evaluierung des Modells, zu sehen in Listing 5.3.
Listing 5.3 Textklassifikation – Evaluierung der Trainingsergebnisse
1. def evaluate(tokenizer, textcat, test_texts, test_cats):
2. docs = (tokenizer(text) for text in test_texts)
3. preds = []
4. for i, doc in enumerate(textcat.pipe(docs)):
5. scores = sorted(doc.cats.items(), key = lambda x: x[1],reverse=True)
6. catList=[]
7. for score in scores:
8. catList.append(score[0])
9. preds.append(catList[0])
10.
11. labels = ["AddToPlaylist", "BookRestaurant", "GetWeather", "PlayMusic",
12. "RateBook", "SearchCreativeWork", "SearchScreeningEvent"]
13.
14. print(classification_report(test_cats, preds, labels=labels))
Die Sätze aus den Testdaten werden einzeln in die Textklassifikations-Pipeline eingespeist 
und die resultierenden Scores gesammelt, der Größe nach sortiert, der beste selektiert und 
dessen Kategorie als Zeichenkette in der Liste preds gespeichert. Alle Testtexte, die jeweiligen 
Vorhersagen und die dazugehören Labels werden der Funktion classification_report()
übergeben, die diese auswertet und textuell aufbereitet, sodass wir sie über die Konsole aus geben können. Bild 5.2 zeigt diese Ausgabe einmal nach der ersten Iteration des Trainings 
und einmal nach der zehnten.
Wie viel Arbeit scikit-learn uns mit classification_report() abnimmt, sieht man, wenn 
man sich die Berechnung der in Bild 5.2 aufgelisteten Metriken ansieht. Was aber sagen 
diese Werte genau aus? Es ist zu sehen, dass Precision, Recall F1-Score und Support für jede 
Klasse im Modell abgebildet werden.
 Precision (Genauigkeit): Wenn das Modell einen Text als Intent der Kategorie AddToPlaylist
klassifiziert, wie oft hat es damit recht?
 Recall (Trefferquote): Wenn der Intent wirklich der Klasse AddToPlaylist angehört, wie 
oft ist die Klassifikation richtig?
 F1-Score: Ist der gewichtete Durchschnitt zwischen Precision und Recall. Dieser sagt 
aus, wie gut der Classifier für diese eine bestimmte Klasse im Vergleich zu allen anderen 
Klassen ist.
 Support: Der Support gibt an, wie oft ein Intent der jeweiligen Klasse in den Testdaten 
vorkommt.
Die Berechnung von Precision, Recall und F1-Score ist nicht allzu kompliziert und kann 
durch drei einfache Formeln abgebildet werden.



5.1 Intent Recognition – Die menschliche Intention verstehen lernen 99
= +
True Positive Precision 
True Positive False Positive
Formel 5.1
= +
True Positive Recall 
True Positive False Negative
Formel 5.2
⋅ = ⋅ +
Precision Recall F1-Score 2 
Precision Recall
Formel 5.3
Was aber sind True Positive, False Positive, True Negative und False Negative? Die folgende 
Liste liefert die Erklärung:
 True Positive (TP): Wie viele der Intents wurden korrekterweise einer Klasse zugeordnet?
 False Positives (FP): Wie viele der Intents wurden fälschlicherweise einer Klasse zu geordnet?
 True Negatives (TN): Wie viele Intents wurden korrekterweise einer Klasse nicht zu geordnet?
 False Negatives (FN): Wie viele Intents wurden fälschlicherweise einer Klasse nicht zu geordnet?
Bild 5.2 Trainingsfortschritt des Testklassifikators nach einer und nach zehn Iterationen und eine 
Klassifikation für zwei unbekannte Sätze auf Basis des fertig trainierten Modells. Die jeweils wahr scheinlichsten Klassen sind rot unterstrichen.



100 5 Dialoge und Intentionen
Zum Glück müssen wir auch diese Auswertung nicht selbstständig vornehmen, gibt es dafür 
doch die sogenannte Confusion Matrix, die diese vier Werte für jede Kombination aus tatsäch licher Klasse (Bild 5.3, y-Achse) eines Intents und vorhergesagter Klasse (Bild 5.3, x-Achse) 
eines Intents abbildet. In der allerersten Zeile sehen wir, dass 387 Datensätze der Kategorie 
AddToPlaylist richtig zugeordnet wurden. Der Wert rechts davon, die 3, sagt weiterhin aus, 
dass 3 Intents der Klasse BookRestaurant fälschlicherweise AddToPlaylist zugeordnet wurden, 
wohingegen kein GetWeather-Intent der Trainingsdaten der Klasse AddToPlaylist zugeordnet 
wurde und so weiter Es handelt sich hierbei um die False Positives, zu sehen in dem blauen Rahmen in 
Bild 5.3. Demgegenüber stehen die False Negatives, die durch den roten Rahmen markiert sind. 
2 Intents der Kategorie AddToPlaylist sind fälschlicherweise PlayMusic zugeordnet worden.
False Posives
Bild 5.3 Confusion Matrix nach dem 10. Schritt des Trainings des Text Classifiers
HINWEIS: Um es mal mit den Worten eines Fernsehkochs zu sagen: Die Confu sion Matrix aus Bild 5.3 können Sie mit nur wenigen Handgriffen und Zutaten ruck, 
zuck selber zaubern. Fügen Sie einfach das nachstehende Listing vor Zeile 11 von 
Listing 5.3 ein (Imports und rcParams.update gerne an den Kopf der Datei).
Listing 5.4 Textklassifikation – Plotten der Confusion Matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from matplotlib import rcParams
rcParams.update({'figure.autolayout': True})
cm = confusion_matrix(test_cats, preds, labels=labels)
ConfusionMatrixDisplay(cm, display_labels=labels).plot()



5.1 Intent Recognition – Die menschliche Intention verstehen lernen 101
plt.xticks(rotation=90)
plt.xlabel(„Vorhergesagte Klasse“)
plt.ylabel(„Tatsächliche Klasse“)
plt.show()
Ich nutze die Möglichkeiten der Visualisierung sehr gerne, hilft die Heatmap doch 
schnell zu verstehen, wie gut ein Modell ist. Wenn Sie das Beispiel ausführen, 
müssen Sie das Fenster jeder Confusion Matrix schließen, bevor das Training wei tergeht. Achten Sie dabei mal auf die positive Veränderung von der ersten Itera tion bis hin zur zehnten!
Unser Classifier ist also recht treffsicher, denn wir haben in Summe kaum False Positives oder 
False Negatives. Die größte Anzahl Ausreißer finden wir dort, wo 17 Intents SearchCreativeWork
zugeordnet wurden, obwohl sie zu SearchScreeningEvent gehören.
Nach unserem Ausflug in die Evaluierung unseres Modells können wir uns endlich an schauen, wie das Modell trainiert wird. Listing 5.5 zeigt die Methode train(), in der das 
erste Mal zu sehen ist, dass wir die Klassifikation auf SpaCy aufsetzen. Wir laden zuerst das 
vortrainierte Modell de_core_news_sm und entfernen alle Pipelines außer der für die Text klassifikation (siehe Zeile 4). Von Zeile 9 bis 15 fügen wir nun manuell alle Labels hinzu, 
die unsere Klassen repräsentieren.
Listing 5.5 Textklassifikation – Training des Modells
1. def train(train_data, iterations, test_texts, test_cats, dropout = 0.3):
2. nlp = spacy.load("de_core_news_sm")
3.
4. textcat = nlp.create_pipe("textcat", config={"exclusive_classes": True,
5. "architecture": "ensemble"})
6. nlp.add_pipe(textcat, last=True)
7.
8. # Festlegen der Intents als Labels für den Classifier
9. textcat.add_label("AddToPlaylist")
10. textcat.add_label("BookRestaurant")
11. textcat.add_label("GetWeather")
12. textcat.add_label("PlayMusic")
13. textcat.add_label("RateBook")
14. textcat.add_label("SearchCreativeWork")
15. textcat.add_label("SearchScreeningEvent")
16.
17. # Nur die Pipeline für den Text Classifier wird benötigt.
18. pipe_exceptions = ["textcat", "trf_wordpiecer", "trf_tok2vec"]
19. other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in
20. pipe_exceptions]
21. with nlp.disable_pipes(*other_pipes):
22. optimizer = nlp.begin_training()
23.
24. print("Beginne Training ...")
25. batch_sizes = compounding(16.0, 64.0, 1.5)



102 5 Dialoge und Intentionen
26. for i in range(iterations):
27. print('Iteration: '+str(i))
28. start_time = time.perf_counter()
29. losses = {}
30. random.shuffle(train_data)
31. batches = minibatch(train_data, size=batch_sizes)
32. for batch in batches:
33. texts, annotations = zip(*batch)
34. nlp.update(texts, annotations, sgd=optimizer, drop=dropout,
35. losses=losses)
36. with textcat.model.use_params(optimizer.averages):
37. evaluate(nlp.tokenizer, textcat, test_texts, test_cats)
38.
39. print ("Iteration " + str(i) + ": " +str(time.perf_counter() -
40. start_time)+ " Sekunden")
41.
42. with nlp.use_params(optimizer.averages):
43. modelName = "ensemble_model"
44. filepath = os.path.join(os.getcwd(), modelName)
45. nlp.to_disk(filepath)
46. return nlp
Es folgt das Training ab Zeile 22. Wir legen zu Beginn die batch_sizes fest, die bestimmen, 
wie viele Daten in jeder Iteration in das Training eingespeist werden. Hier wird keine feste 
Größe verwendet, sondern der Generator compounding, der bei jedem Aufruf einen inter polierten Wert zwischen 16 und 64 zurückgibt. Die sequenzielle Erhöhung der Batch Size 
hat den Effekt, dass das Training massiv beschleunigt wird; wie immer gilt: Probieren Sie 
es gerne mal aus.
Nach dem Erstellen der tatsächlichen Batches in Zeile 31 führen wir ein Update des Mo dells aus Zeile 34 aus und evaluieren dieses im Nachgang in Zeile 37 über die Funktion 
evaluate(), die wir ja bereits kennengelernt haben.
Nun haben wir die Methoden für die Evaluation, das Datenladen und das Training beisam men. Im letzten Schritt müssen wir diese nur noch miteinander verknüpfen. Wir beginnen 
in Listing 5.6 damit, dass wir die Daten laden und die Spalten sentences und intents aus dem 
Trainingsdatensatz in separate Listen laden (siehe die Zeilen 3 bis 5). Nun erstellen wir ein 
etwas kryptisches Objekt, nämlich die cat_list, um die jeder Satz der Trainingsdaten ergänzt 
wird. Statt aber einfach jede Klasse mit Namen zu übergeben, benötigt SpaCy für das Training 
ein Dictionary, in dem jede Klasse namentlich berücksichtigt ist und wenn diese Klasse für 
den Datensatz relevant ist, wird sie mit einer 1 versehen. Wenn die Klasse nicht relevant ist, 
wird sie mit einer 0 versehen. Dieses Konstrukt erstellen wir von Zeile 7 bis 17. In Zeile 20 
wird dann jeder Text der Trainingsdaten mit dem passenden Dictionary cat_list gezippt, also in 
ein Tuple gepackt. Die Liste all dieser Tuples machen letztendlich unsere Trainingsdaten aus.
Es folgt die vergleichsweise einfache Konstruktion der Testdaten, die lediglich darauf basiert, 
dass test_texts und test_cats aus dem Objekt test_set ausgelesen wird. test_cats enthält in 
diesem Fall die Namen der Klassen, nicht den intent_index wie in den Trainingsdaten! Das 
hatten wir ja in der Methode load() gesehen.



5.1 Intent Recognition – Die menschliche Intention verstehen lernen 103
Listing 5.6 Textklassifikation – Aufruf von Datenladen, Training und Evaluation in der Hauptmethode
1. if __name__ == '__main__':
2.
3. train_set, test_set = load()
4. train_texts = train_set['sentences']
5. train_cats = train_set['intents']
6.
7. final_train_cats=[]
8. for cat in train_cats:
9.
10. cat_list = {'AddToPlaylist': 1 if cat == 0 else 0,
11. 'BookRestaurant': 1 if cat == 1 else 0,
12. 'GetWeather': 1 if cat == 2 else 0,
13. 'PlayMusic': 1 if cat == 3 else 0,
14. 'RateBook': 1 if cat == 4 else 0,
15. 'SearchCreativeWork': 1 if cat == 5 else 0,
16. 'SearchScreeningEvent': 1 if cat == 6 else 0,
17. }
18. final_train_cats.append(cat_list)
19.
20. training_data = list(zip(train_texts, [{"cats": cats} for cats in
21. final_train_cats]))
22.
23. test_texts = test_set['sentences']
24. test_cats = test_set['intents']
25.
26. nlp = train(training_data, 10, test_texts, test_cats, "ensemble")
27.
28. test1 = "Buche ein Restaurant für heute Abend".lower()
29. test2 = "Füge alle Musik von Blink 182 meiner Playlist hinzu".lower()
30. nlp2 = spacy.load(os.path.join(os.getcwd(), "ensemble_model"))
31. doc2 = nlp2(test1)
32. print("Text: "+ test1)
33. print(doc2.cats)
34.
35. doc3 = nlp2(test2)
36. print("Text: "+ test2)
37. print(doc3.cats)
In Zeile 26 kommt es zum Aufruf des eigentlichen Trainings. Die Parameter training_data, 
test_texts, test_cats sollen uns nicht mehr überraschen. Die 10 gibt die 10 Iterationen an, die 
wir trainieren wollen. Das ensemble gibt Auskunft über die Modellarchitektur, Alternativen 
sind hier zum Beispiel bow- oder simple_cnn-Architekturen, ersetzen Sie also ensemble mal durch diese 
Begriffe. SpaCy führt für diese Architekturen eine vollständige Liste in der Dokumentation3
.
Ist das Training abgeschlossen, sind wir bereit, Validierungssätze zu definieren, hier test1
und test2. Das eben gespeicherte Modell wird in Zeile 30 geladen und auf die zwei Sätze an gewandt. Indem wir doc2.cats ausgeben, erfahren wir die ermittelten Klassen. Wie schon in der 
Confusion Matrix in Bild 5.3 zu sehen, treffen wir die Kategorien sehr gut. Und das, obwohl wir 
unsere Test- und Trainingsdaten zuvor durch einen maschinellen Übersetzer erzeugt haben.
3 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichspacy.io/api/architectures



104 5 Dialoge und Intentionen
5.1.2 Intent Parsing am Beispiel
Nun können wir also erkennen, ob ein Satz eine bestimmte Intention hat. Wie aber nehmen 
wir diesen nun so auseinander, dass wir die wichtigen Parameter darin identifizieren und 
extrahieren können? Zu wissen, dass der Computer einen speziellen Ort suchen soll, ist ja 
schön und gut, aber zu wissen was für einen Ort genau, ist obligatorisch. Und wenn wir 
mehrere Parameter haben, müssen wir diese ja auch noch einem Objekt zuordnen! Schauen 
wir uns mal ein Beispiel an.
Bild 5.4 zeigt einen sogenannten Dependency Visualizer auf Basis von SpaCy, der syntaktische 
Zusammenhänge zwischen einzelnen Begriffen hervorhebt. Die Labels der Klassen sind 
jedoch von uns festgelegt, oder richtig ausgedrückt: antrainiert. Sie sehen, dass wir so zum Beispiel 
bei der Ortssuche identifizieren können, was gesucht werden soll: einen Urlaubsort. Dieser 
Urlaubsort wiederum muss laut Anfrage eine gewisse Eigenschaft aufweisen, nämlich sehr 
sonnig zu sein. Schauen wir uns mal an, wie die Trainingsdaten für so einen Fall beschaffen 
sein müssen.
QUALITY
PLACE
ATTRIBUTE
Finde einen Urlaubsort mit viel Sonne
Bild 5.4 Dependency Visualizer mit eigenen Labels für einen Intent zur Ortssuche
Listing 5.7 Trainingsdaten für den Intent Parser
TRAIN_DATA = [
 (
 "Finde einen Urlaubsort mit viel Sonne",
 {
 "heads": [0, 2, 0, 5, 5, 2],
 "deps": ["ROOT", "-", "PLACE", "-", "QUALITY", "ATTRIBUTE"],
 }
 ),
 ...
]
Listing 5.7 zeigt einen exemplarischen Datensatz aus dem Beispiel 100_extras/100_05_
intentparser, das wir uns nun anschauen wollen, um damit zu experimentieren. Der Daten satz weist zu Beginn einen einfachen Satz ohne Satzzeichen auf. Wir trainieren das Modell 
ohne Satzzeichen, da es uns um den Kontext geht, in dem die Wörter stehen. Es macht kaum 
einen Unterschied, wenn ein Ausrufezeichen oder ein Punkt am Satzende steht. Ob es sich 
um eine Frage handelt, erkennt das Modell in der Regel am Satzbau.



5.1 Intent Recognition – Die menschliche Intention verstehen lernen 105
TIPP: Wie ist es denn mit Groß- und Kleinschreibung in Trainingsdaten? Im Eng lischen werden Sätze häufig komplett kleingeschrieben, da es dort kaum zum 
Kontext beiträgt, ob ein Satz Großbuchstaben enthält, denn es werden nur Satz anfänge und Eigennamen großgeschrieben. Im Deutschen ist es etwas anders. 
Ein Beispiel:
Der Junge sieht dir Ungeheuer ähnlich.
Der Junge sieht dir ungeheuer ähnlich.
Was uns vielleicht zum Schmunzeln bringt, kann zu falschen Assoziationen in un serem Modell führen und unseren Sprachassistenten entsprechend beeinflussen. 
Wir passen als Konsequenz die Groß- und Kleinschreibung im Deutschen nicht an.
Jedem Satz ist eine numerische Liste mit dem Namen heads zugeordnet. Jedes Token hat 
ein Head-Element, das von SpaCy als syntaktisches Elternelement bezeichnet wird und über 
dessen Index in heads referenziert wird. Diese Zahl darin referenziert ein anderes Wort, auf 
das sich das ursprüngliche Wort bezieht. Die sich aus Listing 5.7 ergebenden Abhängigkeiten 
sind in Tabelle 5.2 dargestellt. Das Elternelement von Urlaubsort ist beispielsweise Finde, 
das von viel ist Sonne. Wenn Sie in naher Zukunft selber Testdaten erstellen, werden Sie 
sich überlegen, welche Regeln Sie dafür anwenden können. Tatsächlich ist der Gedanke des 
hierarchisch übergeordneten Wortes für mich immer am passendsten gewesen.
Tabelle 5.2 Tabellarische Darstellung eines Trainingsdatensatzes für das Intent Parsing
Wort Head Label
Finde Finde ROOT
einen Urlaubsort –
Urlaubsort Finde PLACE
mit Sonne –
viel Sonne QUALITY
Sonne Urlaubsort ATTRIBUTE
Wenn Sie jedoch viele hundert Datensätze taggen müssen, empfehle ich Ihnen, auf SpaCy 
zurückzugreifen, denn über folgendes Listing bekommen Sie die heads anhand eines vor trainierten Modells relativ einfach ausgegeben.
doc = nlp("Finde einen Urlaubsort mit viel Sonne")
for token in doc:
 print(token.text, token.head.text)
Wichtig ist noch, dass zirkuläre Referenzen verboten sind, weswegen beispielsweise Finde 
→ Urlaubsort und gleichzeitig Urlaubsort → Finde eine unzulässige Zuweisung in den Head Elementen ist.



106 5 Dialoge und Intentionen
Beschäftigen wir uns nun damit, dass wir die Beziehungen, die wir zwischen Wort und Head 
gebildet haben, beschreiben oder besser: mit Labels versehen. Die Spalte Label in Tabelle 5.2
bildet dieses Labeling ab und kennzeichnet Finde als das Wurzelelement, Urlaubsort als einen 
Ort, viel als qualitatives Merkmal und Sonne als Attribut.
Nun, da wir wissen, wie wir unsere Trainingsdaten für das Intent Parsing vorbereiten müssen, 
können wir das Training starten. Wir erzeugen zunächst eine leere Pipeline für die deutsche 
Sprache in Zeile 10 und fügen einen Parser hinzu. Dann holen wir aus jedem Eintrag aus 
TRAINING_DATA (hier nicht abgebildet) den Text und die Annotationen und fügen die Labels 
dem Parser hinzu (Zeile 15), sodass dieser weiß, welche Worte wie gelabelt werden sollen.
Listing 5.8 Training und Evaluierung des Intent Parsings
1. import random
2. from pathlib import Path
3. import spacy
4. from spacy.util import minibatch, compounding
5.
6. if __name__ == "__main__":
7. iterations=15
8. nlp = spacy.blank("de")
9.
10. parser = nlp.create_pipe("parser")
11. nlp.add_pipe(parser, first=True)
12.
13. for text, annotations in TRAIN_DATA:
14. for dep in annotations.get("deps", []):
15. parser.add_label(dep)
16.
17. pipe_exceptions = ["parser", "trf_wordpiecer", "trf_tok2vec"]
18. other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in
19. pipe_exceptions]
20. with nlp.disable_pipes(*other_pipes):
21. optimizer = nlp.begin_training()
22. for itn in range(iterations):
23. random.shuffle(TRAIN_DATA)
24. losses = {}
25. batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
26. for batch in batches:
27. texts, annotations = zip(*batch)
28. nlp.update(texts, annotations, sgd=optimizer, losses=losses)
29. print("Losses", losses)
30.
31. texts = [
32. "finde ein hotel mit gutem internet",
33. "suche das günstiges fitnessstudio nahe der arbeit",
34. "suche das beste restaurant in berlin",
35. ]
36.
37. docs = nlp.pipe(texts)
38. for doc in docs:
39. print(doc.text)
40. print([(t.text, t.dep_, t.head.text) for t in doc if t.dep_ != "-"])



5.1 Intent Recognition – Die menschliche Intention verstehen lernen 107
Wie auch beim Intent Classifier schließen wir sämtliche Schritte aus der Pipeline aus, die wir 
nicht benötigen (Zeilen 17–20) und beginnen das Training für 15 Iterationen. Das Beispiel 
ist sehr ähnlich zu dem vorigen, wie Sie sehen werden. Nach dem letzten Schritt wenden 
wir den Parser auf drei unbekannte Sätze an und schauen, wie diese mit Labels versehen 
werden (siehe Bild 5.5).
Bild 5.5 Das Loss reduziert sich mit jeder Trainingsiteration, sodass nach der fünfzehnten Trainings iteration Parameter und Beziehungen innerhalb der drei Testsätze richtig erkannt und zugeordnet 
werden.
Mit nlp.to_disk(output_dir) kann das Modell übrigens auch wie im Beispiel in Ab schnitt 5.1.1 gespeichert werden. Nun haben wir gesehen, wie wir Intents klassifizieren 
und wie wir sie dann auf mögliche Parameter untersuchen. Sollte also die Aufgabe sein, eine 
Suche nach einem Ort zu implementieren, können wir nun herausfinden, ob der Benutzer 
eine solche Ortssuche aufrufen soll und nach welchen Orten mit welchen Eigenschaften wir 
etwa in Google Maps nachschlagen müssen.
Da wir nun der Aufgabe gewachsen wären, genau das umzusetzen, möchte ich Sie erneut 
nüchtern damit konfrontieren, dass es Frameworks gibt, die das bereits wesentlich besser 
machen als wir und auf die wir aufbauen werden. Dennoch war die Arbeit nicht umsonst, 
haben wir doch nun ein grundlegendes Verständnis von den Anforderungen, die wir an ein 
solches Framework stellen müssen. Die Auswahl möglicher Kandidaten ist Teil des folgenden 
Abschnitts.



108 5 Dialoge und Intentionen
■ 5.2 Auswahl eines Chatbot-Frameworks
Das Erste, was Ihnen sicher bei dem Titel auffallen wird, ist, dass dort Chatbot steht. Dieser 
Begriff erweckt bei manchen sicher erst mal Assoziationen zu Anwendungen wie IKEAs 
LATTJO oder anderen, eher unpersönlichen Dialog Engines diverser Websites, die darauf 
programmiert sind, bei Fragen weiterzuhelfen, wie etwa über Preise Auskunft zu geben 
oder zu berichten, wo im Shop man Einbauschränke findet. Die Herausforderung, wie wir ja 
bereits in Abschnitt 5.1.1 und Abschnitt 5.1.2 gesehen haben, ist, auf unvorhergesehene Sätze 
richtig zu reagieren. Nicht gemeint ist irgendeine Art von Visualisierungswerkzeug, dass 
es uns erlaubt, Texte in eine dafür vorgesehene Box einzugeben und Dialoge anzuzeigen. Da 
wir einen Sprachassistenten entwickeln, haben wir dafür sowieso eher wenig Verwendung.
Wir werden nun die Auswahl zweier Frameworks treffen. Das erste soll uns helfen, relativ 
statischen, wenig dynamischen Text gut zu erkennen und auf Basis von Regular Expressions
dessen Intention zu klassifizieren und zu analysieren. Ein Beispiel dafür wäre etwa die Frage 
nach der Uhrzeit. Das zweite Framework hingegen wird auf maschinellem Lernen basieren 
und anhand von Trainingsdaten entscheiden, welche Intention der Benutzer hat und was 
mögliche Parameter sind. Hier wäre etwa die Ortssuche zu nennen, die wir im vorigen Ab schnitt exemplarisch behandelt haben.
Nun gibt es viele verschiedene Bibliotheken, die diese Arbeit sehr gut machen. Die bekann testen habe ich Ihnen in Tabelle 5.3 zusammengestellt. Hierbei war eine der Anforderungen, 
dass diese Frameworks offline funktionieren müssen, also keine Daten herausgeben dürfen, 
die irgendwo anders verarbeitet werden. Damit fallen zwar eine Hand voll vielversprechender 
Kandidaten raus, wie etwa Dialogflow von Google, aber wir bleiben dafür unseren Prinzipien 
treu und werden – so viel sei schon mal gesagt – dennoch sehr gute Ergebnisse erzielen. 
Weitere Anforderungen waren, dass eines der beiden zu nutzenden Frameworks anhand 
von Trainingsdaten die Klassifikation und das Intent Parsing erlernen kann – und dass es 
die deutsche Sprache unterstützt.
Rasa NLU ist die erste Bibliothek, die sich bei der Recherche finden lässt, ist es doch eines der 
größten Vertreter am Markt und wird Open Source und kommerziell vertrieben. Hinter Rasa 
steht ein großes Team von Experten, die Open-Source-Version hat beinahe 13.000 Sterne auf 
Github. Es gibt hunderte Anleitungen, hunderte Videos, die dessen Funktion erklären – aber 
das ist leider auch nötig, denn das Framework ist so komplex, dass es mittlerweile schon ein 
eigenes Buch dazu gibt (Kong, Wang, & Nichol, 2021).
Tabelle 5.3 Vergleich bekannter Chatbot-Bibliotheken in Python
Name Machine Learning Intent-Behandlung Sprachen
Rasa NLU Ja Ja Mehrere
ChatterBot Ja Nein Mehrere
ChatbotAI Nein Ja Deutsch, Englisch
Snips-NLU Ja Ja Mehrere
Transformers Ja Nein Mehrere
DeepPavlov Ja Ja Mehrere



5.2 Auswahl eines Chatbot-Frameworks 109
ChatterBot ist eine mittlerweile auch sehr weitverbreitete, offene Dialog Engine, die auf Basis 
vergangener Konversationen angelernt wird und sich mit dem daraus resultierenden Modell 
mit Benutzern unterhalten kann. Neue Antworten von Gesprächspartnern fließen direkt 
wieder ins nächste Training ein, sodass der Bot automatisch dazulernt. ChatterBot ist eben falls mit 11.500 Sternen auf Github überaus beliebt. Es kann allerdings noch keine Intents 
behandeln, was uns lediglich erlauben würde, mit dem Benutzer unseres Sprachassistenten 
nette Gespräche zu führen, nicht aber zu verstehen, was er oder sie möchte. In diesem Fall 
fehlt also konkret die NLU-Komponente.
ChatbotAI ist ein eher unauffälligeres Framework mit rund 300 Sternen. Da es einen Ansatz 
verfolgt, der auf Regular Expressions und nicht auf Machine Learning basiert, hat es im 
Moment auch nicht die Sichtbarkeit, die andere Bibliotheken haben. Dazu kommt, dass es 
neben der Sprache Englisch nur noch Hebräisch, Deutsch und brasilianisches Portugiesisch 
unterstützt. Allerdings sind beliebige Sprachen sehr einfach hinzuzufügen (ich hatte damals 
die Unterstützung für Deutsch beigesteuert), sodass die Funktionalität in der Hinsicht er weiterbar ist. Der Vorteil dieses Frameworks ist, dass es sehr einfach zu verstehen, leicht gewichtig und dennoch sehr mächtig ist.
Snips-NLU ist eine Bibliothek, die alle unsere Anforderungen erfüllt und dazu noch sehr 
leicht zu verstehen und zu implementieren ist. Allerdings ist Snips.co im November 2019 von 
Sonos aufgekauft worden, wahrscheinlich um dort eine Art Smart Speaker zu entwickeln. 
Als Konsequenz erfährt Snips-NLU keine Weiterentwicklung mehr, doch obwohl es lediglich 
in Version 0.20.2 veröffentlicht und auf Open Source umgestellt wurde, erfüllt es alle An forderungen an eine NLU-Bibliothek, die wir für einen Sprachassistenten benötigen würden.
Transformers von Hugging Face haben wir ja bereits kennengelernt, zum Beispiel als wir unsere 
Trainingsdaten in den vorherigen Abschnitten maschinell übersetzt haben. Da sich deren 
schlaue Leute in Paris und New York nicht nur mit Übersetzungen beschäftigen, sondern sich 
im Großen und Ganzen NLP und NLU widmen, existieren dort auch Modelle und Module zur 
Bereitstellung eines Chatbots. Jedoch ist das Framework nicht auf das Führen von Dialogen 
ausgelegt, das müssten wir umständlich nachimplementieren. Zwar gibt es fertige Modelle 
wie etwa DialoGPT von Microsoft, das einen ähnlichen Aufbau wie GPT-2 hat, ebenfalls auto regressiv ist, jedoch auf Dialogen von Reddit trainiert ist. Allerdings ist das Modell weder auf 
das Parsen von Intents trainiert, noch auf das Identifizieren deren Parameter.
Zuletzt sei noch DeepPavlov genannt, das auch eine beeindruckende Funktionalität hat und 
mit etwa 5000 Sternen auf Github eine gewisse Beliebtheit aufweist. Allerdings basiert es 
auf Deep Learning (Tensorflow oder PyTorch). Das macht es uns etwas schwer, es auf kleinen 
skalierten Geräten, etwa einem Raspberry Pi, zu betreiben, braucht die Anwendung der 
Modelle doch in der Regel eine GPU mit entsprechend Speicher, um den Benutzer nicht zu 
lange auf eine Antwort warten zu lassen.
HINWEIS: Nach dieser sicherlich nicht erschöpfenden Liste stellt sich natürlich noch 
die Frage, warum wir nicht zu NLTK oder SpaCy greifen, schließlich sind das ja die 
Werkzeuge überhaupt, wenn es um die Analyse von Texten geht. Nun ja, die Stärken 
dieser beiden Bibliotheken liegen tatsächlich in der Analyse, etwa in der Aufteilung 
in einzelne Tokens, dem POS-Tagging oder der Entitätsextraktion. Das heißt, beide 
bieten zwar die Funktionen, die vielen Dialog Engines zugrunde liegen, jedoch haben 
sie keine high-level-Features für die eigentliche Mensch-Maschine-Interaktion.



110 5 Dialoge und Intentionen
Die Auswahl, die ich nach einigen Tests getroffen habe, ist auf ChatbotAI und Snips-NLU
gefallen. Erstens aufgrund der genauen Abbildung der Funktionen, die wir für unseren 
Sprachassistenten benötigen. Wir möchten keine langen, möglichst glaubhaften Gespräche 
führen, sondern Befehle klassifizieren und verarbeiten können. Beide Bibliotheken sind nicht 
mit Funktionen überladen, die wir überhaupt nicht benötigen, und lassen sich sehr gut auf 
verschiedene Sprachen und Usecases anpassen. Ein weiterer Grund war für mich auch die 
Möglichkeit, anhand dieser Frameworks zu unterrichten. Wir werden mit ChatbotAI einen 
Durchstich, wie man so schön sagt, machen. So, dass wir schnell die ersten Funktionen auf 
die Straße bringen und wir erste Interaktionen per Sprache zulassen. Für anspruchsvollere 
Intents, die zuvor nicht von ChatbotAI erkannt wurden, werden wir dann Snips-NLU einset zen. Bild 5.6 verdeutlicht den Prozess noch einmal und zeigt auch gleich, wie wir einzelne 
Interaktionen in ChatbotAI (links) und mit Snips-NLU (rechts) definieren können; dazu aber 
gleich mehr.
 Sekundärer Prozess (Snips-NLU), um
 unbekannte Sätze über ein vortrainiertes
Modell zu erkennen,
 flexibel Intenonen zu verstehen,
Schlüsselwörter zu extrahieren 
und Akonen aufzurufen.
Wurde der Befehl nicht erkannt …
 Primärer Prozess (ChatbotAI), um
 fest vorgegebene Eingaben zu erkennen,
 Intenonen zu verstehen und Akonen 
aufzurufen.
Bild 5.6 Zwei sequenzielle Frameworks ermöglichen eine statische und eine dynamische Erkennung 
von Befehlen.
Eine Analogie, um zu erklären, warum wir hier zweigleisig fahren, ist, eine Klausur in der 
Schule oder im Studium zu schreiben. Wenn Sie die Frage bereits kennen, müssen Sie sie nicht 
großartig interpretieren, sondern die Antwort lediglich abspulen. Diese ist dann meistens 
zu einhundert Prozent korrekt. Wenn die Frage aber neu und unbekannt ist, muss sie erst 
interpretiert werden und Sie müssen sich Gedanken machen, wie die konkrete Fragestel lung überhaupt lautet, gegebenenfalls an den Parametern der Frage oder anhand ähnlicher 
Fragen, die Sie bereits anders formuliert gehört haben. Hier kann es sein, dass Sie nicht 
vollständig richtigliegen, wenn Sie nicht genug gelernt haben oder schlechtes Lernmaterial 
zur Hand hatten. Aber Sie sind dennoch besser, als wenn Sie eine gelernte Antwort geben, 
die überhaupt nicht richtig ist.
Sehen Sie den Vergleich der beiden Methoden auch gerne als Experiment, mit dem Sie sich 
für Ihren ganz individuellen Anwendungsfall ein Bild machen können, ob eine regelbasierte 



5.2 Auswahl eines Chatbot-Frameworks 111
oder eine Implementierung auf Basis von Machine Learning bessere Ergebnisse erzielt. Im 
realen Leben würden Sie sich wahrscheinlich irgendwann für eines der beiden entscheiden, 
denn mehrere Frameworks zu betreiben, bedeutet auch mehr Wartungsaufwände zum Beispiel für 
Updates oder Betrieb und womöglich auch mehr Lizenzkosten.
In Bild 5.7 sehen Sie eine Auflistung der Fragestellungen zu den beiden unterschiedlichen 
Ansätzen nach den Themen Verwendungszweck, Implementierungsaufwand, Güte und 
Performance.
Für welche Zwecke sind
starre und für welche
flexible Parser hilfreich?
• Ist ein eindeuger
Befehl zu erwarten?
• Werden längere
Konversaonen
geführt?
• Kommt es zu
Smalltalk?
Wie kompliziert sind die
jeweiligen Vorgehen
umzusetzen?
• Wie genau muss die
Konversaon vor besmmt werden?
• Wie schwer ist es,
eine weitere Sprache
hinzuzufügen?
Wie gut sind beide
Ansätze zu warten?
• Wie können Trainings daten oder Dialoge
erweitert werden?
• Wie prüfe ich die Güte
meines Chatbots?
Wie ist deren
Performance?
• Wie lange dauert es,
eine Antwort zu
generieren?
• Wie lange dauert es,
den Dialog
vorzubereiten?
Bild 5.7 Fragestellung zu dem Experiment starres vs. dynamisches Chat-Framework
Die erste Fragestellung zielt auf den Verwendungszweck des Frameworks in Ihrer Anwen dung ab. Gibt es eine klar formulierbare Aufgabe, womöglich eine Art Befehl, der sich in ein 
bis drei Wörter fassen lässt? Denkbare Beispiele wären Start/Stopp für einen Timer, Uhrzeit
oder Licht aus/an. Demgegenüber stehen längere Konversationen oder gar Smalltalk, der so 
variabel ist, dass er sich überhaupt nicht vorhersagen lässt.
Hinsichtlich der Komplexität der Umsetzung wird im zweiten Block von Bild 5.7 hinterfragt, 
wie genau Sie eine Konversation vorbestimmen müssen und wie komplex der Prozess der 
Entwicklung initial ist und wie sich auch eine Mehrsprachigkeit abbilden lässt. Reicht es etwa, 
eine Hand voll Sätze zu formulieren, die dem Framework genügen, um den Dialog eindeutig 
zu klassifizieren und zu analysieren? Oder müssen Sie, wie im Beispiel in Abschnitt 5.1.1, 
mehrere tausend Trainingsdatensätze formulieren?
Die dritte Frage beschäftigt sich mit der Wartung Ihrer Anwendung. Ist sie nachträglich er weiterbar? Oder sind die Regeln so starr auszuformulieren, dass jede Erweiterung hunderte 
von Entwicklerstunden verschlingt? Und wie stelle ich fest, dass Intentionen richtig erkannt 
werden und sich nicht mit anderen überschneiden? Sie sollten sich ebenso fragen, ob Ihr 
Sprachassistent auch nach dem ersten Release noch kontinuierlich weiterentwickelt wird, 
oder ob er nur alle paar Monate oder Jahre in einer neuen Version veröffentlicht wird.
Zu guter Letzt stellt sich natürlich auch noch die Frage nach der Performance. Wie schnell 
liefert ein Assistent eine Antwort? Wie lange dauert es, die Eingabe zu verarbeiten, die Ein gabe zu verstehen und die Ausgabe zu generieren? Der Mensch hat in der Regel recht wenig 
Geduld. Fragen Sie sich mal, wie lange Sie bereit sind zu warten, bis eine Website lädt. Oder 
wie anstrengend ein Gespräch mit einem Menschen sein könnte, der mehrere Sekunden 
benötigt, um zu antworten.



112 5 Dialoge und Intentionen
5.2.1 Intents auf Basis regulärer Ausdrücke
Machen wir uns endlich ans Eingemachte, fehlt einem ersten Assistenten doch nur noch die 
Fähigkeit, die Benutzereingabe zu verarbeiten und passend zu reagieren. Legen Sie dazu 
ein neues Beispiel mit dem Namen 07_dialoge_und_intents an und fügen Sie den benötigten 
Ordner und das Anaconda Environment hinzu. Die Abhängigkeiten, die Sie über pip vorins tallieren sollten, können Sie wie immer dem letzten Kapitel entnehmen.
Wir beginnen ganz seicht, indem wir zwei Intents für das Framework ChatbotAI in der main.
py definieren. Intents werden hier als Calls bezeichnet und müssen mit der Annotation 
@register_call versehen werden, die als Parameter den Namen des Calls mitgegeben 
bekommt, in Listing 5.9 time und stop. Eine solche Annotation weist darauf hin, dass eine 
Methode folgt. Schauen wir uns zuerst getTime() an. Diese Methode ruft in Zeile 6 zuerst 
das aktuelle Datum inklusive der Uhrzeit ab und konstruiert in den Zeilen 7–8 einen String, 
um die Uhrzeit in Worte zu fassen. Ein paar Worte zu den Parametern: session_id kann 
übergeben werden, um mehrere verschiedene Sitzungen zu handhaben, falls zum Beispiel mehrere 
Gesprächspartner mit dem Chatbot interagieren. In unserem Fall werden wir aber immer 
die Session general verwenden, da wir Benutzer und komplexere Dialoge selber verwalten. 
Die Variable dummy mag Ihnen etwas merkwürdig vorkommen. Zu Recht, hat sie doch keine 
weitere Funktion in der Logik des Intents. Jedoch ist Vorgabe von ChatbotAI, dass ein Call 
immer einen Parameter aufweisen muss. Und diesem muss dazu noch ein Wert zugewiesen 
sein, denn Python schreibt vor, dass einem Parameter mit einem Default-Wert (in unserem 
Fall session_id) kein Parameter folgen darf, der keinen Default-Wert besitzt. Demnach weisen 
wir dummy den Wert 0 zu, wir referenzieren aber weder die Variable noch deren Inhalt.
Listing 5.9 Definition zweier Calls für eine Zeitabfrage und zum Unterbrechen des Assistenten
1. from chatbot import Chat, register_call
2. from datetime import datetime
3.
4. @register_call("time")
5. def getTime(session_id = "general", dummy=0):
6. now = datetime.now()
7. return "Es ist " + str(now.hour) + " Uhr und " + str(now.minute) + "
8. Minuten."
9.
10. @register_call("stop")
11. def stop(session_id = "general", dummy=0):
12. if va.tts.is_busy():
13. va.tts.stop()
14. return "okay ich bin still"
15. return "Ich sage doch gar nichts"
Die Methode stop() wird ebenfalls mit @register_call annotiert und erhält session_id und 
dummy als Parameter. Sie dient dazu, den Sprachassistenten jeder Zeit zu unterbrechen, wenn 
wir stop sagen. Zu sehen ist in den Zeilen 12 und 13, dass geprüft wird, ob derzeit ein Text 
gesprochen wird. Die Methode is_busy() müssen wir allerdings in TTS.py noch nachtragen. 
Definieren Sie diese innerhalb der Klasse Voice wie in Listing 5.10 zu sehen.



5.2 Auswahl eines Chatbot-Frameworks 113
Listing 5.10 Prüfen, ob derzeit gesprochen wird oder nicht
def is_busy(self):
 if self.process:
 return self.process.is_alive()
Die Logik ist simpel: Ist der Prozess noch aktiv, wird True zurückgeliefert, andernfalls False. 
Sie können TTS.py nun wieder schließen und zur main Punkt Pei zurückkehren.
Abhängig davon, ob der Sprachassistent tatsächlich gesprochen hat oder nicht, geben wir 
eine entsprechende Rückmeldung in Form einer Zeichenkette, die den Befehl aus Sicht des 
Assistenten kommentiert. Zur Erinnerung: va definieren wir global bei der Initialisierung 
der Klasse VoiceAssistent am Ende der Datei.
Um ChatbotAI verwenden zu können, sollten Sie die Bibliothek über pip install ChatbotAI
in Ihr Environment installieren. Kümmern wir uns nun darum, dass das Framework weiß, 
wann es mit welchem Call auf die Benutzereingaben reagieren soll. ChatbotAI organisiert 
sich in sogenannten Templates. Legen Sie im Projektordner einen Ordner dialogs und darin 
eine Datei mit dem Namen dialogs.template an. Dessen Inhalt sollte dem aus Listing 5.11
entsprechen.
Listing 5.11 Template für die Intents stop und time
1. {% block %}
2. {% client %}(wie spät ist es|wie viel uhr ist es){% endclient %}
3. {% response %}{% call time: 0 %}{% endresponse %}
4. {% endblock %}
5.
6. {% block %}
7. {% client %}(stop|halt|stopp|schweigefuchs){% endclient %}
8. {% response %}{% call stop: 0 %}{% endresponse %}
9. {% endblock %}
Ein Dialog wird in einem sogenannten Block organisiert. Man erkennt schnell, dass der erste 
der Wiedergabe der Uhrzeit und der zweite dem Unterbrechen des Assistenten dient. Der 
mit client markierte Text kennzeichnet die Eingabe eines Benutzers in Form einer Regular 
Expression. Der Intent wird also aufgerufen, wenn der Benutzer entweder wie spät ist es oder 
wie viel uhr ist es sagt.
Die Response kann entweder ein Text sein, der sofort zurückgeliefert wird, oder aber auch ein 
Funktionsaufruf, durch den die Funktion selber einen Rückgabewert generiert. In unserem 
Fall nutzen wir Letzteres und rufen im ersten Block die mit time annotierte Methode und 
im zweiten Block die mit stop annotierte Methode auf. Einem Call können wir verschiedene 
Parameter mitgeben. Einer ist, wie eben schon angesprochen, obligatorisch, weswegen wir 
hier jeweils eine 0 für den Parameter dummy übergeben.
Lassen Sie uns nun zurück in die main Punkt Pei gehen und dort am Ende der Methode __init__()
das Dialog-Framework initialisieren.



114 5 Dialoge und Intentionen
Listing 5.12 Initialisierung von ChatbotAI
1. logger.info("Initialisiere Chatbot...")
2. dialog_template_path = './dialogs/dialogs.template'
3. if os.path.isfile(dialog_template_path):
4. self.chat = Chat(dialog_template_path)
5. else:
6. logger.error('Dialogdatei konnte nicht in {} gefunden werden.', 
 dialog_template_path)
7. sys.exit(1)
8. logger.info('Chatbot aus {} initialisiert.', dialog_template_path)
Wir legen zuerst den Pfad des Templates fest, das wir soeben formuliert haben, und prüfen, 
ob die Datei existiert oder nicht. Da wir ja später lediglich über Sprache arbeiten und unsere 
Benutzer nicht sehen können, ob das Log irgendeine Fehlermeldung ausgibt, müssen wir in 
vielen Situationen besonders genau auf Fehler prüfen, diese in manchen Fällen akustisch 
ausgeben und zumindest sauber loggen. Wird das Template nicht gefunden, beenden wir 
den Assistenten, denn wir haben keine Möglichkeit, dessen Funktion auf irgendeine Art und 
Weise wiederherzustellen. sys.exit() beendet die Applikation und weist mit dem Parameter 
1 darauf hin, dass die Anwendung mit einem Fehler-Code geschlossen wurde – 0 hingegen 
kennzeichnet eine fehlerfreie Beendigung.
Damit haben wir schon alles beisammen, um die letzten zwei Zeilen in die While-Schleife unse rer Methode run() zu integrieren. Konkret geht es nur um die Zeilen 4–6 aus Listing 5.13, 
in denen wir self.chat.respond() aufrufen und den verstandenen Satz des Benutzers als 
Parameter übergeben.
Listing 5.13 Generieren einer Antwort über ChatbotAI
1. sentence = recResult['text']
2. logger.debug('Ich habe verstanden "{}"', sentence)
3.
4. output = self.chat.respond(sentence)
5. logger.debug('Output ist "{}", output)
6. self.tts.say(output)
7.
8. self.is_listening = False
9. self.current_speaker = None
Das Resultat output ist eine Zeichenkette, die wir über tts.say() von unserer Text-To-Speech Engine sprechen lassen können. Probieren Sie den Assistenten gerne mal ausführlich aus, 
nachdem Sie chatbot über pip installiert haben. Dabei geht es vor allem darum, ein Gefühl 
dafür zu bekommen, wie VOSK die Befehle interpretiert, die die Benutzer geben. Versuchen 
Sie es zunächst mit diesen Sätzen:
 Wie spät ist es?
 Stopp!
Diese entsprechen nun eins zu eins den Templates aus Listing 5.11 und sollten vom Frame work mühelos erkannt werden. Entspricht der gesprochene Befehl einem der Muster in der 



5.2 Auswahl eines Chatbot-Frameworks 115
dialogs.template, wird die Funktion aufgerufen, die per call referenziert wird. Versuchen Sie 
es nun mit Uhrzeit!, werden Sie keinen Erfolg haben. Natürlich, passt der Befehl doch auf 
keinen regulären Ausdruck im Template.
Die ersten zwei Intents zu programmieren, ging also recht leicht von der Hand. Jedoch haben 
wir ein paar wesentliche Punkte noch nicht bedacht: Was passiert, wenn ein Befehl nicht 
korrekt interpretiert werden kann? Im Moment bekommen wir in diesem Fall noch eine eng lischsprachige Fehlermeldung von chat.respond() zurückgeliefert. Weiterhin organisieren 
wir noch unsere beiden Intents in einem Template. Wie aber sieht es aus, wenn Benutzer 
mehrere Intents nachträglich hinzufügen wollen? Damit beschäftigen wir uns gleich, lassen 
Sie uns zunächst aber mal schauen, wie wir unser zweites Framework auf Basis maschinellen 
Lernens in die Anwendung integrieren.
5.2.2 Intents auf Basis maschinellen Lernens
In diesem Abschnitt werden Sie lernen, die Bibliothek Snips-NLU einzusetzen, um auf Be fehle eines Nutzers zu reagieren, die auf andere Art und Weise formuliert wurden, als wir 
während der Implementierung vorgesehen hatten. Diese Fähigkeit bekommen wir allerdings 
nicht geschenkt, denn wir müssen, wie schon in den Machine-Learning-Beispielen, zuvor 
Trainingsdaten erstellen – das wiederum allerdings nicht in übermäßig großem Umfang, 
wie Sie gleich sehen werden.
Sie finden das aktuelle Beispiel im Ordner 08_dialogs_and_intents_ml. Richten Sie sich einen 
Projektordner und ein Conda Environment ein und kopieren Sie den Code des gesamten letzten 
Kapitels in diesen neuen Ordner.
HINWEIS: Snips-NLU baut auf der Sprache RUST auf und erwartet, dass diese 
auf dem ausführenden System installiert ist. Sie finden die Installationsdatei hier: 
Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichwww.rust-lang.org
Dann löschen Sie zuerst den Ordner dialogs, in dem wir unser Template für ChatbotAI ab gelegt haben; dieses verwenden wir in diesem Beispiel nicht. Erstellen Sie stattdessen einen 
Ordner mit dem Namen dialog-datasets und darin die Datei stop_dataset.yaml. Die Endung 
deutet darauf hin, dass wir einen alten Bekannten, nämlich das YAML-Format wiedersehen 
und damit arbeiten werden. Befüllen Sie nun die Datei mit dem Inhalt von Listing 5.14. Darin 
finden Sie die Definition eines Intents (Snips-NLU verwendet intern dieselbe Bezeichnung 
wie wir) mit dem Namen stop. Nun übergeben wir in utterances einige Beispielausdrücke, 
die diesen Intent charakterisieren.
Listing 5.14 Definition des Stop-Intent
1. type: intent
2. name: stop
3. utterances:
4. - stop
5. - halt



116 5 Dialoge und Intentionen
6. - sei still
7. - sei ruhig
8. - abbrechen
9. - schweigefuchs
Sie sehen, dass wir kaum mehr Tipparbeit zu leisten haben als bei dem Framework, das auf 
Regular Expressions basiert. Nun gut, hier haben wir bisher weder einen Funktionsaufruf 
noch einen Parameter definiert.
Schauen wir uns gleich noch ein Beispiel an. Dabei wird es zugunsten der Vergleichbarkeit 
um die Zeitansage gehen, wie auch schon in Abschnitt 5.2.1. Legen Sie ebenfalls im Ordner 
dialog-datasets die Datei time_dataset.yaml an und befüllen Sie diese gemäß Listing 5.15.
Listing 5.15 Definition der Entität country und des Time-Intents
1. type: entity
2. name: country
3. automatically_extensible: true
4. use_synonyms: false
5. matching_strictness: 0.8
6. values:
7. - deutschland
8. - england
9.
10. type: intent
11. name: getTime
12. utterances:
13. - Wie spät ist es in [place:country](deutschland)
14. - Wie viel Uhr ist es in [place:country](deutschland)
15. - Uhrzeit in [place:country](deutschland)
16. - Wie spät ist es
17. - Wie viel Uhr ist es
18. - Uhrzeit
Hier lernen Sie einen zweiten Typ kennen, Entity. Eine Entität kann ein beliebiges Objekt 
sein, in diesem Fall definieren wir ein Land. Eine Entität kann mehrere Eigenschaften ha ben. Die erste, automatically_extensible, erlaubt dem Parser, eigene Werte für diese Entität 
anzunehmen. Falls die Eigenschaft auf False steht, werden nur die Werte erlaubt, die wir in 
der Liste values übergeben. Es folgt der Parameter use_synonyms, der es erlaubt, Synonyme 
für die diese Entität zu ermitteln und auch diese als zulässiges Element zu vermerken. In 
Zeile 5 von Listing 5.15 wird zuletzt noch matching_strictness gesetzt. Es gibt vor, wie streng 
das Framework beim Erkennen der Entität sein soll. Nun folgt eine Liste von Werten, die die 
Entität exemplarisch oder ausschließlich annehmen kann.
TIPP: Snips-NLU verfügt über eine recht große Liste von vordefinierten Entitäten, 
darunter etwa Geldbeträge, Zahlen, Datums- und Zeitangaben, Städte, Länder, Re gionen etc. Eine Übersicht finden Sie in der entsprechenden Dokumentation unter 
Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichsnips-nlu.readthedocs.io/en/latest/builtin_entities.html.



5.2 Auswahl eines Chatbot-Frameworks 117
Es folgt die Definition des Intents getTime ab Zeile 10. In der Liste utterances sehen Sie nun, 
dass wir an bestimmten Stellen unsere Entität vom Typ country referenzieren. Der in eckigen 
Klammern stehende Ausdruck place:country bedeutet, dass eine Variable mit Namen place
und vom Typ country an dieser Stelle zu finden sein soll. Für eine solche Variable (einen 
Slot im Snips-NLU-Jargon) wird auch immer ein Beispielwert angegeben, hier deutschland.
HINWEIS: Wie findet man eigentlich Synonyme für ein bestimmtes Wort? Das ist 
tatsächlich nicht schwer umzusetzen, wie Listing 5.16 zeigt. Wir setzen dabei auf 
Wortfelder (Synsets), die ähnliche Wörter gruppieren und so Sammlungen von 
Synonymen bilden.
Listing 5.16 Synonyme von good über Wordnet Synsets
from nltk.corpus import wordnet
for syn in wordnet.synsets("good"):
 for name in syn.lemma_names():
 print(name)
Das n, v, a oder r hinter dem Synonym deutet auf die Wortart hin, ist es also ein 
Nomen, ein Verb, ein Adjektiv oder ein Adverb. Die Nummer am Ende der Ausgabe 
indiziert verschiedene Bedeutungsgruppen, sodass Synonyme mit einer gleichen 
Bedeutung eine gleiche Nummer haben. Wordnet ist eine fantastische Bibliothek, 
die neben Synonymen auch Erklärungen zu einzelnen Wörtern vorhält oder Ähn lichkeiten zwischen Wörtern bewerten kann. Schauen Sie sich mal das offizielle 
Howto an Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichwww.nltk.org/howto/wordnet.html.
Die letzten drei Beispielsätze in utterances enthalten keine Ortsangabe. Das Framework wird 
den Parameter in diesem Fall nicht befüllen.
Fahren wir nun fort, indem wir Snips-NLU initialisieren. Löschen Sie den Initialisierungs part von ChatbotAI aus der Methode __init__() und ersetzen Sie diesen durch Listing 5.17.
Listing 5.17 Initialisierung von Snips-NLU
1. logger.info("Initialisiere Chatbot...")
2. self.nlu_engine = None
3. dataset = Dataset.from_yaml_files("de", ['./dialog-datasets/stop_dataset.yaml',
4. './dialog-datasets/time_dataset.yaml'])
5. nlu_engine = SnipsNLUEngine(config=CONFIG_DE)
6. self.nlu_engine = nlu_engine.fit(dataset)
7.
8. if not self.nlu_engine:
9. logger.error("Konnte Dialog Engine nicht laden.")
10. sys.exit(1)
11. else:
12. logger.debug("Dialog Metadaten: {}.", self.nlu_engine.dataset_metadata)



118 5 Dialoge und Intentionen
Darin Initialisieren wir SnipsNLUEngine auf Basis der deutschen Sprache (zu sehen an CON FIG_DE in Zeile 5). Es folgt der Aufruf der Methode fit(), die das Fine Tuning auf Basis 
unserer beiden YAML-Dateien, die wir zuvor erstellt haben, durchführt. Die Zeilen 8 bis 12 
bestehen lediglich aus Error-Handling und Logging.
Damit Klassen und Module bekannt sind, fügen Sie noch die Imports aus Listing 5.18 hinzu. 
Die Bibliothek pytz wird uns gleich helfen, Zeitzonen zu verarbeiten.
Listing 5.18 Imports für die Verwendung von Snips-NLU, Ein- und Ausgabe und Verarbeitung von 
Zeitzonen
import io
import pytz
from snips_nlu import SnipsNLUEngine
from snips_nlu.default_configs import CONFIG_DE
from snips_nlu.dataset import Dataset
Entfernen Sie nun die Funktionen stop und getTime, also alle, die wir mit register_call an notiert haben und ersetzen Sie sie durch die in Listing 5.19. Der Hintergrund ist einmal 
natürlich der Austausch des Dialog-Frameworks, aber auch die Erweiterung der Methode 
getTime um eine Zeitzone. Diese ermitteln wir händisch über ein Dictionary, das erst mal 
nur Einträge für fünf Länder beinhaltet. Natürlich verfügen Amerika und China aufgrund 
ihrer Größe über mehrere Zeitzonen, aber das soll uns hier für einen ersten Entwurf erst 
mal nicht interessieren.
Listing 5.19 Neuimplementierung der Intents getTime und stop
1. def getTime(place):
2. country_timezone_map = {
3. "deutschland": pytz.timezone('Europe/Berlin'),
4. "england": pytz.timezone('Europe/London'),
5. "frankreich": pytz.timezone('Europe/Paris'),
6. "amerika": pytz.timezone('America/New_York'),
7. "china": pytz.timezone('Asia/Shanghai')
8. }
9.
10. now = datetime.now()
11. timezone = country_timezone_map.get(place.lower())
12. if timezone:
13. now = datetime.now(timezone)
14. return "Es ist " + str(now.hour) + " Uhr und " + str(now.minute) +
15. "Minuten in " + place.capitalize() + "."
16. return "Es ist " + str(now.hour) + " Uhr und " + str(now.minute) +
17. "Minuten."
18.
19. def stop():
20. if va.tts.is_busy():
21. va.tts.stop()
22. return "okay ich bin still"
23. return "Ich sage doch gar nichts"



5.2 Auswahl eines Chatbot-Frameworks 119
Sie sehen, dass die Anwendungslogik sich nicht großartig ändert. Die Methode stop() sieht 
sogar so aus wie vorher, bloß dass wir die Annotation entfernt haben.
Letztendlich müssen wir, wie auch bei ChatbotAI, das Parsing in die Methode run() der 
Klasse VoiceAssistant integrieren. Löschen Sie dazu die Zeile output = self.chat.
respond(sentence) und ersetzen Sie sie durch den Inhalt von Listing 5.20.
Listing 5.20 Parsen und Verarbeiten des Sprachbefehls
1. parsing = self.nlu_engine.parse(sentence)
2. output = ""
3.
4. if parsing["intent"]["intentName"] == "getTime":
5. if len(parsing["slots"]) == 0:
6. output = getTime("Germany")
7. elif parsing["slots"][0]["entity"] == "country":
8. output = getTime(parsing["slots"][0]["rawValue"])
9. elif parsing["intent"]["intentName"] == "stop":
10. stop()
11. self.tts.say(output)
Das Parsen des verstandenen Satzes geschieht in Zeile 1 und liefert ein Objekt zurück, 
das als JSON-Objekt adressiert werden kann. Darin befinden sich unter dem Index intent
zum Beispiel der intentName, der den Namen beinhaltet, der in der YAML-Datei festgelegt 
wurde. Auf diesen prüfen wir in Zeile 4 und Zeile 9 und rufen entsprechend die Funktion 
getTime() oder stop() auf. Im ersten Fall lesen wir die sogenannten Slots (Zeile 7) aus, die 
die Parameter beinhalten, die wir in Form einer Entität angelegt haben, also country. Ist ein 
solcher Slot befüllt, holen wir dessen Wert und übergeben ihn der Funktion getTime(). Die 
Funktion stop() benötigt keine Parameter und kann einfach aufgerufen werden, ohne dass 
die Slots Beachtung finden.
Bevor wir unsere nächste Version testen können, müssen wir noch snips-nlu und pytz über 
pip installieren. Dazu kommt noch eine Besonderheit: Snips-NLU benötigt ein deutsches 
Sprachpaket, das allerdings als root installiert werden muss. Auf Windows klicken Sie dazu 
einfach auf den Windows-Button unten links, tippen Anaconda Prompt und sobald das Icon 
im Startmenü erscheint, machen Sie einen Rechtsklick darauf und wählen Als Administrator
ausführen. Dann aktivieren Sie wie gewohnt die Umgebung und führen den Befehl python 
-m snips_nlu download de aus. Vorher muss Snips-NLU zwingend installiert sein.
Beim Testen unseres Assistenten werden Sie feststellen, dass das Framework sehr gut darin 
ist, Intents zu erkennen und das, obwohl es im Vergleich zu unserem Intent Classifier aus 
Abschnitt 5.1.1 kaum Trainingsdaten bekommen hat.
Nun haben Sie beide Frameworks kennengelernt und gesehen, wie diese integriert werden 
können. Für mich persönlich ist die Definition von Intents in beiden Fällen ähnlich simpel, 
bloß dass die Formatierung für den Machine-Learning-Ansatz ein wenig sauberer ist und 
dass jedem Start des Assistenten ein Training vorausgeht, das jedoch in wenigen Sekunden 
durchgeführt ist.



120 5 Dialoge und Intentionen
■ 5.3 Modularisierung von Intents
Die ersten zwei Intents haben wir nun relativ starr in die Hauptanwendung integriert. Zeit, 
uns anzuschauen, wie wir unseren Sprachassistenten so erweitern, dass wir verschiedene 
Intents hinzufügen und intern verwalten können. Und das zur Laufzeit, nicht während der 
Entwicklung, denn wir wollen ja unseren Benutzern später die Möglichkeit geben, weitere 
Intents nachträglich zu aktivieren. Das bedeutet praktisch nicht nur, dass wir Skripte dyna misch laden, sondern auch Bibliotheken zur Laufzeit installieren. Weiterhin soll jeder Intent 
die Möglichkeit haben, eine eigene Konfigurationsdatei mitzubringen, um etwa Konstanten 
für Ausgaben in verschiedenen Sprachen zu definieren oder Einstellungen für den indivi duellen Intent vorzuhalten.
Bild 5.8 verdeutlicht noch mal den initialen Aufbau des Inkrements, das wir in diesem Kapitel 
entwickeln wollen. Intents werden zusammen mit Konfigurations- und Trainingsdaten in einer 
zentralen Datenbank abgelegt, aus der sich der Intent Parser die passendste Verarbeitungs logik ausgeben lässt. Diese wird dann aufgerufen. In der Grafik sind diese exemplarisch als 
Wikipedia-, Wetter- und Uhrzeit-Intent abgebildet.
Wikipedia Weer Uhrzeit
Intent-Parser
Sprachassistent
Intent DB
Intent Management
Konfiguraon
Trainingsdaten
Bild 5.8 Intents werden dynamisch in eine Datenbank geladen, um zur Laufzeit mittels Intent Parser
dagegen prüfen zu können, ob ein Befehl mit einem Intent sinnvoll verarbeitet werden kann.
Wir werden nun die Vorarbeit leisten, um im Anschluss das Intent Management selber 
zu implementieren. Dazu gehört, die entsprechende Projektstruktur anzulegen und das 
Zusammenspiel von ChatbotAI und Snips-NLU zu choreographieren. Legen Sie bei Bedarf 
einen Klon des letzten Projekts als 08_intent_management inklusive des dazugehörigen 
Environments an. Legen Sie darin einen neuen Ordner intents an. Wie der Name schon ver muten lässt, werden wir darin alle Intents verwalten und vorhalten. In dem Ordner intents
erstellen Sie die drei Unterordner chatbotai, functions und snips-nlu. Damit hat es folgende 
Bewandtnis:
 chatbotai: Darin halten wir die template-Dateien vor, die wir in Abschnitt 5.2.1 kennen gelernt haben. Kopieren Sie direkt das stop.template aus diesem Abschnitt in den Ordner.



5.3 Modularisierung von Intents 121
 functions: Hier legen wir in einzelnen Ordnern die Python-Dateien ab, die die Logik für die 
jeweiligen Intents enthalten. Dabei unterscheiden wir nicht zwischen den beiden Dialog 
Engines und verwenden nur einen Ordner functions. Zusätzlich ermöglichen wir jedem 
Ordner eines Intents, eine Konfigurationsdatei mitzuliefern, sowie eine requirements.txt, 
die die Abhängigkeiten für die Intents beinhaltet.
 snips-nlu: Hier liegen die yaml-Dateien, die Snips-NLU benötigt und die wir in Abschnitt 5.2.2
besprochen haben. Kopieren Sie gerne schon einmal stop_dataset.yaml und time_dataset.
yaml aus dem vorherigen Beispiel in diesen Ordner.
Nun können Sie zunächst den Ordner dialog-datasets aus dem Hauptordner des Projekts 
löschen, wir benötigen diesen nicht mehr. Kümmern wir uns im Anschluss um das Zusam menspiel von ChatbotAI und Snips-NLU. Legen Sie dazu die Datei wildcard.template im Ordner 
intents/chatbotai an. Deren Inhalt ist in Listing 5.21 zu sehen.
Listing 5.21 Der Wildcard-Intent leitet alle unverstandenen Intents von ChatbotAI zu Snips-NLU weiter
{% block %}
 {% client %}.*{% endclient %}
 {% response %}{% call default_snips_nlu_handler: %0 %}{% endresponse %}
{% endblock %}
Was geschieht hier? Wenn Sie sich bereits mit Regular Expressions beschäftigt haben, sehen 
Sie schnell, dass das Muster in dem Tag client-endclient auf jede beliebige Zeichenkette zutrifft. 
Es ist also egal, was der Benutzer sagt, dieser Intent passt immer. In der Response wird nun 
die Funktion default_snips_nlu_handler() aufgerufen. Die Besonderheit ist dabei, dass als 
Parameter der gesamte Text der Eingabe übergeben wird, gekennzeichnet durch die Gruppe 0 
(%0), die stehts den gesamten Ausdruck beinhaltet. Wird also ein Befehl von ChatbotAI nicht 
erkannt, wird der gesamte Befehl an die Funktion default_snips_nlu_handler() weiter geleitet, die wir in naher Zukunft ausformulieren.
Schauen wir nun einmal in den Ordner functions und legen dort den Ordner gettime an. Er stellen Sie darin zusätzlich eine requirements.txt, in die Sie lediglich den Begriff pytz eintragen. 
Damit hinterlegen wir für später, dass für diesen Intent die Abhängigkeit für die Verarbeitung 
von Zeitzonen installiert wird. Speichern und schließen Sie die requirements.txt im Anschluss.
Kommen wir nun zur Konfiguration von gettime, für die eine Datei mit Namen config_gettime.
yml erstellt wird, die Sie in Listing 5.22 sehen.
Listing 5.22 Konfiguration für den Intent für die Zeitansage
1. intent:
2. gettime:
3. de:
4. time_in_place: ["Es ist {} Uhr {} in {}.",
5. "Die Uhrzeit ist {} Uhr {} in {}.",
6. "Es ist {} Uhr und {} Minuten in {}."]
7. time_here: ["Es ist {} Uhr {}.", "Es ist {} Uhr und {} Minuten."]
8. place_not_found: ["Die Region {} kenne ich leider nicht",
9. "Das Land oder den Ort {} kenne ich nicht.", "{}, nie gehört!"]



122 5 Dialoge und Intentionen
10. en:
11. time_in_place: ["It is {} {} in {}."]
12. time_here: ["It is {} {}.", "It is {} o'clock and {} minutes."]
13. place_not_found: ["I don't know the place {}.",
14. "I don't know a place called {}."]
15. timezones:
16. Europe/Berlin: ["deutschland", "germany"]
17. Europe/London: ["england", "großbrittanien", "great britain"]
18. Europe/Paris: ["frankreich", "france"]
29. America/New_York: ["amerika", "america"]
20. Asia/Shanghai: ["china"]
In gewohnter Formatierung legen wir nun ein Objekt an. Ich habe aus Gewohnheit als erstes 
Element den Bezeichner intent gewählt, damit immer klar ist, um was für eine Art Datei es 
sich handelt. Dem folgt der Intent-Name, hier gettime, als Unterelement. Dieses beherbergt 
wiederum jeweils zwei Konfigurationen für Zeichenketten in den Sprachen Deutsch und 
Englisch sowie eine mit Key-Value-Paaren für das Mapping der Zeitzonen. Wir werden in dem 
Buch lediglich deutschsprachig arbeiten, dennoch möchte ich Ihnen zeigen, wie Sie alles für 
die Verwendung einer oder mehrerer weiterer Sprachen vorbereiten können.
Zu sehen ist, dass, statt wie in den ersten Beispielen dieses Kapitels, Zeichenketten nun 
Teil der Konfiguration sind. So enthält time_in_place etwa drei frei wählbare Sätze, um die 
Uhrzeit in einer bestimmten Region mitzuteilen. Die Platzhalter, die durch die geschweiften 
Klammern zu erkennen sind, werden wir später per String-Formatierung mit den richtigen 
Werten befüllen. Wir halten weiterhin noch eine Liste von Sätzen für die Zeitangabe ohne 
eine Ortsangabe in time_here vor sowie eine Liste von Sätzen in place_not_found, die kund tun, dass ein bestimmter Ort nicht bekannt ist.
Sie sehen, dass die Bezeichner innerhalb der YAML-Datei identisch sind. Später müssen wir 
lediglich die Sprache austauschen (en ↔ de) und können dann einfach die Eigenschaften über 
dieselben Schlüssel referenzieren.
Die fünf exemplarischen Zeitzonen beinhalten jeweils eine Liste von Orten, in denen diese 
gelten. Natürlich ist diese Liste nicht vollständig, sie zeigt aber das Prozedere der Ermittlung. 
Hier können wir getrost auf eine Gruppierung à la de und en verzichten, denn wir schreiben 
einfach die mehrsprachigen Begriffe, die die Zeitzonen identifizieren, in die jeweilige Liste. 
So wird deutschland als auch germany die Zeitzone Europe/Berlin zugewiesen. Wie das genau 
funktioniert, sehen wir nun in der dazugehörigen Logik in Listing 5.23. Diese sollte im Ordner 
functions/gettime in der Datei intent_gettime.py niedergeschrieben werden.
Wir beginnen damit, dass wir einige Module importieren, die der Ermittlung der korrekten 
Zeit in Zeitzone XY (datetime und pytz), dem sauberen Konstruieren von Pfaden (os), der Se lektion von Zufallswerten aus einer Liste (random) und der Arbeit mit YAML-Dateien (yaml) 
dienen. Weiterhin sehen Sie noch den Import von global_variables, den wir uns gleich noch 
im Detail anschauen werden.
Es folgt die Implementierung von gettime() auf Basis der YAML-Konfiguration für Snips NLU aus Listing 5.15. Den Parameter place versehen wir direkt mit einem Default-Wert, 
falls dieser nicht vom Benutzer übergeben oder vom NLU-Framework detektiert wurde. Der 
Wert default weist, wie wir gleich sehen werden, darauf hin, dass die Uhrzeit am jeweiligen 
Standort des Benutzers ausgegeben werden soll.



5.3 Modularisierung von Intents 123
In Zeile 10 beginnen wir damit, dass wir die Konfiguration aus config_gettime.yml, so wie 
wir sie in Listing 5.22 erstellt haben, einlesen. Durch die Methode os.path.join(), die 
alle einzelnen Ordner und Dateien aneinanderhängt, gehen wir sicher, dass wir auf jedem 
Betriebssystem valide Pfade verwenden, egal ob mit Backslashes (\) oder Schrägstrichen 
(/) gearbeitet wird. Nach dem Öffnen der Konfigurationsdatei in Zeile 13 wird diese über 
yaml.load() gelesen und sie steht uns von nun an in der Variablen cfg bereit. Im Falle eines 
Fehlers beim Lesen (siehe Zeile 16) geben wir eine Meldung ins Log aus und retournieren 
einen leeren String, sodass der Sprachassistent nichts sagt.
Ein wesentlicher Bestandteil der meisten Intents ist die Ermittlung der Sprache, die in der 
globalen Konfiguration eingetragen ist. Es handelt sich also hierbei nicht um die Datei, die 
wir gerade eingelesen haben und die nur für den Time-Intent da ist, sondern um die Konfi guration, die für die gesamte Anwendung gilt. Offensichtlich ist diese über global_variables.
voice_assistant.cfg zugreifbar (Zeile 21) – wie genau, sehen wir gleich. Wichtig ist, dass die 
Sprache in der Konstanten LANGUAGE gespeichert wird.
Weiterhin lesen wir aus den diversen möglichen Sätzen, die wir in der Liste place_not_found
festgelegt haben, einen zufälligen Satz aus. Dabei hilft uns die Methode random.choice(), 
die einen zufälligen Wert aus einer Liste aussucht und zurückliefert, der in diesem Fall in 
PLACE_UNKNOWN abgelegt wird (Zeile 24).
In Zeile 28 hingegen ist zu sehen, wie ein Platzhalter, den wir unter anderem in Listing 5.22
als {} gesehen haben, durch eine Zeichenkette ersetzt wird. Ganz konkret wird in PLACE_
UNKNOWN der erste Platzhalter durch die Methode format() durch place ersetzt. format()
akzeptiert eine variable Anzahl von Parametern und ersetzt alle Platzhalter von links nach 
rechts durch die jeweils nächste übergebene Zeichenkette. Zum besseren Verständnis sehen 
Sie das Vorgehen in Bild 5.9 illustriert. Die Herausforderung liegt darin, wie Sie beim Erstellen 
eigener zu sprechenden Sätze feststellen werden, dass Sie die Reihenfolge immer einhalten 
müssen. Würde der Satz in der Zeichenkette vorstellung so lauten Ich bin {}, komme aus {} 
und mein Name ist {}., würde format() in der Grafik die Platzhalter in der falschen Reihen folge ersetzen, sodass sich Ich bin Jonas, komme aus 36 und mein Name ist Landau. ergeben 
würde, was offensichtlich nicht sehr viel Sinn ergibt.
vorstellung = „Mein Name ist {}, ich bin {} und ich komme aus {}.“
vorstellung = vorstellung.format(„Jonas“, „36“, „Landau“)
Bild 5.9 Die Methode format() ersetzt nach und nach alle Platzhalter durch die ihr übergebenen 
Parameter.
Ab Zeile 31 machen wir uns an die Verarbeitung des übergebenen Ortes und den Versuch, die sem eine Zeitzone zuzuordnen. Dabei lesen wir aus einer Liste von Key-Value-Paaren alle Paare 
über die Methode items() aus und schreiben diese in das Dictionary country_timezone_map.



124 5 Dialoge und Intentionen
Listing 5.23 Die Funktion gettime für den gleichnamigen Intent liefert die Uhrzeit in einer bestimmten 
Zeitzone zurück
1. from datetime import datetime
2. import pytz
3. import global_variables
4. import os
5. import random
6. import yaml
7.
8. def gettime(place="default"):
9.
10. config_path =
11. os.path.join('intents','functions','gettime','config_gettime.yml')
12. cfg = None
13. with open(config_path, "r", encoding='utf8') as ymlfile:
14. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
15.
16. if not cfg:
17. logger.error("Konnte Konfigurationsdatei für gettime nicht lesen.")
18. return ""
19.
20. # Holen der Sprache aus der globalen Konfigurationsdatei
21. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
22.
23. # Der Ort ist nicht bekannt
24. PLACE_UNKNOWN =
25. random.choice(cfg['intent']['gettime'][LANGUAGE]['place_not_found'])
26.
27. # Wir fügen den unbekannten Ort in die Antwort ein
28. PLACE_UNKNOWN = PLACE_UNKNOWN.format(place)
29.
30. # Lesen aller Orte aus der Konfigurationsdatei
31. country_timezone_map = {}
32. for key, value in cfg['intent']['gettime']['timezones'].items():
33. country_timezone_map[key] = value
34.
35. # Versuche den angefragten Ort einer Zeitzone zuzuordnen
36. timezone = None
37. now = datetime.now()
38. for c in country_timezone_map:
39. if place.strip().lower() in country_timezone_map[c]:
40. timezone = pytz.timezone(c)
41. break
42.
43. # Wurde eine Zeitzone gefunden?
44. if timezone:
45. now = datetime.now(timezone)
46. TIME_AT_PLACE =
47. random.choice(cfg['intent']['gettime'][LANGUAGE]['time_in_place'])
48. TIME_AT_PLACE = TIME_AT_PLACE.format(str(now.hour), str(now.minute),
49. place.capitalize())
50. return TIME_AT_PLACE
51. else:
52. # Falls nicht, prüfe, ob nach der lokalen Uhrzeit gefragt wurde



5.3 Modularisierung von Intents 125
53. if place == "default":
54. TIME_HERE =
55. random.choice(cfg['intent']['gettime'][LANGUAGE]['time_here'])
56. TIME_HERE = TIME_HERE.format(str(now.hour), str(now.minute))
57. return TIME_HERE
58.
59. # Wurde ein Ort angefragt, der nicht gefunden wurde, antworte entsprechend
60. return PLACE_UNKNOWN
Es folgt der Versuch der Zuordnung der Zeitzone ab Zeile 38, in der wir alle Einträge aus 
unserem frisch erstellten Dictionary durchgehen und schauen, ob der vom Benutzer über gebene Ort in der Liste der Werte von einer der Einträge ist. Falls ja, weisen wir timezone
diese Zeitzone zu, falls nicht, bleibt timezone None. Ob diese Variable nun belegt ist, wird 
in Zeile 44 geprüft und im positiven Fall eine Zeichenkette aus der Liste time_in_place aus 
unserer Konfiguration vorbereitet. Dafür holen wir uns die aktuelle Zeit in der jeweiligen 
Zeitzone in Zeile 46 über datetime.now() und ersetzen die Platzhalter für Stunden und Mi nuten in Zeile 48. Sehen Sie, dass wir Stunden und Minuten in die Funktion str() einbetten? 
format() akzeptiert hier nur Zeichenketten, keine Integer-Werte, wir müssen now.hour und 
now.minute also zuvor in Strings konvertieren. Zuletzt wird TIME_AT_PLACE fertig formatiert 
zurückgegeben und die Funktion verlassen.
Wurde keine Zeitzone aus dem Befehl ermittelt oder ist diese unbekannt, springt die Funktion 
in den Else-Zweig in Zeile 51 und befüllt eine zufällige Zeichenkette aus der Liste time_here
aus der Intent-Konfiguration und gibt diese als formatierten String TIME_HERE zurück.
In Zeile 60 fangen wir zu guter Letzt den Fall ab, dass die Uhrzeit für einen Ort angefragt 
wurde, dem jedoch keine Zeitzone zugeordnet werden kann.
Verwalten von anwendungsweiten Variablen
Manchmal ist es notwendig, verschiedene Variablen aus mehreren Klassen lesen und schrei ben zu können. Arbeitet man objektorientiert, initialisiert man sicher eine Klasse, deren 
Instanz man an andere Klasseninstanzen weiterreicht und damit auf deren Eigenschaften 
zugreifen kann. Das tun wir allerdings nicht so konsequent, wie es vielleicht in Java oder C# 
üblich wäre, denn in Python kann man getrost die komplexesten Anwendungen schreiben, 
ohne überhaupt ein einziges Objekt selbst zu definieren.
TIPP: Die Betonung liegt auf selbst, denn tatsächlich ist in Python alles als Objekt 
definiert und kann auch so behandelt werden. Das heißt, sowohl bei Variablen, 
Klassen als auch bei Funktionen können Sie mit der Methode type() abfragen, 
um was für eine Art Objekt es sich handelt. Wir können Objekte weiterhin als 
Funktionsparameter übergeben oder deren Eigenschaften oder Methoden aus lesen. Häufig wird genau unter diesen Gesichtspunkten von Python als einer ob jektorientierten Sprache gesprochen. Jedoch forciert weder der Python-Styleguide 
PEP noch die Syntax selbst, dass Sie irgendwelche Klasse verwenden.
Wir werden uns also keine Klasse zunutze machen, um die Eigenschaften vorzuhalten – viel mehr werden wir Module als Abstraktionsschicht nutzen, um dort Variablen abzulegen, die wir 



126 5 Dialoge und Intentionen
dann über den Import des Moduls im Zugriff haben. Wir legen nun die Datei global_variables.py
in unserem Hauptordner an, befüllen diese mit der einfachen Zeile voice_assistant = None
und speichern sie ab. Wie Sie wahrscheinlich schon richtig vermuten, werden wir dieser 
Variablen nun die Instanz unserer Klasse Voice Assistant zuweisen. Das tun wir, indem wir in 
der main Punkt Pei unseres Projekts deren Initialisierung, wie in Listing 5.24 zu sehen, abändern. 
Löschen Sie noch die Funktionen stop() und getTime() aus der main Punkt Pei. Wir benötigen 
diese nicht mehr, da wir sie ja jetzt in unseren eigenen Intent-Ordnern abgelegt haben.
Listing 5.24 Setzen der globalen Variablen voice_assistant
if __name__ == '__main__':
 import global_variables
 multiprocessing.set_start_method('spawn')
 global_variables.voice_assistant = VoiceAssistant()
 global_variables.voice_assistant.run()
Den Import von global_variables habe ich in den Mechanismus gelegt, der beim Anwendungs aufruf über die main Punkt Pei greift. Damit stellen wir für uns während der Entwicklung sicher, 
dass global_variables.voice_assistant auf jeden Fall initialisiert ist, wenn irgendwo darauf 
zugegriffen wird.
■ 5.4 Das Intent Management
In diesem Abschnitt feiern wir Bergfest, haben wir doch am Ende ein solides Grundgerüst, 
um unseren Assistenten nach und nach mit Funktionen zu versehen. Zwar werden wir das 
Framework noch stückweise erweitern, doch die eigentliche Verwaltung von Funktionen 
sowie die Erweiterbarkeit ist nach diesem Abschnitt gewährleistet. Bis dahin liegt aber noch 
ein Stück Arbeit vor uns.
Alles beginnt wie so oft mit einer neuen Datei. Wir nennen diese intentmgmt.py und legen 
sie in den Hauptordner unseres Projekts 09_intent_management. Darin implementieren 
wir nun nach und nach die Klasse IntentMgmt und beginnen mit einer Methode, die den 
Pfad einer Datei mit dem bekannten Aufbau einer requirements.txt entgegennimmt und die 
Bibliotheken darin Zeile für Zeile installiert. Allerdings nicht vor dem Start der Anwen dung, sondern zu deren Laufzeit. Listing 5.25 zeigt den Klassenrumpf sowie die Methoden 
install_requirements() und get_count(). Letztere gibt einfach nur zurück, wie viele 
Intents im Intent Management registriert sind. install_requirements() dagegen ist um 
einiges spannender. Darin öffnen wir zunächst die requirements.txt in Zeile 23 und gehen 
diese Zeile für Zeile durch. Dann rufen wir pip.main() mit den Parametern install und der 
aktuellen Zeile der requirements.txt auf, was einem Aufruf von pip auf Kommandozeilenebene 
gleicht. Zurück bekommen wir eine 1 oder eine 0, wobei die 1 für einen Fehlerfall steht und 
darauf hindeutet, dass eine Abhängigkeit nicht erfolgreich installiert werden konnte. Das 
Ergebnis jeder einzelnen Installation speichern wir in pipcode. In Zeile 26 wird die mit 0 



5.4 Das Intent Management 127
initialisierte Variable retcode mit pipcode verodert. Entschuldigen Sie den Begriff, aber er fällt 
unter Entwicklern häufiger, als man denkt. Für alle die, die keinen informationstechnischen 
Hintergrund haben: Das logische Oder (zu erkennen am or in Zeile 26) ist immer dann 1, 
wenn einer der Werte 1 ist, also 1 oder 1 = 1, 1 oder 0 = 1, aber 0 oder 0 = 0. Wenn also die 
Installation eines einzelnen Pakets fehlschlägt, wird für den gesamten Installationsprozess 
1 zurückgeliefert. Mit dieser Methode können wir also sehr bequem alle Abhängigkeiten 
installieren, die ein Intent mit sich bringt.
Listing 5.25 Programmatische Installation von Abhängigkeiten
1. from loguru import logger
2. import pip
3. import importlib
4. import importlib.util
5. import glob
6. import os
7. import sys
8. from pathlib import Path
9. from snips_nlu import SnipsNLUEngine
10. from snips_nlu.default_configs import CONFIG_DE
11. from snips_nlu.dataset import Dataset
12. from chatbot import Chat, register_call
13. import json
14. import global_variables
15. import random
16.
17. class IntentMgmt:
18.
19. intent_count = 0
20.
21. def install_requirements(self, filename):
22. retcode = 0
23. with open(filename, 'r') as f:
24. for line in f:
25. pipcode = pip.main(['install', line.strip()])
26. retcode = retcode or pipcode
27. return retcode
28.
29. def get_count(self):
30. return self.intent_count
Es folgt die Initialisierung der Klasse, zu sehen in Listing 5.26. Da die Methode etwas länger 
ist, habe ich sie in drei Teile aufgeteilt, die jeweils in den Kommentaren in den Zeilen 3, 
34 und 49 markiert sind. Wir werden uns nun ganz in Ruhe sequenziell durch die Logik 
arbeiten. Im ersten Abschnitt wollen wir alle Funktionen laden, die wir im Ordner intents/
functions in einzelnen Ordnern abgelegt haben. Das Modul glob hilft uns dabei, diese Ordner 
zu finden. In Zeile 6 sehen Sie, wie ein Suchmuster definiert wird, das alle Unterordner in 
intents/functions auflistet. Diese Unterordner speichern wir in der Liste functions_folders und 
traversieren deren Elemente sogleich in Zeile 10. Wenn in einem der Ordner einer Funktion 
eine requirements.txt abgelegt ist, die Prüfung erfolgt in Zeile 13, dann rufen wir unsere 
Methode install_requirements() auf, die wir in Listing 5.25 erstellt haben.



128 5 Dialoge und Intentionen
Da wir die Abhängigkeiten für den jeweiligen Intent nun schon installiert haben, suchen 
wir als Nächstes die entsprechende Funktion. Wieder verwenden wir glob (Zeile 19), um in 
dem Funktionsordner nach Python-Dateien zu suchen, die mit intent_ beginnen und auf .py
enden. Wie schon eben machen wir dabei Gebrauch von der Wildcard-Funktion, die durch 
einen Stern (*) symbolisiert wird und aussagt, dass an dieser Position im Such-String be liebige Zeichen vorkommen dürfen. Die hierbei gefundenen Dateien legen wir in der Liste 
intent_files ab, die wir wiederum direkt durchgehen, mit dem Zweck, diese einzeln in unsere 
Anwendung zu importieren. Dazu müssen diese aber eine ganz bestimmte Struktur haben, 
denn bei einem Import-Statement werden in Python Ordner per Punkt separiert, nicht durch 
Schrägstriche. Außerdem hat ein Modulname keine py-Endung. Diese Änderungen nehmen 
wir in den Zeilen 23 bis 26 vor. So wird beispielsweise aus intents/functions/gettime/
intent_gettime.py der String intents.functions.gettime.intent_gettime.
Listing 5.26 Initialisierung der Klasse IntentMgmt
1. def __init__(self):
2.
3. # 1. Registriere Funktionen, die von snips-nlu und chatbotai aufgerufen
4. # werden
5. self.functions_folders = [os.path.abspath(name) for name in
6. glob.glob("./intents/functions/*/")]
7. self.dynamic_intents = []
8.
9. self.intent_count = 0
10. for ff in self.functions_folders:
11. logger.debug("Suche nach Funktionen in {}...", ff)
12. req_file = os.path.join(ff, 'requirements.txt')
13. if os.path.exists(req_file):
14. install_result = self.install_requirements(req_file)
15. if install_result == 0:
16. logger.debug("Abhängigkeiten für {} installiert oder vorhanden.", ff)
17.
18. # Finde Python-Dateien, die mit Intent beginnen
19. intent_files = glob.glob(os.path.join(ff, 'intent_*.py'))
20. for infi in intent_files:
21. logger.debug("Lade Intent-Datei {}...", infi)
22.
23. name = infi.strip('.py')
24. name = "intents.functions." + Path(ff).name + ".intent_" +
25. Path(ff).name
26. name = name.replace(os.path.sep, ".")
27.
28. logger.debug("Importiere modul {}...", name)
29. globals()[Path(ff).name] = importlib.import_module(name)
30. logger.debug("Modul {} geladen.", str(Path(ff).name))
31. self.dynamic_intents.append(str(Path(ff).name))
32. self.intent_count +=1
33.
34. # 2. Finde alle Dialoge, die über snips nlu abgehandelt werden
35. logger.info("Initialisiere snips nlu...")
36. snips_files = glob.glob(os.path.join("./intents/snips-nlu", '*.yaml'))
37. self.snips_nlu_engine = SnipsNLUEngine(Config=CONFIG_DE)
38. dataset = Dataset.from_yaml_files("de", snips_files)



5.4 Das Intent Management 129
39. nlu_engine = SnipsNLUEngine(config=CONFIG_DE)
40. self.nlu_engine = nlu_engine.fit(dataset)
41. logger.info("{} Snips NLU files gefunden.", len(snips_files))
42. if not self.nlu_engine:
43. logger.error("Konnte Dialog Engine nicht laden.")
44. else:
45. logger.debug("Dialog Metadaten: {}.", self.nlu_engine.dataset_metadata)
46.
47. logger.debug("Snips NLU Training abgeschlossen")
48.
49. # 3. Finde alle Dialoge, die über ChatbotAI abgehandelt werden
50. logger.info("Initialisiere ChatbotAI...")
51.
52. chatbotai_files = glob.glob(os.path.join("./intents/chatbotai",
53. '*.template'))
54. WILDCARD_FILE = './intents/chatbotai/wildcard.template'
55. MERGED_FILE = './intents/chatbotai/_merger.template'
56.
57. # Füge alle Dateien zusammen
58. with open(MERGED_FILE, 'w') as outfile:
59. for caf in chatbotai_files:
60. # Das Wildcard-Template darf erst am Ende geladen werden
61. # Das Merger-Template darf gar nicht geladen werden
62. # (das ist eine Zusammenführung aller einzelnen Template-Dateien)
63. if (not Path(caf).name == Path(WILDCARD_FILE).name) and (not
64. Path(caf).name == Path(MERGED_FILE).name):
65. logger.debug("Verarbeite chatbotai Template {}...", Path(caf).name)
66. with open(caf) as infile:
67. outfile.write(infile.read())
68.
69. # Hänge den Wildcard Intent ans Ende
70. if os.path.exists(WILDCARD_FILE):
71. logger.debug("Prozessiere letzendlich Chatbotai Wildcard Template...")
72. with open(WILDCARD_FILE) as infile:
73. outfile.write(infile.read())
74. else:
75. logger.warning("Wildcard-Datei {} konnte nicht gefunden werden." +
76. " Snips NLU ist damit nicht nutzbar.", WILDCARD_FILE)
77.
78. if os.path.isfile(MERGED_FILE):
79. # Wir müssen hier kein Default-Template setzen, da dieses durch den
80. # Wildcard Intent nie aufgerufen wird.
81. self.chat = Chat(MERGED_FILE)
82. else:
83. logger.error('Dialogdatei konnte nicht in {} gefunden werden.',
84. MERGED_FILE)
85. logger.info('Chatbot aus {} initialisiert.', MERGED_FILE)
intent_gettime könnten wir nun wunderbar per Import in unserem Modul referenzieren. 
Das Problem ist nur, dass wir den Import ja bisher während der Entwicklung statisch durch geführt haben. Dem nehmen wir uns in Zeile 29 an. Dort rufen wir die Funktion globals()
auf, die ein Dictionary zurückgibt, das alle Variablen enthält, die im globalen Kontext des 
Moduls definiert sind, in dem die Funktion zu finden ist.



130 5 Dialoge und Intentionen
Ein kleines Beispiel: In diesen zwei Zeilen wird die Bibliothek pprint (Pretty Print) importiert, 
die uns hier als Beispiel dienen soll und gleichzeitig dafür sorgt, dass wir das Dictionary etwas 
schöner formatiert bekommen. Darauf nutzen wir die Funktion pprint(), um globals()
auszugeben.
import pprint
pprint.pprint(globals())
Die Ausgabe sehen Sie in Bild 5.10 unten. Neben einigen wertvollen Informationen wie 
__file__ und __name__ ist dort auch ein Schlüssel pprint zu finden, der auf das Modul pprint
und den dazugehörigen Pfad verweist.
Bild 5.10 Ausgabe von globals() in einem minimalen Testprojekt
Zurück zu Zeile 29 von Listing 5.26. Dort reichern wir nun das Dictionary unter globals()
um einen weiteren Eintrag an. Der Schlüssel ist der Name des Intent-Ordners aus dem 
Ordner function, also zum Beispiel gettime und der Wert ist das Modul, das wir über importlib.
import_module(name) importieren. Damit haben wir zur Laufzeit ein neues Modul geladen 
und dafür gesorgt, dass die Funktionen darin nun aufgerufen werden können.
Im zweiten Abschnitt laden wir alle Konfigurationsdaten von Snips-NLU. Das Framework 
macht es uns leicht, können wir doch einfach eine gesammelte Liste von yaml-Dateien an 
die Funktion Dataset.from_yaml_files() in Zeile 38 übergeben und anschließend die 
NLU-Engine darauf initialisieren und über fit() in Zeile 40 trainieren.
Bleibt noch die Initialisierung von ChatbotAI in Schritt 3. Den Luxus, einen Haufen template Dateien in eine Funktion zu schmeißen, haben wir hier nicht. Wir sammeln dennoch erst mal 
alle Dateien dieses Typs in Zeile 52 ein und definieren kurz darauf zwei Pfade:
 WILDCARD_FILE: Das Wildcard-Template haben wir bereits in Listing 5.21 kennengelernt. 
Dessen Aufgabe besteht darin, alle nicht erkannten Befehle an Snips-NLU über einen 
Default Handler weiterzuleiten.
 MERGED_FILE: Da ChatbotAI bei der Initialisierung nur eine template-Datei annimmt, 
müssen wir all unsere Templates aus allen unterschiedlichen Intents in eine Datei zusam menführen und diese Datei nennen wir hier MERGED_FILE. Ich habe den Dateinamen mit 
einem Unterstrich begonnen, um darauf hinzuweisen, dass die Datei nicht händisch ver-
ändert werden sollte, da wir sie bei jedem Start des Assistenten maschinell neu erstellen.
Es folgt das Schreiben des MERGED_FILE in Zeile 58, in dem die zuvor gesammelten template Dateien nacheinander durchgegangen werden. Wenn diese nicht dem Namen des WILD CARD_FILE oder dem MERGED_FILE entsprechen, werden die template-Dateien nach und nach 
in eine Datei geschrieben, die dann unter dem Namen des MERGED_FILE gespeichert wird.



5.4 Das Intent Management 131
Erst am Schluss in Zeile 70 hängen wir das WILDCARD_FILE an. Warum? Weil ChatbotAI die 
Templates sequenziell durchläuft und erst am Ende, wenn keiner der Befehle von irgend einem regulären Ausdruck erkannt wurde, das Wildcard-Template greifen soll, um den Befehl 
resigniert an Snips-NLU weiterzuleiten. Letztendlich wird die Klasse Chat in Zeile 81 auf 
Basis des Pfades zum MERGED_FILE initialisiert. Dort sind nun alle unsere Templates aus 
den verschiedenen Intents enthalten und ChatbotAI ist nun in der Lage, diese zu verstehen 
und den dynamisch geladenen Funktionen zuzuordnen.
Der Default Handler
Es ist fast alles erledigt. Es fehlt zunächst noch der Default Handler, von dem wir nun schon 
einige Male gesprochen haben. Implementieren Sie diesen analog zu Listing 5.27 in der Datei 
intentmgmt.py außerhalb der Klasse IntentMgmt.
Zunächst parsen wir den Befehl, den wir in Form des Parameters text übergeben bekommen, 
und erhalten das Objekt parsing, das alle Informationen über den erkannten Befehl beinhaltet. 
Anschließend lesen wir in den Zeilen 10 und 11 die Sprache aus der globalen Konfiguration 
des Assistenten. Falls diese gesetzt ist, holen wir, ebenfalls aus der globalen Konfiguration, die 
Liste der Sätze in der ausgewählten Sprache, die gesprochen werden sollen, wenn überhaupt 
kein Befehl erkannt wurde. Diese werden wir später noch ergänzen. I did not understand
dient als Fallback, falls keine Sprache definiert wurde.
Nun prüfen wir in Zeile 26 Intent für Intent, ob der erkannte Name des Befehls einem Intent 
aus unserer Liste entspricht, die wir in der Methode __init__() angelegt haben. Ist das der 
Fall, schauen wir noch, wie wahrscheinlich es ist, dass es sich auch tatsächlich um diesen 
Intent handelt (Zeile 35). Ich habe einen Schwellenwert von 0,7 gewählt. Man mag also etwas 
unwissenschaftlich sagen, dass sich das Framework zu 70 % sicher sein muss, dass der Be nutzer eben diesen Befehl ausführen möchte.
Listing 5.27 Der Default Handler reicht nicht verstandene Befehle von ChatbotAI an Snips-NLU weiter
1. @register_call("default_snips_nlu_handler")
2. def default_snips_nlu_handler(session, text):
3. parsing =
4. global_variables.voice_assistant.intent_management.nlu_engine.parse(text)
5.
6. # Schaue, ob es einen Intent gibt, der zu dem NLU-Intent passt
7. intent_found = False
8.
9. # Lese die Sprache des Assistenten aus der Konfigurationsdatei
10. ASSISTANT_LANGUAGE =
11. global_variables.voice_assistant.cfg['assistant']['language']
12.
13. # Hole die Liste aller Antworten, die darauf hindeuten,
14. # dass kein Intent detektiert wurde
15. if ASSISTANT_LANGUAGE:
16. NO_INTENT_RECOGNIZED = global_variables.voice_assistant.cfg['defaults']
17. [ASSISTANT_LANGUAGE]['no_intent_recognized']
18. else:
19. NO_INTENT_RECOGNIZED = ['I did not understand']
20.



132 5 Dialoge und Intentionen
21. # Wähle ein zufälliges Item, das erst mal aussagt,
22. # dass kein Intent gefunden wurde. Wird ein Intent gefunden, dann wird output
23. # durch eine vernünftige Antwort ersetzt.
24. output = random.choice(NO_INTENT_RECOGNIZED)
25.
26. for intent in global_variables.voice_assistant.
27. intent_management.dynamic_intents:
28.
29. # Wurde überhaupt ein Intent erkannt?
30. if parsing["intent"]["intentName"]:
31.
32. # Die Wahrscheinlichkeit wird geprüft, um sicherzustellen,
33. # dass nicht irgendein Intent angewendet wird, der gar nicht gemeint war
34. if (parsing["intent"]["intentName"].lower() == intent.lower()) and
35. (parsing["intent"]["probability"] > 0.7):
36. intent_found = True
37.
38. # Parse alle Parameter
39. arguments = dict()
40. for slot in parsing["slots"]:
41. arguments[slot["slotName"]] = slot["value"]["value"]
42.
43. # Rufe Methode dynamisch mit der Parameterliste auf
44. argument_string = json.dumps(arguments)
45. logger.debug("Rufe {} auf mit den Argumenten {}.", intent,
46. argument_string)
47. output = getattr(globals()[intent], intent)(**arguments)
48.
49. break
50.
51. return output
Nun wissen wir, welchen Intent wir aufrufen müssen. Den Aufruf selber bereiten wir ab 
Zeile 40 vor. Dort erstellen wir ein Dictionary namens arguments, das wir mit den Namen 
aller erkannten Slots und den dazugehörigen Werten befüllen (Zeilen 40 und 41). In den 
Zeilen 44 und 45 geben wir die Argumente einmal zur Kontrolle aus und in Zeile 47 erfolgt 
dann der Aufruf. Hier holen wir uns das Objekt der Methode, die wir aufrufen möchten, über 
getattr(). Hier verweisen wir auf das Modul im ersten Parameter und auf die Funktion 
im zweiten und rufen die Funktion damit auf. Deren Parameter, die wir zuvor im Dictionary 
arguments vorbereitet haben, übergeben wir nachgestellt. Die beiden Sterne vor arguments
deuten darauf hin, dass die Parameter als Schlüsselwertpaare in die Funktion eingegeben 
werden. Der Rückgabewert der Funktion wird in der Variablen output gespeichert und am 
Ende zurückgegeben. Wird kein passender Intent gefunden, enthält output einen für diesen 
Fall vordefinierten Rückgabewert.
Aufruf von process() und Anpassung der Konfiguration
Bevor wir im letzten Schritt die Konfiguration ergänzen, wollen wir noch zügig die Methode 
process() in die Klasse IntentMgmt einpflegen (siehe Listing 5.28).



5.4 Das Intent Management 133
Listing 5.28 Die Methode process verarbeitet Befehle, die dem Intent Management übergeben werden
def process(self, text, speaker):
 # Evaluiere ChatbotAI, wenn keines der strikten Intents greift,
 # wird die Eingabe über die dialogs.template automatisch an snips nlu umgeleitet.
 return self.chat.respond(text)
Dieser spendieren wir zwei Parameter, text und speaker, von denen wir zu Beginn lediglich 
text verwenden. Diese Zeichenkette spielen wir in die Methode respond() von ChatbotAI
ein. Damit durchläuft jeder Befehl erst mal die regelbasierte Verarbeitung durch eben dieses 
Framework. Wenn er dann nicht erkannt wird, landet er zwangsweise im Wildcard-Intent und 
wird über den Default Handler an Snips-NLU weitergereicht. Wie wir eben gesehen haben, 
suchen wir dann dort einen passenden Intent und wenn keiner gefunden wird, geben wir 
aus, dass der Befehl vom Assistenten nicht verstanden wurde.
Nun fügen wir noch der config.yml aus unserem Hauptverzeichnis einen Eintrag hinzu. 
Listing 5.29 zeigt, dass der Eintrag defaults in mehrere Sprachen unterteilt ist. Für die 
deutsche Sprache fügen wir das erste Element hinzu, no_intent_recognized, das wir ja bereits 
in Listing 5.27 referenziert haben. Es handelt sich um eine Liste von Sätzen, die gesprochen 
werden sollen, falls kein Befehl erkannt wurde.
Listing 5.29 Anwendungsweite Standardtexte werden nun in der Konfiguration abgelegt
defaults:
 de:
 no_intent_recognized: ["Ich habe dich nicht verstanden", "Den Befehl habe ich 
 nicht verstanden"]
Einbindung des Intent Managements in die Anwendung
Nun sind wir fertig und können das Intent Management in die main Punkt Pei einbinden. Löschen 
Sie zunächst alle Imports, die Snips-NLU referenzieren, sowie pytz. Den geschaffenen Platz 
nutzen wir, um die Klasse IntentMgmt aus dem Modul intentmgmt zu importieren (from 
intentmgmt import IntentMgmt).
Fahren Sie dann mit dem Aufräumen fort, indem Sie in der Methode __init__() die Ini tialisierung von Snips-NLU löschen und durch Listing 5.30 ersetzen. Was für einen Dritten 
sehr unübersichtlich aussehen mag, ist dem Umstand geschuldet, dass wir die komplexe 
Logik der Initialisierung komplett in die Klasse IntentMgmt ausgelagert haben. Nun geben 
wir noch zügig zurück, wie viele Intents geladen wurden – die genaue Zahl erfahren wir ja 
von unserer Methode get_count() –, und schließen damit die Initialisierung ab.
Listing 5.30 Initialisierung unseres Intent Managements
logger.info("Initialisiere Intent Management...")
self.intent_management = IntentMgmt()
logger.info('{} intents geladen', self.intent_management.get_count())
self.tts.say("Initialisierung abgeschlossen")



134 5 Dialoge und Intentionen
Auch der Aufruf der Methode process() in run() verschlankt unsere Anwendung. In 
Listing 5.31 habe ich den Aufruf von self.nlu_engine.parse() und die anschließende 
Verarbeitung bereits entfernt und durch den einfachen Aufruf unserer Methode process()
ersetzt. Dieser übergeben wir den gesprochenen Satz und den Namen des Sprechers, ver arbeiten aber zunächst nur den Satz.
Listing 5.31 Aufruf von process in der Methode run()
logger.debug('Ich habe verstanden "{}"', sentence)
# Lasse den Assistenten auf die Spracheingabe reagieren
output = global_variables.voice_assistant.intent_management.process(sentence, speaker)
global_variables.voice_assistant.tts.say(output)
global_variables.voice_assistant.is_listening = False
global_variables.voice_assistant.current_speaker = None
Damit haben wir alle Arbeiten für dieses Inkrement abgeschlossen und wir können die 
Anwendung ausführen, um zu prüfen, ob all unsere Intents erkannt und geladen werden. 
Bild 5.11 zeigt, wie die Abhängigkeiten für gettime und stop installiert und die Module samt 
deren Funktionen importiert werden.
Bild 5.11 Das Laden der Intents gettime und stop wird im Log quittiert, ebenso die Installation der 
benötigten Abhängigkeiten.
Testen wir den Aufruf von gettime über den Satz Wie spät ist es in Deutschland, wird dieser 
richtig klassifiziert und der Slot place wird vom Parser erkannt (siehe Bild 5.12). Die Ausgabe 
erfolgt in Textform im Log sowie auditiv über unsere TTS-Klasse.
Bild 5.12 Der Intent-Aufruf von gettime wird erkannt und die Funktion liefert den korrekten Rück gabewert.
Das Grundgerüst steht also und wir können unserem Sprachassistenten beliebige Intents 
hinzufügen, indem wir entweder eine template- oder eine yaml-Datei in den Ordner chatbotai
respektive snips-nlu kopieren und die dazugehörige Funktion im functions-Ordner ablegen. 



5.5 Microservice-Organisation von Intents 135
Diese sollten dann beim nächsten Start des Sprachassistenten erkannt werden. Unser 
Framework ist damit jedoch noch nicht ganz fertig. Wie bereits angekündigt, werden wir 
im nächsten Kapitel mehrere Intents programmieren und unseren Assistenten erst richtig 
nützlich machen. Parallel dazu werden wir das Grundgerüst Stück für Stück um kleine 
Funktionalitäten erweitern. Ich habe diesen Weg gewählt, weil sonst der Aufbau eine lange 
und trockene Lektüre gewesen wäre und ich Ihnen so gleich zeigen kann, wozu die einzel nen Funktionalitäten da sind.
■ 5.5 Microservice-Organisation von Intents
Bevor wir aber in die Intent-Entwicklung einsteigen, noch ein paar kurze Worte zur Orga nisation unserer Intents in Form von Code in der Anwendung. Zwar haben wir durch das 
dynamische Laden neuer Intents nun eine gewisse Flexibilität erreicht und ein in sich abge schlossenes, portables System geschaffen, jedoch gibt es einige Gründe, die dafür sprechen, 
die Architektur zu überdenken.
Ganz konkret möchte ich in diesem Kapitel mit Ihnen die Möglichkeit diskutieren, Intents als 
Microservices zu implementieren. In den letzten Jahren wurde dieser Begriff mehr und mehr 
geprägt und er kennzeichnet eine Anwendung, die in einer Anwendung nur einen kleinen 
Aufgabenbereich abdeckt und in der Regel über ein leichtgewichtiges Protokoll wie HTTP
(Hypertext Transfer Protocol) abgebildet wird, gerne in Form einer REST API (Representa tional State Transfer). Nehmen wir uns unseren Zeit-Intent zum Beispiel, würden wir diesen 
als kleine Anwendung formulieren, die sowohl in Listing 5.32 als auch im Github Repository 
unter 100_extras\100_06_microservices als time_service.py zu finden ist.
Das Framework Flask erlaubt es uns, mit relativ wenig Code einen Webserver zu starten, 
was wir auch in Zeile 40 tun. Dieser ist auf unserer lokalen Maschine (die IP 0.0.0.0 macht 
den Server sowohl auf dem localhost als auch von externen Maschinen erreichbar) auf dem 
Port 5000 erreichbar. Die Flask-Instanz definieren wir in Zeile 5 und können nun Routen 
anlegen, über die verschiedene Methoden unserer Anwendung erreichbar sein sollen. Hier 
definieren wir zwei davon in den Zeilen 24 und 25:
 /gettime: Wird Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichlocalhost:5000/gettime aufgerufen, wird der Parameter place, der für 
den Aufruf der folgenden Methode gettime benötigt wird, auf 'default' gesetzt.
 /gettime/<place>: Wenn wir in der Route einen Platzhalter, beispielsweise mit dem Namen 
place definieren, können wir diesen setzen, indem wir an der Stelle der URL einen belie bigen String einfügen, zum Beispiel Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichlocalhost:5000/gettime/deutschland. Dieser Platzhalter 
wird dann automatisch an den Funktionsaufruf weitergereicht, er muss aber denselben 
Namen haben.
Den Definitionen einer oder mehrere Routen muss eine Funktion folgen, die an diese Route(n) 
gebunden wird. Hier ist das unsere altbekannte gettime() (nur ein wenig gekürzt), die den 
Parameter place entgegennimmt und den Rückgabewert als String aufbereitet. Die einzige 
Besonderheit liegt noch darin, dass output über jsonify() in ein JSON-Format umgewandelt 
wird, das wir später einfacher lesen können.



136 5 Dialoge und Intentionen
Listing 5.32 Der Zeit-Intent in Form eines REST-Service
1. from flask import Flask, jsonify
2. from datetime import datetime
3. import pytz
4.
5. app = Flask(__name__)
6.
7. country_timezone_map = {
8. 'Europe/Berlin': ["deutschland", "germany"],
9. 'Europe/London': ["england", "großbrittanien", "great britain"],
10. 'Europe/Paris': ["frankreich", "france"],
11. 'America/New_York': ["amerika", "america"],
12. 'Asia/Shanghai': ["china"]
13. }
14.
15. def gettimezone(place):
16. timezone = None
17. now = datetime.now()
18. for c in country_timezone_map:
19. if place.strip().lower() in country_timezone_map[c]:
20. timezone = pytz.timezone(c)
21. break
22. return timezone
23.
24. @app.route('/gettime', defaults={'place': 'default'})
25. @app.route('/gettime/<place>')
26. def gettime(place):
27. timezone = gettimezone(place)
28. output = ""
29. if timezone:
30. now = datetime.now(timezone)
31. output = "Es ist " + str(now.hour) + " Uhr und " + str(now.minute) + 
 " Minuten in " + place
32. else:
33. now = datetime.now()
34. output = "Hier ist es " + str(now.hour) + " Uhr " + str(now.minute)
35.
36. return jsonify(output)
37.
38.
39. if __name__ == '__main__':
40. app.run(host='0.0.0.0', port=5000)
Installieren Sie einmal flask und pytz über pip und starten Sie die Anwendung. Sie werden 
sehen, dass im Log ausgegeben wird, dass ein Server auf Ihrer lokalen IP gestartet wurde. 
In meinem Fall (Bild 5.13) ist dieser unter der URL Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrich192.168.1.151:5000 zu erreichen.



5.5 Microservice-Organisation von Intents 137
Bild 5.13 Starten eines Flask-Servers für den Time-Microservice
Preisfrage: Warum bekommen wir einen 404 Not Found-Fehler, wenn wir diese URL auf rufen? Klar, weil wir die Root-URL nicht per app.route gemappt haben. Wenn Sie hingegen 
Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrich192.168.1.151:5000/gettime aufrufen, werden Sie in Ihrem Browser das Resultat Ihres 
Microservice-Aufrufs sehen (siehe Bild 5.14).
Bild 5.14 Der Aufruf des Microservice gibt die Uhrzeit in Form eines Strings zurück
Statt also eine starre Methode in Python zu schreiben, die wir programmatisch fest anbinden, 
haben wir einen eigenständigen Service geschrieben, den wir selbstständig starten, stoppen, 
debuggen und ersetzen können. Klar fehlt für den produktiven Betrieb noch der Einsatz eines 
SSL-Zertifikats, ein sauberes Monitoring, gegebenenfalls ein API Gateway, aber der Service 
selbst soll uns für den Hausgebrauch erst mal genügen.
Listing 5.33 zeigt, wie unser Microservice in ein einfaches Intent Management eingebunden 
werden kann. Die Intents organisiere ich hier der Einfachheit halber in einem Dictionary in 
den Zeilen 3 bis 4 und füge unseren gettime-Service hinzu, indem ich dessen Endpunkt, also 
die URL, über die er erreichbar ist, als Wert des Schlüsselwertpaares hinterlege.
Beim Start der Demo-Anwendung nehmen wir in Zeile 11 an, dass der Intent gettime durch 
eines der Dialog-Frameworks ermittelt und der Slot mit dem Namen place auf Deutschland
gesetzt wurde. Nun gehen wir exemplarisch in Zeile 18 alle uns bekannten Intents aus dem 
Dictionary durch und ermitteln den Endpoint, also dessen URL. Anschließend werden die 
Parameter des Endpunkts befüllt, indem wir alle Zeichenketten in spitzen Klammern ersetzen, 
die dem Namen eines Slots entsprechen. In diesem Fall wäre das nur ein Parameter namens 
place. Sind die Parameter in die URL eingearbeitet, kann diese in Zeile 28 aufgerufen werden. 
Die Response ist ein Objekt, das sowohl einen HTTP Return Code als auch den Inhalt der 
Response beinhaltet. Codes, die mit 2 beginnen, sind in der Regel Erfolgsmeldungen, wobei 
eine 4 am Anfang auf einen Fehler hindeutet. Sie kennen sicher die Fehlermeldung 404, die 
Sie manchmal im Browser erhalten, wenn eine Seite nicht gefunden wurde.



138 5 Dialoge und Intentionen
Erhalten wir eine 200, die für „okay“ steht, dann holen wir uns den Inhalt der Response und 
geben ihn in Zeile 36 aus. Im Fehlerfall schreiben wir in Zeile 34 eine kurze Notiz ins Log.
Listing 5.33 Ein rudimentäres Intent Management auf Basis von Microservices
1. import requests
2.
3. intents = {
4. 'gettime': 'Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrich192.168.1.151:5000/gettime/<place>'
5. }
6.
7. if __name__ == '__main__':
8.
9. # Annahme: Intent Management hat gettime als passenden Intent ermittelt
10. # und die Slots befüllt.
11. intent = 'gettime'
12. slots = {
13. 'place': 'Deutschland'
14. }
15.
16. # Ermittle den Microservice
17. endpoint = ""
18. for key in intents:
19. if key == intent:
20. endpoint = intents[key]
21. break
22.
23. # Befülle die Slots, indem die Variablen in <> aus der Service-URL
24. # ersetzt werden
25. for slot in slots:
26. endpoint = endpoint.replace('<' + slot + '>', slots[slot])
27.
28. response = requests.get(endpoint)
29.
30. output = ""
31. if response.status_code == 200:
32. output = response.json()
33. elif response.status_code == 404:
34. print('Der Service ' + intent + ' ist nicht erreichbar.')
35.
36. print(output)
Wie würde es aussehen, wenn wir nun einen Service hinzufügen wollten? Das wäre relativ 
simpel, müssten wir doch lediglich das Intents-Dictionary um weitere Einträge ergänzen, 
indem wir den Intent-Namen und dessen Endpunkt eintragen.
Die Vorteile, die sich aus dem Einsatz von Microservices ergeben, sind umfangreich und 
sollen hier kurz zusammengefasst werden.
 Austauschbarkeit: Sie können einen Microservice zu jeder Zeit austauschen, um bei spielsweise eine neue Version auszurollen. Dazu müssen Sie den Sprachassistenten selber 
nicht stoppen, denn solange der Endpunkt des Service nicht angesprochen wird, sind 
Sprachassistent und Microservice nicht verbunden.



5.5 Microservice-Organisation von Intents 139
 Testbarkeit: Bevor Sie einen neuen Service ins Leben rufen, können Sie ihn sehr modular 
auf Herz und Nieren testen, etwa indem Sie verschiedenste Werte eingeben und schauen, 
wie die Logik reagiert. Diese Tests lassen sich auch ganz wunderbar automatisieren, müssen 
Sie doch nur für eine Liste von Testfällen verschiedene URLs nacheinander aufrufen und 
den Rückgabewert prüfen. Die Logik im Intent Management oder beispielsweise der Sprach erkennung wird bei den Tests von vornherein ausgeschlossen, da Sie die Anwendungsteile 
ja gar nicht zu Gesicht bekommen. Somit können Fehler ausgeschlossen werden, die von 
anderen Anwendungsbereichen hervorgerufen wurden.
 Wiederverwendbarkeit: Wie in Bild 5.15 zu sehen, können Sie auch mehrere Sprach assistenten an einen Microservice hängen, sodass dieser für jedes einzelne Gerät genutzt 
werden kann. Dadurch erhöht sich auch gleich die Wartbarkeit, denn statt Intents auf zig 
Geräten zu betreiben, müssen Sie nur dafür sorgen, dass der Server, der den Intent als 
Microservice hostet, seinen Dienst verrichtet.
Python Runme
Python Runme
REST Endpoint REST Endpoint
Netzwerk
GET/PUT/
POST/DELETE
JSON / XML
API Gateway
Zentrale(r) Server
Wikipedia
Uhrzeit
hps://myintents:5001/geme
hps://myintents:5002/wiki
Bild 5.15 Microservice-basierte Intents können von verschiedenen Devices über das HTTP-Protokoll 
konsumiert und im Betrieb ausgetauscht werden. Ein API Gateway moderiert bei Bedarf die Kommuni kation zwischen Service und Abnehmer.
 Skalierung: Nehmen wir mal an, Sie bauen einen Sprachassistenten in Ihrem Unter nehmen auf und stellen fest, dass Ihre Kollegen und Mitarbeiter sehr häufig die neusten 
Unternehmensmeldungen abhören. Der Flask-Server glüht, was Sie natürlich erst mal sehr 
freut, denn die Technologie scheint gut anzukommen, allerdings könnte die Last zu einem 
echten Problem werden. Die Lösung ist recht simpel: Sie starten einfach einen zweiten 
Service auf einem anderen Port und schalten einen Load Balancer (häufig wird NGINX
verwendet) vor die beiden Microservices. Dieser entscheidet dann selbstständig, an welche 
Microservice-Instanz die Anfrage weitergeleitet wird.



140 5 Dialoge und Intentionen
 Lose Kopplung und Sprachunabhängigkeit: Die Verwendung von Microservices erlaubt 
es, Services in jeder Sprache zu implementieren, die einen REST-Endpunkt bereitstellen 
kann. Sie können also ebenfalls Anwendungen in Java oder C# schreiben, die Sie mit Ihrem 
Python-basierten Sprachassistenten verknüpfen. Wenn Sie merken, dass ein Service zehn 
Jahre später neu implementiert werden muss, dann können Sie das wiederum in einer 
anderen Sprache tun, solange die URL des Microservices erhalten bleibt.
Bei so vielen Vorteilen liegt eine Frage auf der Hand: Warum entwickeln wir den Sprach assistenten nicht in einer Microservice Architektur? Na ja, es gibt eben auch Nachteile: Der 
Anwendungsfall, dass wir den Sprachassistenten mal eben auf einen Raspberry Pi deployen 
und mit zu einem Freund nehmen, um ihn dort vorzuführen, funktioniert mit Microservices 
nicht so einfach, denn wir müssten diese so verfügbar machen, dass sie auch aus dem frem den Netz erreichbar sind. Das wiederum bedarf hoher Betriebsaufwände, müssten wir uns 
doch um Firewallfreischaltungen, um feste IPs und um Sicherheitsbelange kümmern, die 
durch den Expose von Ports ins Internet notwendig werden würden. Weiterhin müssten wir 
freie Ports auf unserem Server verwalten, wenn wir neue Services in Betrieb nehmen. In 
Bild 5.15 sind lediglich Port 5001 und 5002 von den Flask-Servern belegt, wenn Sie aber 
beispielsweise 20 Intents verwalten, müssen Sie schon etwas genauer hinschauen, welche 
Ports noch zu Verfügung stehen und welche schon verwendet werden. Abhilfe schafft hier 
ein API Gateway, das die Endpunkte für uns verwaltet und den Sprachassistenten gebündelt 
zu Verfügung stellt.
HINWEIS: Ein API Gateway ist eine Anwendung, die zwischen verschiedenen An wendungen sitzt (häufig Client und Backend) und dazu dient, APIs zu organisieren, 
Aufrufe entgegenzunehmen, zu bündeln und weiterzuleiten. Das nimmt einer 
Microservice-Architektur häufig einen Teil ihrer Komplexität und ermöglicht eine 
verstärkte Zugriffskontrolle auf die einzelnen Dienste oder aber auch ein Monito ring über Verfügbarkeiten oder Aufruffrequenz. Bekannte Vertreter dieser Tools 
sind etwa Kong Gateway, Tyk oder Apache APISIX.
Doch auch ein solches Gateway will verwaltet und gepflegt werden. Des Weiteren sind Sie 
darauf angewiesen, dass Ihr Netzwerk funktioniert, denn sollte dieses einmal ausfallen, kön nen Microservices und Sprachassistent nicht mehr miteinander kommunizieren. Sie sehen, 
dass es für den Einzelanwender weniger Sinn ergibt eine solche Architektur zu wählen, für 
ein Unternehmen, dass sich den Betrieb personell leisten kann und will, wahrscheinlich 
schon eher.



6 Intents entwickeln
Mit zwei Intents ist unser Sprachassistent kaum in der Lage, die Aufmerksamkeit zu er wecken, die er in seiner finalen Ausbaustufe erzielt. Daher werden wir ihn in diesem Kapitel 
um weitere Funktionen ergänzen und, wie schon angekündigt, dabei das Framework Stück 
für Stück ausbauen, sodass wir u. a. Audiodateien, Streaming Audio, Callbacks oder mehr teilige Dialoge verwenden können. Auch wenn Sie auf den ersten Blick keine Verwendung für 
die jeweilige Funktion haben sollten, ist mein Rat, das Kapitel zu bearbeiten, um zu sehen, 
was für Änderungen am Framework wir implementieren. Bei akuter Unlust laden Sie sich 
einfach den Stand nach dem jeweiligen Abschnitt aus dem Github Repository. Beginnen wir 
nun mit etwas Einfachem, nämlich der Wiedergabe von Audiodateien.
■ 6.1 Tierstimmen
Offen gesprochen sind Tierlaute sicher nicht das Erste, woran man bei einem professionellen 
Sprachassistenten denkt, jedoch ist dies einerseits der Intent, den meine Kinder in ihren 
jungen Jahren am häufigsten mit viel Freude verwendet haben, und zweitens ist der Anwen dungsfall auf technischer Ebene ein Abspielen beliebiger Audiodateien. Es muss sich dabei 
also nicht um Tierlaute handeln. Denkbar wäre ein Audioplayer für Podcasts, Nachrichten, 
Witze oder Musik.
Für das Abspielen von Audiodateien werden wir Pygame verwenden, eine Game Engine, die 
eigentlich wesentlich mehr kann, jedoch habe ich festgestellt, dass sie auf den meisten Platt formen lauffähig ist und mit nur wenigen Zeilen Code bedient werden kann. Beim einfachen 
Abspielen soll es jedoch nicht bleiben, muss doch der Stop-Intent noch dahingehend angepasst 
werden, dass er nicht nur Sprache, sondern auch Audio unterbrechen kann.
Um praktisch mit der Umsetzung zu beginnen, legen Sie einen Ordner 10_a_tierlaute und ein 
dazu passendes Environment an. Kopieren Sie dazu wie gehabt Ordner und Abhängigkeiten 
aus dem vorherigen Kapitel. Führen Sie dann pip install pygame aus.
Die Initialisierung des Moduls ist denkbar einfach. Fügen Sie from pygame import mixer
der main Punkt Pei hinzu. Weiterhin fügen Sie in der Methode __init__() der Klasse VoiceAssistant
vor der Initialisierung des Intent Managements die Zeile mixer.init() hinzu. Damit sind 
wir tatsächlich auch schon fertig.



142 6 Intents entwickeln
Im nächsten Schritt passen wir den Stop-Intent in functions/stop/intent_stop.py an, sodass 
dieser auch den Mixer unterbricht, sollte dieser eine Audiodatei wiedergeben. Wir bauen 
ihn weiterhin so aus, dass wir dessen Parameter in eine Konfiguration (siehe Listing 6.2) 
auslagern und sie in Listing 6.1 in den Zeilen 12–19 einlesen. In Zeile 30 beginnen wir dann 
damit, den Mixer zu unterbrechen. Über mixer.music.get_busy() können wir feststellen, 
ob etwas gespielt wird. Ist das der Fall, rufen wir mixer.music.stop() auf. Fällt Ihnen auf, 
dass wir gar keine Instanz auf mixer vorhalten und in die Methode reinreichen müssen? 
Pygame verwaltet diese als Singleton in der Bibliothek, sodass es stehts nur einen Mixer gibt.
Listing 6.1 Der Stop-Intent unterbricht nicht nur gesprochenen Text, sondern auch die Wiedergabe 
von Audiodateien.
1. from loguru import logger
2. from chatbot import register_call
3. import global_variables
4. import yaml
5. import random
6. import os
7. from pygame import mixer
8.
9. @register_call("stop")
10. def stop(session_id = "general", dummy=0):
11.
12. cfg = None
13.
14. # Laden der Intent-eigenen Konfigurationsdatei
15. config_path = os.path.join('intents','functions','stop','config_stop.yml')
16. with open(config_path, "r", encoding='utf8') as ymlfile:
17. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
18.
19. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
20.
21. if cfg:
22.
23. result =
24. random.choice(cfg['intent']['stop'][LANGUAGE]['not_saying_anything'])
25.
26. if global_variables.voice_assistant.tts.is_busy():
27. global_variables.voice_assistant.tts.stop()
28. result = random.choice(cfg['intent']['stop'][LANGUAGE]['be_silent'])
29.
30. if mixer.music.get_busy():
31. mixer.music.stop()
32. result = random.choice(cfg['intent']['stop'][LANGUAGE]['be_silent'])
33.
34. return result
35. else:
36. logger.error("Konnte Konfigurationsdatei für Intent 'stop' nicht laden.")
37. return ""



6.1 Tierstimmen 143
Die Konfiguration dieses Intents ist in Listing 6.2 zu sehen. Ich überlasse es Ihnen, die Rück gabewerte in not_saying_anything und be_silent nur mit einem leeren String auszustatten. 
Eventuell ist es sinnvoller, einen Sprachassistenten nach einem Schweigebefehl nicht noch 
etwas in der Folge sprechen zu lassen.
Listing 6.2 Konfiguration des Stop-Intents
1. intent:
2. stop:
3. de:
4. not_saying_anything: ["Ich sage doch gar nichts."]
5. be_silent: ["Okay, ich bin still."]
6. en:
7. not_saying_anything: ["I am not saying anything."]
8. be_silent: ["Okay, I am silent."]
Kommen wir nun endlich zum Intent animalsound. Legen Sie für diesen einen gleichnami gen Ordner im Ordner intents/functions an. Im Repository finden Sie an dieser Stelle einen 
Ordner animals, der verschiedene Laute im ogg-Format beinhaltet (siehe Bild 6.1). Kopieren 
Sie diesen hinein oder verwenden Sie andere Dateien, die Sie aufrufen und abspielen lassen 
möchten. Die einem Intent zugehörigen Dateien können Sie theoretisch ablegen, wo immer 
Sie möchten. Ich habe mir jedoch angewöhnt, sie im Ordner der Funktion zu speichern, um 
Logik und Ressourcen an einem Platz zu haben und bei Bedarf schnell wiederzufinden.
Bild 6.1 14 Beispielhafte Tierlaute, die über einen Sprachbefehl wiedergegeben werden können.



144 6 Intents entwickeln
Fahren wir damit fort, dass wir unserem Intent eine Konfigurationsdatei spendieren, die 
ebenso wie der animals-Ordner im Ordner animalsound angelegt wird. Das Format können 
wir nach unseren Wünschen bestimmen, lesen wir es doch nur in der Implementierung des 
Intents selber. Ich halte mich jedoch wie bei den vorherigen an den Aufbau intent.[intentname].
[eigenschaften]. Eine einheitliche Struktur erhöht nicht nur die Lesbarkeit ungemein, sondern 
ermöglicht es uns auch, Teile, wie etwa das Auslesen der Konfiguration, per Copy-and-paste 
für weitere Funktionen zu übernehmen.
Listing 6.3 Konfigurationsdatei config_animalsounds.yml
1. intent:
2. animalsounds:
3. de:
4. animal_not_found: ["Das Tier kenne ich nicht."]
5. en:
6. animal_not_found: ["I don't know such an animal."]
7. animals:
8. cat: ["cat", "katze"]
9. chicken: ["chicken", "huhn", "mum", "mama"]
10. cock: ["cock", "hahn"]
11. cow: ["cow", "kuh"]
12. dog: ["dog", "hund"]
13. donkey: ["donkey", "esel"]
14. duck: ["duck", "ente"]
15. goat: ["goat", "ziege"]
16. goose: ["goose", "gans"]
17. horse: ["horse", "pferd"]
18. ox: ["ox", "ochse", "dad", "papa"]
19. pig: ["pig", "schwein"]
20. sheep: ["sheep", "schaf"]
21. turkey: ["turkey", "truthahn"]
Sie sehen in Listing 6.3, dass wir ebenso wie bei der Zuordnung der Zeitzone auf ein Map
(oder in Python entsprechend auf ein Dictionary) zurückgreifen. Das Ziel ist es, dass, wenn 
ein Begriff aus den Listen der Dictionary-Einträge intent.animalsounds.animals erkannt wird, 
der Schlüssel zurückgeliefert wird, der Auskunft darüber gibt, welche Audiodatei abgespielt 
werden soll. Da Tiere in der Regel nicht zwischen verschiedenen Sprachen unterscheiden, 
müssen wir das hier auch nicht tun. Wie macht die Katze? und What sound does a cat make?
evaluieren beide zum Schlüssel cat.
Es folgt die Implementierung des Intents selber. Zeile 8 in Listing 6.4 verrät, dass wir den 
Intent auf Basis von ChatbotAI implementieren, denn hier verwenden wir die Annotation 
@register_call. Den Parameter animal in Zeile 9 werden wir dafür benutzen, um einen 
Parameter über das Framework zu setzen. Wir müssen ihn hier mit dem Default-Wert none
versehen, weil wir ja auch schon session_id mit einem solchen versehen haben und alles 
andere unzulässig wäre. Das nun häufiger gesehene Auslesen der Konfigurationsdatei ge schieht in den Zeilen 11–25. Kurz darauf ermitteln wir, ob das Tier, das im Parameter animal
übergeben wurde, in der Konfiguration hinterlegt und einer Audiodatei zugeordnet ist. Wenn 
dem so ist, unterbrechen wir den mixer, falls derzeit etwas anderes gespielt wird, in Zeile 37, 
laden die ermittelte ogg-Datei in Zeile 38 und spielen sie eine Zeile darunter ab.



6.1 Tierstimmen 145
Listing 6.4 Implementierung von intent_animalsounds.py
1. from chatbot import register_call
2. import global_variables
3. import random
4. import os
5. import yaml
6. from pygame import mixer
7.
8. @register_call("animalSound")
9. def animalSound(session_id = "general", animal="none"):
10.
11. config_path =
12. os.path.join('intents','functions','animalsounds',
13. 'config_animalsounds.yml')
14. ogg_path = os.path.join('intents','functions','animalsounds','animals')
15. cfg = None
16. with open(config_path, "r", encoding='utf8') as ymlfile:
17. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
18.
19. if not cfg:
20. logger.error("Konnte Konfigurationsdatei für animalsounds nicht lesen.")
21. return ""
22.
23. # Holen der Sprache aus der globalen Konfigurationsdatei
24. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
25.
26. # Das Tier ist nicht bekannt
27. ANIMAL_UNKNOWN =
28. random.choice(cfg['intent']['animalsounds'][LANGUAGE]['animal_not_found'])
29.
30. animals = {}
31. for key, value in cfg['intent']['animalsounds']['animals'].items():
32. animals[key] = value
33.
34. for a in animals:
35. if animal.strip().lower() in animals[a]:
36. ogg_file = os.path.join(ogg_path, a + '.ogg')
37. if mixer.music.get_busy():
38. mixer.music.stop()
39. mixer.music.load(ogg_file)
40. mixer.music.play()
41.
42. # Der Assistent muss nicht sprechen, wenn ein Tierlaut wiedergegeben wird
43. return ""
44.
45. # Wurde kein Tier gefunden?
46. return ANIMAL_UNKNOWN
Eine Textwiedergabe ist nicht vonnöten, wenn eine Audiodatei gespielt wird, deswegen geben 
wir in Zeile 42 eine leere Zeichenkette zurück. Konnte das Tier nicht identifiziert werden, 
wird ANIMAL_UNKNOWN ausgesprochen, ein Zufallswert aus der Liste animal_not_found
aus der Konfiguration.



146 6 Intents entwickeln
Fehlt zu guter Letzt noch das Template für den Intent, den wir in Form der Datei animalsounds.
template im Ordner intents/chatbotai anlegen. Der Inhalt in Listing 6.5 birgt eine maßgebliche 
Neuerung. Wir übergeben hier nämlich das erste Mal einen Parameter. In Zeile 2 erzeugen 
wir eine Regular Expression, die einen Satz erwartet, der entweder mit Wie macht der, Wie 
macht die oder Wie macht das beginnt. Darauf kann eine beliebige Zeichenkette folgen (zu 
erkennen am .*). Diese Zeichenkette wird dem Parameter mit dem Namen animal zugewiesen, 
das spezifiziert der Ausdruck ?P<animal>.
Listing 6.5 Template für den Tierlaute-Intent
1. {% block %}
2. {% client %}(Wie macht (der|die|das)) (?P<animal>.*){% endclient %}
3. {% response %}{% call animalSound: %animal %}{% endresponse %}
4. {% endblock %}
Als Antwort des Frameworks auf eine Anfrage, die dem Muster aus Zeile 2 entspricht, wird 
die Funktion animalSound() aufgerufen. Als erster Parameter wird der Wert aus animal
übergeben.
Probieren Sie den Intent einmal über python main Punkt Pei und den Befehl Wie macht die Katze?
aus. Sie sollten als Belohnung für Ihre Mühen ein zufriedenes Miauen hören. Machen Sie 
auch entsprechende Negativtests mit Tieren, die nicht erkannt werden, und versuchen Sie 
auch mal, eigene Audiodateien einzubinden.
■ 6.2 Wikipedia
Der nächste Intent ist sehr schlank, kommt mit wenig Code aus, hat aber dennoch eine hohe 
Informationsdichte und bringt einen entsprechenden Mehrwert mit sich. Die Rede ist von 
einem Wikipedia-Intent, den wir implementieren wollen, um die umfangreiche und gut ge pflegte Wissensbasis der Seite anzuzapfen und dadurch Fragen beispielsweise zu Personen, 
Orten und Ereignissen stellen zu können.
Der Preis, den wir für dieses Wissen zahlen, ist eine notwendige Verbindung ins Internet. 
Zwar gibt es auch Dumps der Seite, die ganz offiziell heruntergeladen werden können1
, doch 
wollen wir es uns etwas einfacher machen und ein entsprechendes Python Package nutzen, 
das uns die Abfragen über eine simple API anbietet. Des Weiteren ist ein Dump derzeit etwa 
5,9 GB groß und nicht jedes System ist in der Lage, diese Datenmenge in annehmbarer Zeit 
zu durchsuchen.
Akzeptieren wird der Intent folgende Fragen:
 Wer ist …?
 Was ist …?
1 Diese finden Sie hier: Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichdumps.wikimedia.org/dewiki



6.2 Wikipedia 147
 Kennst du …?
Mit dem Handwerkszeug, das ich Ihnen gleich an die Hand gebe, können Sie diese zugegeben 
etwas kurze Liste leicht um weitere W-Fragen erweitern.
Klonen Sie wie gehabt das letzte Projekt und Environment und legen Sie auf deren Basis ein 
neues mit dem Namen 10_b_wikipedia an. Aktivieren Sie das Environment, installieren Sie 
alle bisherigen Abhängigkeiten und denken Sie auch daran, python -m snips_nlu download de
als Administrator auszuführen, um das deutsche Sprachpaket für Snips-NLU zu installieren.
Legen Sie dann im Ordner intents/functions einen neuen Ordner wiki an und darin wiederum 
eine Konfigurationsdatei config_wiki.yml. Deren Inhalt ist in Listing 6.6 zu sehen.
Listing 6.6 Konfigurationsdatei für den Intent wiki
intent:
 wiki:
 de:
 unknown_entity: ["Ich weiß nichts über {}.", "{} sagt mir nichts."]
 en:
 unknown_entity: ["I don't know {}.", "{} rings no bell."]
Wir reagieren hierbei lediglich darauf, dass wir keinen Eintrag zu einer abgefragten Entität 
in Wikipedia finden und sorgen für die auditive Wiedergabe dieses Scheiterns.
Die Anlage der Datei intent_wiki.py ist der nächste Schritt. Sie sehen in Listing 6.7, dass 
wir hier auf ChatbotAI setzen, denn unsere Fragen sind relativ statisch. Der Einsatz von 
Snips-NLU wäre naheliegend, um leicht eine Erweiterung auf mehrere W-Fragen (Wann ist 
Weihnachten? oder Wo liegt Mexiko?) umzusetzen, jedoch bedarf es in diesem Fall weiterer 
Logik und nicht nur eines besseren Parsers.
Wir registrieren also in gewohnter Manier in Zeile 10 einen Funktionsaufruf mit Namen 
wiki und implementieren im Folgenden die dazugehörige Funktion. Der Parameter query
beinhaltet die zu suchende Entität, die als Query in Zeile 35 an das Modul wikipedia über geben wird. Über wikipedia.summary() bekommen wir durch den Parameter sentences=1
eine kurze Zusammenfassung zu dem gesuchten Objekt zurückgeliefert. Liefert summary
kein Resultat, bemühen wir in einem zweiten Versuch wikipedia.search() in Zeile 37 
und schauen, was die Suche für Resultate zu dem angeforderten Begriff liefert. Hier wird 
eine Liste zurückgegeben und sofern diese nicht leer ist, erfragen wir erneut per summary()
eine Zusammenfassung für die Resultate der Suche an. Liefert die Suche jedoch keine Ergeb nisse, wird UNKNOWN_ENTITY zurückgegeben, das wir zuvor aus der Konfigurationsdatei 
geladen haben.
Listing 6.7 Logik des Intents wiki
1. from loguru import logger
2. from chatbot import register_call
3. import global_variables
4. import yaml
5. import random
6. import os



148 6 Intents entwickeln
7. import wikipedia
8. import constants
9.
10. @register_call("wiki")
11. def wiki(session_id = "general", query="none"):
12. cfg = None
13.
14. # Laden der Intent-eigenen Konfigurationsdatei
15. config_path = constants.find_data_file(os.path.join('intents',
16. 'functions','wiki','config_wiki.yml'))
17. with open(config_path, "r", encoding='utf-8') as ymlfile:
18. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
19.
20. # Sprache aus der globalen Konfigurationsdatei holen
21. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
22.
23. # Setze die richtige Sprache für Wikipedia
24. if LANGUAGE:
25. wikipedia.set_lang(LANGUAGE)
26.
27. UNKNOWN_ENTITY = random.choice(cfg['intent']['wiki'][LANGUAGE]
28. ['unknown_entity'])
29. UNKNOWN_ENTITY = UNKNOWN_ENTITY.format(query)
30.
31. # Konnte die Konfigurationsdatei des Intents geladen werden?
32. if cfg:
33. query = query.strip()
34. try:
35. return wikipedia.summary(query, sentences=1)
36. except Exception:
37. for new_query in wikipedia.search(query):
38. try:
39. return wikipedia.summary(new_query)
40. except Exception:
41. pass
42. return UNKNOWN_ENTITY
43. else:
44. logger.error("Konnte Konfiguration für Intent 'wikipedia' nicht laden.")
45. return ""
Speichern Sie die Datei nun und legen Sie im selben Ordner eine requirements.txt an, die 
lediglich die Zeile wikipedia==1.4.0 beinhaltet. Dadurch weiß unser Intent Management
genau, welche Version des Packages installiert werden soll. Legen wir nun noch das Template
für ChatbotAI in intents\chatbotai\wiki.template an.
Listing 6.8 Template für den Wikipedia-Intent
{% block %}
 {% client %}(Wer ist|was ist|Kennst du) (?P<query>.*){% endclient %}
 {% response %}{% call wiki: %query %}{% endresponse %}
{% endblock %}



6.2 Wikipedia 149
Der Ausdruck in Listing 6.8 ist wieder leicht zu interpretieren, wird doch nur auf die drei 
Satzanfänge reagiert, die wir zuvor aufgelistet haben. Diesen muss ein Begriff folgen, den 
wir hier query nennen, und dieser wird dann der Funktion übergeben, die wir zuvor in 
Listing 6.7 ausformuliert haben.
Nach dem Start der Anwendung sollte zunächst wikipedia nachinstalliert und dann der 
Sprachassistent gestartet werden. Probieren Sie einige Abfragen aus – Sie werden verblüfft 
sein, was für eine Funktionsbandbreite wir mit diesem Intent abdecken. Leider beinhalten 
Zusammenfassungen häufig Informationen wie etwa die Lautschrift eines Namens oder 
Datumswerte. Versuchen Sie es mal mit Wer ist Barack Obama? Sie werden feststellen, dass 
hier noch ein wenig Arbeit auf uns zukommt, wollten wir die Antwort nachträglich perfekt 
formatieren. Für einen ersten Entwurf würde es aber sicher reichen, per Regular Expression 
alle von eckigen oder runden Klammern eingefassten Satzteile zu entfernen.
6.2.1 Question Answering mittels Language Model
Der vorherige Abschnitt war etwas zu kurz geraten, als dass ich Sie schon in den Feierabend 
entlassen möchte. Deshalb schieben wir noch das Thema Question Answering ein, das durch 
die Fortschritte im Bereich NLP und Deep Learning erneut große Aufmerksamkeit erhalten 
hat. Dies ist für uns ganz besonders relevant, da wir uns ja mit der Interpretation natürlicher 
Sprache beschäftigen.
0
50
100
150
200
250
Jemals verwendet Monatlich verwendet Täglich verwendet
Bild 6.2 Übersicht über aufgerufene Funktionen von Sprachassistenten in den USA nach Thema 
und Nutzungszeitraum (Statista, 2020)



150 6 Intents entwickeln
Bild 6.2 zeigt einerseits, dass das Stellen von Fragen eine der elementarsten Funktionen 
eines Sprachassistenten ist, ist dies doch eine Funktion, die mit am häufigsten aufgerufen 
wird. Doch ich möchte auch betonen, dass wir im Verlauf des Buches einen Großteil der hier 
gezeigten Funktionen abdecken werden und wir mit unserem fertigen Assistenten sehr nah 
an dem dran sind, was Benutzer im täglichen Leben tatsächlich verwenden.
Kommen wir nun auf die eigentliche Aufgabe des Question Answering zu sprechen, dem auto matisierten Beantworten von Fragen in natürlicher Sprache. Fragen und Antworten können 
auf viele verschiedene Arten formuliert werden.
Tabelle 6.1 ist sicher nicht erschöpfend und besonders den Germanisten unter Ihnen werden 
sicher noch einige Ausprägungen mehr einfallen. Mir ist wichtig zu zeigen, wie unterschied lich Fragen gestellt sein können. Dazu kommt, dass sich Fragen noch auf eine spezifische 
Domäne oder auf einen nicht umrissenen Bereich beziehen können. Weiß der Sprachassistent, 
dass eine Frage zum Wetter gestellt wird, ist es häufig mit weniger Aufwand verbunden, 
diese zu beantworten. Weiterhin kann es sein, dass eine Frage zu einem ganz bestimmten 
Text gestellt wird. Ein schönes Beispiel ist ein Chatbot, der einen User Help Desk entlasten 
soll, indem ihm Fragen zu bestimmten Prozessen in einem Unternehmen gestellt werden 
können. Die Antworten kann der Chatbot dann beispielsweise einem Handbuch oder einer 
Verfahrensanweisung entnehmen.
Tabelle 6.1 Übersicht über verschiedene Fragetypen
Frage Beschreibung Beispiel
Alternativ fragen
Stellen zwei oder mehr Alternativen 
zur Auswahl.
Wird es morgen regnen oder 
schneien?
Einwandsfrage Erwartet konkrete Gegenargumente 
gegen eine Aussage.
Was spricht dagegen, unangeschnallt 
Auto zu fahren?
Informations frage
Eine Frage nach einer konkreten 
Information.
Wie hoch ist der Eiffelturm?
Rhetorische 
Frage
Eine Frage, die keine Antwort er wartet.
Bumblebee, warum verstehst du 
mich nie?
Zustimmungs frage
Eine Frage, die eine konkrete Zustim mung oder Ablehnung erwartet.
Schaffe ich es vor der Arbeit noch, 
laufen zu gehen?
Auch logische Fragen spielen eine Rolle, hier zwei Beispiele:
 Wie viel ist x, wenn gilt x2 = 16?
 Wenn Peters Vater drei Söhne hat, wie viel Brüder hat dann Peter?
Doch wie schon angedeutet, können auch Antworten sehr vielfältig ausfallen. Im einfachsten 
Fall kann eine Frage durch ein Schlagwort, ein einfaches Ja oder Nein oder eine Zahl ausge drückt werden. In schwierigeren Fällen können auch Texte, Listen, Tabellen oder Bilder als 
Antwort zurückgeliefert werden. Sie merken schon, dass es darauf hinausläuft, dass wir eher 
zum Training eines Modells greifen, anstatt die verschiedenen Fragetypen manuell zu parsen.



6.2 Wikipedia 151
Wie aber trainiert man ein solches Modell und wie sehen die Trainingsdaten dafür aus? 
Beginnen wir mit Letzterem. Das Stanford Question Answering Dataset 2.02
 (SQuAD) ist ein 
Datensatz aus dem Gebiet des Sprachverständnisses, der pro Datensatz einen Text, eine 
Frage und die dazugehörige Antwort enthält. Die Texte stammen aus Wikipedia und Fragen 
und Antworten wurden durch Menschen gestellt und beantwortet, sodass in Version 2.0 
circa 100.000 Datensätze zusammengetragen wurden. Bild 6.3 zeigt einen exemplarischen 
Datensatz zur Pest, samt der von Menschen gestellten Fragen und der gegebenen Antworten 
(Ground Truth Answers). Neben Prediction ist zu sehen, wie die verschiedenen auf SQuAD trai nierten Modelle die Fragen beantworten. Hier trifft nlnet von Microsoft Research Asia bei der 
ersten und dritten Frage ins Schwarze, beantwortet aber Frage zwei nicht zufriedenstellend.
Bild 6.3 Ein Beispiel aus dem SQuAD-Datensatz zu einem Artikel über den „Schwarzen Tod“. 
Links zu sehen ist der Context, auf der rechten Seite die dazugehörigen Fragen und neben „Ground 
Truth Answers“ die von Menschen gegebenen Antworten. Die Vorhersage (hier durch das Modell 
nlnet von Microsoft Research Asia) wird gegen die Ground Truth geprüft. Eine Übereinstimmung wird 
grün hervorgehoben, eine Diskrepanz rot.
Bevor wir uns nun anschauen, wie wir ein solches Modell trainieren, möchte ich mit Ihnen 
eine kleine Demo entwickeln, mit der wir zuvor auf Basis eines bestehenden Modells erste 
Tests in diesem Aufgabenbereich durchführen können. Wir arbeiten also zur Abwechslung 
mal entgegengesetzt dem Sprichwort Erst die Arbeit, dann das Vergnügen.
Einsatz eines vortrainierten Modells zur Beantwortung von Fragen zu einem 
gegebenen Text
Das Projekt, das wir aufbauen, finden Sie im Repository unter 100_extras\100_07_ques tionanswering. Die darin befindliche requirements.txt soll Ihnen helfen, das Environment 
aufzusetzen. Passen Sie gegebenenfalls die PyTorch-Version an, falls Sie ein anderes CUDA Release installiert haben.
2 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichrajpurkar.github.io/SQuAD-explorer



152 6 Intents entwickeln
Listing 6.9 Question Answering mit Hugging Face Transformers und einem multilingualem 
BERT-Modell der deutschen Telekom
1. from transformers import pipeline
2. from multiprocessing import freeze_support
3.
4. if __name__ == '__main__':
5. freeze_support()
6. qa_pipeline = pipeline(
7. "question-answering",
8. model="deutsche-telekom/electra-base-de-squad2",
9. tokenizer="deutsche-telekom/electra-base-de-squad2"
10. )
11.
12. contexts = ['''Obamas Vater, Barack Hussein Obama Senior (1936–1982), stammte 
 aus Nyang’oma Kogelo in Kenia und gehörte der Ethnie der Luo an. Obamas 
 Mutter, Stanley Ann Dunham (1942–1995), stammte aus Wichita im US-
 Bundesstaat Kansas und hatte irische, britische, deutsche und Schweizer 
 Vorfahren. Obamas Eltern lernten sich als Studenten an der University of 
 Hawaii at Manoa kennen. Sie heirateten 1961 in Hawaii, als Ann bereits 
 schwanger war. Damals waren in anderen Teilen der USA Ehen zwischen 
 Schwarzen und Weißen noch verboten. 1964 ließ sich das Paar scheiden. Der 
 Vater setzte sein Studium an der Harvard University fort. Obama sah ihn 
 als Zehnjähriger zum letzten Mal.'''}*2
13.
14. questions = ["Woher kommt Obamas Vater?",
15. "Wann sah Obama seinen Vater zum letzten Mal?"]
16. print(qa_pipeline(context=contexts, question=questions))
In Zeile 1 von Listing 6.9 laden wir pipeline aus dem Package transformers. Pipelines spe zifizieren eine sequenzielle Verarbeitungslogik zur Erledigung bestimmter Aufgaben. In 
Zeile 7 ist zu sehen, dass diese Pipeline die Aufgabe question-answering bewältigen soll. Die 
Zeilen 8 und 9 zeigen, mithilfe welcher Modelle sie dies tut. Die Deutsche Telekom war so 
freundlich, uns Model und Tokenizer auf Basis eines maschinell ins Deutsche übersetzten 
SQuAD2-Datensatzes vortrainiert zu Verfügung zu stellen.
Hugging Face stellt in ihrer Bibliothek transformers eine API bereit, die selbstständig mit dem 
eignen Model Hub kommuniziert und die Modelle anhand des Namens bezieht. Sind diese 
bereits einmal heruntergeladen worden, werden sie in der Regel aus dem lokalen Cache 
geholt. Der Model Hub ist einen längeren Besuch wert, Sie finden diesen unter der URL 
Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichhuggingface.co/models. Dort finden sich Modelle zum Beispiel nach Aufgaben und Sprachen 
sortiert angezeigt (siehe Bild 6.4), die Sie manuell oder über die API beziehen können, um 
sie direkt einzusetzen oder einem Fine Tuning zu unterziehen.
Ist die Pipeline initialisiert, legen wir einen Text in der Liste context an, zu dem wir eine Frage 
stellen. Die Liste enthält zwar nur ein Element, das aber über den Multiplikator *2 am Ende 
dupliziert wird. Es folgt die Definition zweier Fragen in Zeile 14 sowie die Anwendung der 
Pipeline in Zeile 16. Das Ergebnis ist in Bild 6.5 zu sehen und gibt auf die Frage nach der 
Herkunft des Vaters von Obama die korrekte Antwort, nämlich Nyang’oma Kogelo in Kenia. 
Das jedoch mit einem relativ geringen Confidence Score von etwa 0,47 von möglichen 1,0. 
Das Modell ist sich hierbei also nicht allzu sicher.



6.2 Wikipedia 153
Bild 6.4 Der Model Hub von Hugging Face zeigt verfügbare, vortrainierte Modelle für verschiedene 
Sprachen und Aufgaben.
Bild 6.5 Das Ergebnis zeigt, dass für beide Fragen die korrekten Antworten ermittelt werden konnten.
Auch die Frage, wann Obama seinen Vater zuletzt sah, beantwortet das Modell mit als Zwölf jähriger richtig. Dieses Beispiel verdeutlicht sehr schön, warum man neben dem Begriff 
Question Answering auch oft von Text Extraction spricht. Denn die Eigenschaften start und 
end einer Antwort aus Bild 6.5 deuten darauf hin, dass das Modell nichts anderes tut, als 
Text aus einem bestehenden Kontext herauszuschneiden. Es formuliert keine eigenen Texte 
oder optimiert die Antworten in irgendeiner Art und Weise.
Theorie und Training eines eigenen Question-Answering-Modells
Nun schauen wir uns an, was im Hintergrund von Listing 6.9 geschieht. Unsere Ausgangs situation sieht so aus, dass uns eine Sequenz von Wörtern (w1, w2 … wn) vorliegt, aus denen 
wir ein bestimmtes oder mehrerer Wörter extrahieren möchten (siehe Ansatz in Bild 6.6). 



154 6 Intents entwickeln
Als Beispiel soll uns der Satz Peter ist 30 Jahre alt. dienen. Unser Ziel ist, darin die Wortfolge 
30 Jahre zu identifizieren.
w1, w2, w3, w4 .. wn
Context Target
Wort oder Satz (w3, w4)
Peter ist 30 Jahre alt. 30 Jahre
00000000000000000000000 00000000001101111100000
„Peter“ „ist“ „30“ „Jahre“ „alt“ „.“ „30“ „Jahre“
Startvektor:
Endvektor:
00000000001000000000000
00000000000000000100000
Prozess
Beispiel
Tokens
Character Vektor
Ansatz
0 0 0 0 0 0 0 0 1 1 0 0 Token Vektor
10
17
Index des ersten Zeichens der Antwort:
Index des letzten Zeichens der Antwort:
Bild 6.6 Ganzheitlicher Prozess der Textextraktion
Im ersten Verarbeitungsschritt unterteilen wir den vorliegenden Satz in Tokens. Das können 
ganze Wörter oder Wortteile sein. Wenn Sie bereits Tokenizer von NLTK oder SpaCy einge setzt haben, kennen Sie das Prozedere und wundern sich sicherlich, warum auch Wortteile 
ein Token darstellen können. Hier ist es an der Zeit, ein wenig Hintergrund zu vermitteln.
Ein Modell für eine Textextraktion zu trainieren, beginnt in der Regel nicht from scratch, also 
von null an. Vielmehr handelt es sich bei dem Trainingsvorgang um ein Transfer Learning, 
wobei ein bestehendes Modell auf eine ganz bestimmte Aufgabe trainiert wird. Eine Technik 
des maschinellen Lernens für NLP ist BERT, das für Bidirectional Encoder Representations 
from Transformers steht und 2018 von Jacob Devlin et al. bei Google entwickelt wurde (Devlin, 
Chang, Lee, & Toutanova, 2018). BERT hatte so einen großen Einfluss auf die Verarbeitung 
natürlicher Sprache, dass es schon ein Jahr später in Googles Search Engine eingebettet 
wurde. Ein großer Vorteil von BERT ist, dass die Trainingsdaten nicht gelabelt sein müssen 
und somit beliebig viele Texte für das Training verwendet werden können, lediglich limitiert 
durch die Rechenleistung des vorhandenen GPU-Clusters.
Wenn man sich erste Experimente mit BERT anschaut, stößt man häufig auf dessen Fähigkeit, 
fehlende (maskierte) Tokens in Sätzen zu identifizieren. Tatsächlich wurde BERT zunächst 
auf zwei Tasks trainiert:
 Language Modelling
 Next Sentence Prediction
Laut Devlin kann ein generisches BERT-Modell mit weniger Rechenleistung und Trainings daten auf spezielle Tasks trainiert werden, wie etwa hier auf das Question Answering.



6.2 Wikipedia 155
HINWEIS: Liest man sich in die Theorie von Language Models ein, trifft man 
schnell auf den Begriff Bias, eine Bezeichnung für die Voreingenommenheit eines 
Modells. Dieser Bias ist das, was vielen Menschen einen Schrecken einjagt, sind 
Machine-Learning-Modelle doch per-se nicht immer freundlich, geschweige denn 
politisch neutral oder korrekt. Ein einfaches Beispiel ist die Ungleichheit von Ge schlechtern in Texten. Ein simples Beispiel sehen Sie in Tabelle 6.2.
Tabelle 6.2 Bias von bert-base-uncased von (Devlin, Chang, Lee, & Toutanova, 2018)
Maskierte Eingabe Vorhergesagtes Token
She <mask> the ball. dropped
He <mask> the ball. caught
Der Begriff <mask> bezeichnet die Position im Satz, die BERT befüllen soll, und 
zwar mit dem Token, das statistisch am wahrscheinlichsten ist. Dabei nutzt BERT 
seine Fähigkeit, bei der Vorhersage die Tokens links und rechts des fehlenden 
Wortes in Betracht zu ziehen (deshalb die Bezeichnung Bidirectional in dessen 
Namen). Experimentieren Sie mal auf Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichhuggingface.co/bert-base-uncased
mit besagtem Bias, damit Sie ein Gefühl dafür bekommen, wie sich Modelle bei 
verschiedenen Formulierungen verhalten (siehe Bild 6.7).
Bild 6.7 Die Hosted inference API auf huggingface.co erlaubt es, Modelle online auszu probieren, ohne sie lokal aufsetzen zu müssen.
Dass der Bias nicht nur hinsichtlich des Vertrauens fachfremder Leute in der KI, 
sondern auch in der Forschung eine Rolle spielt, habe ich 2020 bei einem Vortrag 
gelernt, in dem ich den Einsatz von Language Models für die Generierung von in teraktiven Text Adventures vorgestellt habe. Ich war auf alle Fragen bestens vorbe reitet und hatte sogar eine Implementierung zur Hand, sodass ich meinen Ansatz



156 6 Intents entwickeln
praktisch hätte zeigen können. Stattdessen habe ich 20 Minuten ein wirklich gu tes Gespräch mit zwei Forschern über die Voreingenommenheit der Modelle und 
die Auswirkung auf den Verlauf der Geschichten geführt, u. a. wer statistisch am 
wahrscheinlichsten stirbt oder welche der Charaktere eine Beziehung eingehen.
In dieser Diskussion und einer längeren Google-Session im Anschluss, habe ich 
erfahren, dass es umfassende Bestrebungen gibt, diesen Bias aus Language Mo dels zu entfernen. Der Ansatz, der aus meiner Sicht am erfolgversprechendsten 
ist, ist, den Bias in den Trainingsdaten zu entfernen, etwa indem Geschlechter 
neutralisiert werden. Das allerdings hat zur Folge, dass die Datenvorverarbeitung 
wesentlich aufwendiger wird. Lassen Sie uns gespannt beobachten, was sich in 
den nächsten Monaten und Jahren hinsichtlich dieser Bestrebungen tut!
Kommen wir nun zurück zu unseren Tokens aus Bild 6.6: Obwohl BERT so trainiert ist, dass 
es ein verhältnismäßig großes Vokabular kennengelernt hat, kann es dennoch vorkommen, 
dass ein Wort in einem Satz auftritt, dass dem Modell bisher unbekannt ist. BERT unterteilt 
diese häufig zusammengesetzten Begriffe in einzelne Tokens, so würde zum Beispiel aus 
dem Wort Amazonasregenwald die zwei Tokens Amazonas und ##regenwald werden. Dieses 
Vorgehen wird auch als Subword Tokenization bezeichnet und eingesetzt, um seltene Wörter 
in bekanntere Wörter aufzubrechen, um diese so stärker im Modell zu repräsentieren. Dass 
eine Trennung der Tokens stattfindet, ist aber für unseren Anwendungsfall kein Problem, 
da wir diese nur für die interne Weiterverarbeitung verwenden. Die Rauten am Beginn eines 
Tokens deuten darauf hin, dass dieser Begriff mit dem vorigen unmittelbar zusammenhing 
und kein Whitespace (zum Beispiel ein Tabulator oder ein Leezeichen) dazwischen zu finden ist.
Im nächsten Schritt wird aus der Tokenliste ein binärer Token Vector erstellt. Ziel ist es, in 
dieser Liste all die Tokens mit einer 1 zu versehen, die für die Antwort der Frage relevant 
sind. Das hieße konkret für unsere Frage nach Peters Alter, dass der binäre Wert an der Stelle 
von 30 und Jahre auf 1 und alle restlichen Positionen auf 0 gesetzt werden. Hier noch mal der 
Hinweis, dass wir uns in der Vorbereitung des Trainings des Question-Answering-Modells 
befinden. Bei der Anwendung hingegen ist dem Modell natürlich nicht vorgegeben, welche 
Tokens für die Antwort relevant ist.
Folgend wird ein Character Vector erstellt, der ebenfalls eine binäre Repräsentation der 
Zeichen enthält, die für die Frage relevant sind. Dieser Vektor dient später dazu, die Start und Endposition des Antworttexts zu identifizieren. Wenn Sie sich den Vektor in Bild 6.6
anschauen, dann werden Sie feststellen, dass zwischen den Einsen auch eine Null steht. 
Diese kennzeichnet ein Leerzeichen.
Bei der Inference setzt sich das Modell das Ziel, die erste 1 des Character-Vektors richtig zu 
ermitteln, um den Beginn der aus dem Kontext zu extrahierenden Antwort zu ermitteln; 
ebenso natürlich die letzte 1 im Character-Vektor für das Ende der Antwort. Bild 6.5 zeigte 
anschaulich, dass ein Question Answering so arbeitet, denn dort sind Start und Ende des 
Antworttexts in der Ausgabe zu sehen. In unserem Beispiel aus Bild 6.6 läge der Start beim 
10. Zeichen des Kontexts und das Ende bei Zeichen Nummer 17.
Lassen Sie uns nun einmal einen Blick in das Transfer Learning werfen, das aus einem her kömmlichen BERT-Modell ein BERT-Modell für Question Answering macht. Das dazugehörige 
Beispiel finden Sie im Ordner 100_extras\100_08_custom_questionanswering. Ich habe mich 



6.2 Wikipedia 157
hier an dem offiziellen Beispiel von Hugging Face3
 orientiert. Werfen wir zuerst einen Blick 
in die main_training.py und schauen uns die Parameter des Trainings in dem Dictionary 
training_args in Listing 6.10 an.
Listing 6.10 Trainingsparameter für das Question Answering
28. training_args = {
29. "n_gpu":1,
30. "model_name_or_path":"bert-base-german-cased",
31. "dataset_name":"deepset/germanquad",
32. "max_seq_length":384,
33. "output_dir":"./models",
34. "per_device_train_batch_size":12,
35. "per_device_eval_batch_size":12,
36. "learning_rate":3e-05,
37. "num_train_epochs":10,
38. "doc_stride":128,
39. "save_steps":5000,
40. "logging_steps":5000,
41. "seed":42
42. }
Wir legen zu Beginn fest, wie viele GPUs uns für das Training zu Verfügung stehen, in 
meinem Fall ist das genau eine. Der Parameter model_name_or_path gibt den Namen des 
Modells an, auf Basis dessen wir das Fine Tuning ausführen wollen. Verwendung findet ein 
deutschsprachiges BERT-Modell. Das cased deutet darauf hin, dass in dessen Training Text 
mit Groß- und Kleinschreibung verwendet wurde, da diese im deutschen Sprachgebrauch ja 
eine essenzielle Rolle spielen. Sie können es sich sparen, die Daten dafür herunterzuladen, 
das macht die API nachher dankenswerterweise für uns. Gleiches gilt für das Dataset, das 
wir über das Argument dataset_name auswählen. Hier sei deepset.ai lobenswert erwähnt, die 
einen deutschsprachigen SQuAD-Datensatz geschaffen haben und frei zum Download anbie ten. Anders als man vielleicht erwartet, erfüllt max_seq_length zwei Zwecke. Einmal legt es 
fest, bis zu welcher Länge ein Trainingssatz aufgefüllt werden muss, und zum anderen, wie 
lang ein Trainingsdatensatz sein darf, bis er abgeschnitten (truncated) wird. Ersteres kennt 
man vielleicht eher aus dem Bereich der Convolutional Neural Networks (CNNs), jedoch kann 
es auch im NLP-Bereich notwendig sein, Sequenzen mit sogenannten Pad-Tokens aufzufüllen, 
um alle Datensätze auf eine einheitliche Länge zu bringen.
HINWEIS: BERT kann in der Regel Sequenzen einer Länge von 512 Zeichen verar beiten, warum verwenden wir hier nur 384? Ganz einfach, trainiere ich das Modell 
mit längeren Sequenzen, läuft mein 12 GB GPU-Speicher über und das Training 
bricht ab. Sollten Sie über mehr Memory verfügen, maximieren Sie die Sequenz länge gerne bis auf 512 Zeichen.
3 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/huggingface/transformers/tree/master/examples/pytorch/question-answering



158 6 Intents entwickeln
Der nächste Parameter output_dir ist wieder denkbar einfach zu verstehen, hier legen wir 
lediglich fest, wo unser Modell gespeichert wird. In den Zeilen 7 und 8 folgt eine eher per formancerelevante Konfiguration von Batches, in denen Training und Evaluierung durchge führt wird. Die learning_rate ist ein elementarer Bestandteil des Trainings eines neuronalen 
Netzes. Sie bezeichnet einen Hyperparameter, der die Ausprägung der Gewichtsanpassung 
nach einer Trainingsiteration festlegt. Stellen Sie sich einfach vor, Sie müssten in einem 
Kaufmannsladen eine Waage austarieren. Eine hohe Lernrate wäre ein sehr schweres Ge wicht. Zwar kommen Sie damit Ihrem Ziel eventuell schnell näher, schießen aber ebenso 
schnell über das gesuchte Ziel hinaus. Wenn Sie hingegen viele kleine Gewichte nehmen, 
um die Waage ins Gleichgewicht zu bringen, müssen Sie voraussichtlich sehr viele Iteratio nen in Kauf nehmen, um Ihr Ziel zu erreichen. Die Anzahl der zu trainierenden Epochen, 
hier abgebildet als num_train_epochs, kennen wir bereits aus den vorherigen Kapiteln. 
Eine Epoche ist vergangen, wenn ein Datensatz einmal vorwärts und rückwärts durch ein 
neuronales Netz gereicht wurde. In der Regel werden immer Blöcke von Datensätzen durch 
das Netz geschickt, die sogenannten Batches. Hier sind in train_batch_size 12 Datensätze 
definiert. Wenn wir schon beim Definieren sind, kommt jetzt vielleicht die Frage auf, was 
in diesem Zusammenhang eine Iteration ist. Eine Iteration ist die Anzahl von Batches, die 
benötigt werden, um eine Epoche in Gänze zu durchlaufen. Wenn wir also insgesamt 2000 
Texte hätten, die wir in unserem Question Answering prozessieren wollen und die somit eine 
Epoche ausmachen und wir dazu pro Batch 500 Texte verarbeiten, dann bräuchten wir vier 
Iterationen. Der nächste Kandidat in der Liste, doc_stride, wird genutzt, um lange Texte zu 
unterteilen und daraus einzelne Features zu schneiden, die dann für das Training herange zogen werden können. Da BERT in der Regel auf eine Länge von 512 Tokens beschränkt ist 
und Trainingsfragen durchaus länger ausfallen können, sind Strides ein sinnvoller Ansatz, 
um diese für das Training zu kürzen. Nun folgen save_steps und logging_steps, die festlegen, 
wann der Status des Trainings geloggt und wann ein Modell zwischengespeichert wird. Zu 
guter Letzt wird noch ein seed für sämtliche Zufallszahlengeneratoren (NumPy, PyTorch, 
Tensorflow …) gesetzt, um Ergebnisse reproduzierbar zu machen. Denn derselbe Seed führt 
zu den gleichen aufeinanderfolgenden Zufallszahlen. Damit haben wir auch sogleich alle 
Stellschrauben kennengelernt, mit denen wir unser Modelltraining beeinflussen können.
Das Dictionary aus Listing 6.10 lesen wir nun mithilfe des HfArgumentParsers ein, laden den 
Trainingsdatensatz und initialisieren unser Basismodell sowie den Tokenizer.
Listing 6.11 Initialisierung von Modell und Tokenizer und Laden des Datensatzes auf Basis der vorher 
definierten Parameter
44. if __name__ == '__main__':
45.
46. logger.info("Parse Parameter...")
47. parser = HfArgumentParser((ModelArguments, DataTrainingArguments,
48. TrainingArguments))
49. model_args, data_args, training_args = parser.parse_dict(training_args)
50.
51. # Setze festen Start für Zufallswerte von NumPy, Torch und Random
52. set_seed(training_args.seed)
53.
54. logger.info("Lade Trainings- und Testdaten...")
55. datasets = load_dataset(data_args.dataset_name, None)



6.2 Wikipedia 159
56. logger.info("Aufbau des Datensatzes: {}.", datasets)
57.
58. logger.info("Erstelle Konfiguration, Tokenizer und Modell für QA...")
59. config = AutoConfig.from_pretrained(model_args.model_name_or_path)
60.
61. tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)
62.
63. model = AutoModelForQuestionAnswering.from_pretrained(
64. model_args.model_name_or_path,
65. from_tf=bool(".ckpt" in model_args.model_name_or_path),
66. config=config
67. )
68.
69. logger.info("Beispielhafter Datensatz: {}", datasets["train"][0])
70. logger.info("Beispiel Tokenisierung: {}",
71. tokenizer(datasets["train"][0]["context"]))
Schauen Sie sich ausgiebig die Ausgaben der Zeilen 41–43 aus Listing 6.11 an. Kenntnis der 
Daten ist oft eine maßgebliche Voraussetzung für das Training, besonders wenn wir den Daten satz verzehrfertig aus dem Hub von Hugging Face laden und ihn nicht vorher aufbereiten. 
Wir sehen hier, dass ein Datensatz eine ID, einen Kontext (also einen Text), eine Frage und 
die darauf lautende Antwort sowie deren Startindex besitzt. Dadurch, dass wir den Kontext 
durch den Tokenizer laufen lassen, ist gut zu sehen, wie dieser die einzelnen Satzteile, die ja 
bekanntlich nicht immer ganze Wörter sein müssen, in Tokens übersetzt. Die attention_mask, 
die der Tokenizer ebenfalls zurückliefert, sagt aus, ob ein String durch Padding künstlich auf 
eine bestimmte Länge gebracht wurde. Die Pad-Tokens sind dann in dieser Liste durch eine 0 
gekennzeichnet. Besteht die attention_mask nur aus Einsen, wird der gesamte Kontext für das 
Training herangezogen. Hinter den token_type_ids versteckt sich ein weiteres, interessantes 
Konzept. In manchen Fällen, zum Beispiel beim Vergleich von Sätzen auf eine gemeinsame Aussage 
oder eben auch bei unserem Question Answering, werden zwei Sätze benötigt, um eine Aus sage treffen zu können. Nun kann es vorkommen, dass die Modellarchitektur es voraussetzt, 
dass beide Sätze zu einem einzelnen zusammengeführt werden müssen. Eine Annotation, die 
man im Internet häufig findet, ist [CLS]Mein erster Satz[SEP]Mein zweiter Satz[SEP]. 
Der Tokenizer versteht die beiden Tokens [CLS] und [SEP] und kann über die Repräsentation 
in token_type_ids eine Trennung der Sätze vornehmen, indem er die Tokens aus Mein erster 
Satz mit einer 0 markiert und die aus Mein zweiter Satz mit einer Eins.
Im nächsten Schritt bereiten wir unsere Trainings- und Testdaten auf, erstellen einen Data 
Collator, der für uns Batches schneidet, und laden die Metriken, die die Güte unseres Mo dells darstellen und dankenswerter Weise für einen SQuAD-Datensatz bereits vorliegen. 
In Listing 6.12 beginnen wir damit, dass wir zuerst die Spaltennamen aus dem Trainings datensatz überprüfen, die für Fragen, Kontext und Antwort verwendet werden sollen. Dann 
bereiten wir ab Zeile 14 respektive Zeile 16 den Trainings- und Testdatensatz vor. Da diese 
Funktionen weitestgehend aus den Beispielen der Transformers-Bibliothek stammen und 
gut dokumentiert sind, werde ich diese hier nicht ausführlich besprechen. Es sei lediglich 
gesagt, dass in der Vorbereitung der Trainingsdaten die Start- und Endindizes der Antwort 
ermittelt werden und geprüft wird, ob die Frage überhaupt zu beantworten ist. Das wäre 
beispielsweise nicht der Fall, wenn keine Antwort gegeben ist oder die gegebene Antwort 
nicht im Kontext enthalten ist, etwa weil der Index über die Länge des Kontexts hinausgeht.



160 6 Intents entwickeln
Listing 6.12 Vorbereiten der Trainings- und Testdaten, der Data Collator und Trainingsmetriken
1. # Auswahl der Spalten für Frage, Kontext und Antwort
2. column_names = datasets["train"].column_names
3. question_column_name = "question" if "question" in column_names else
4. column_names[0]
5. context_column_name = "context" if "context" in column_names else
6. column_names[1]
7. answer_column_name = "answers" if "answers" in column_names else
8. column_names[2]
9.
10. logger.info("Spalte für Fragen '{}', Kontexte '{}' und Antworten '{}'
11. gesetzt.", question_column_name, context_column_name, answer_column_name)
12.
13. logger.info("Bereite Trainingsdaten vor...")
14. train_dataset = datasets["train"].map(
15. prepare_train_features,
16. batched=True,
17. remove_columns=column_names,
18. fn_kwargs=dict(
19. tokenizer=tokenizer,
20. question_column_name=question_column_name,
21. context_column_name=context_column_name,
22. answer_column_name=answer_column_name,
23. max_seq_length=data_args.max_seq_length,
24. doc_stride=data_args.doc_stride)
25. )
26.
27. logger.info("Bereite Testdaten vor...")
28. validation_dataset = datasets["test"].map(
29. prepare_validation_features,
30. batched=True,
31. remove_columns=column_names,
32. fn_kwargs=dict(
33. tokenizer=tokenizer,
34. question_column_name=question_column_name,
35. context_column_name=context_column_name,
36. max_seq_length=data_args.max_seq_length,
37. doc_stride=data_args.doc_stride)
38. )
39.
40. data_collator = default_data_collator
41.
42. logger.info("Lade Metriken für SQuAD2 ...")
43. metric = load_metric("squad_v2")
44.
45. def compute_metrics(p: EvalPrediction):
46. return metric.compute(predictions=p.predictions, references=p.label_ids)
Die Funktion prepare_validation_features() hingegen bereitet eine attention_mask aus 
Frage und Antwort auf (nur die Frage bekommt die Attention, also die Aufmerksamkeit), 
eine token_type_ids Liste, um per Token zwischen Fragen und Antwort unterscheiden zu 
können und ein offset_mapping, dass in Paaren (Tuples) eine Referenz von Token-Id auf die 
entsprechende Position des tatsächlichen Tokens im Kontext abbildet.



6.2 Wikipedia 161
In Zeile 40 wird ein data_collator angelegt, der in der Lage ist, aus dem Trainingsdatensatz 
Batches zu schneiden. Dabei übernimmt dieser die Formatierung, sodass das Format dem 
vom Trainer erwarteten entspricht.
Schließlich laden wir noch die Metrik squad_v2, die es uns erlaubt, das trainierte Modell 
während des Trainings zu evaluieren. Ebenso wie Modelle und Datensätze werden Metriken 
aus einem Hub geladen, weswegen keine expliziten Dateien, sondern nur ein Name über geben wird. Voraussetzung dafür ist natürlich auch, dass während der Entwicklung eine 
Internetverbindung besteht, falls Modelle, Datensätze oder Metriken noch nicht im lokalen 
Cache liegen. Die Funktion compute_metrics()ist nach dem Download in der Lage, die 
Metrik anzuwenden, indem die Vorhersagen p mit den tatsächlichen Labels label_ids ver glichen werden. Einfach ausgedrückt: Es wird geprüft, ob die Testfragen richtig beantwortet 
werden oder nicht.
Nun können wir bereits mit dem Training beginnen, wie in Listing 6.13 zu sehen ist. Hier 
initialisieren wir zu Beginn ein Monstrum namens QuestionAnsweringTrainer, das eine Viel zahl von Argumenten entgegennimmt, aber eigentlich – zumindest oberflächlich – recht 
simpel ist. Wir kommen gleich noch dazu.
Listing 6.13 Beginn und Abschluss des Trainings.
1. logger.info("Initialisiere Training...")
2. trainer = QuestionAnsweringTrainer(
3. model=model,
4. args=training_args,
5. train_dataset=train_dataset,
6. eval_dataset=validation_dataset,
7. eval_examples=datasets["test"],
8. tokenizer=tokenizer,
9. data_collator=data_collator,
10. post_process_function=post_processing_function,
11. compute_metrics=compute_metrics,
12.
13. n_best_size = data_args.n_best_size,
14. max_answer_length = data_args.max_answer_length,
15. null_score_diff_threshold=data_args.null_score_diff_threshold,
16. output_dir=training_args.output_dir,
17. answer_column_name=answer_column_name
18. )
19.
20. last_checkpoint = get_last_checkpoint(training_args.output_dir)
21. if last_checkpoint is not None:
22. checkpoint = last_checkpoint
23. elif os.path.isdir(model_args.model_name_or_path):
24. checkpoint = model_args.model_name_or_path
25. else:
26. checkpoint = None
27. logger.info("Letzter Trainings-Checkpoint: {}", checkpoint)
28.
29. logger.info("Starte Training...")
30. train_result = trainer.train(resume_from_checkpoint=checkpoint)
31.
32. # Speichere das Modell nach dem Training



162 6 Intents entwickeln
33. trainer.save_model()
34.
35. logger.info("Speichere Ergebnis...")
36. output_train_file = os.path.join(training_args.output_dir,
37. "train_results.txt")
38. if trainer.is_world_process_zero():
39. with open(output_train_file, "w") as writer:
40. logger.info("Ergebnis des Trainings:")
41. for key, value in sorted(train_result.metrics.items()):
42. logger.info(f" {key} = {value}")
43. writer.write(f"{key} = {value}\n")
44.
45. trainer.state.save_to_json(os.path.join(training_args.output_dir,
46. "trainer_state.json"))
Nach Initialisierung des QuestionAnsweringTrainer beginnen wir in Zeile 20 damit, den letzten 
Checkpoint des Modells zu ermitteln, falls ein solcher existiert. Bild 6.8 zeigt beispielsweise 
einen Ordner, der wiederum zwei Checkpoint-Ordner aufweist, deren Namen am Ende mit dem 
Durchgang der jeweiligen Iteration versehen sind. Ein Blick in den jeweiligen Ordner zeigt 
unter anderem die Datei trainer_state.json, die in log_history.step den jeweiligen Fortschritt 
des Trainings zeigt. In dem übergeordneten Ordner wird ein ebenso übergeordneter Stand 
des Trainings festgehalten, der in der trainer_state.json die Fortschritte aller untergeordneten 
Checkpoint-Ordner vereint und meistens den aktuellen Stand des trainierten Modells enthält. 
Dieser aktuelle Stand wird für uns automatisiert von get_last_checkpoint() ermittelt. Ist 
kein Checkpoint vorhanden, beginnt das Training from Scratch.
Bild 6.8 Exemplarische Darstellung des Output-Ordners des trainierten Question-Answering-Modells
In Zeile 30 wird das Training gestartet. Der Prozess benötigt bei mir mit einer einzelnen GPU
etwa 1,5 Stunden, bis dann das Modell in Zeile 33 gespeichert wird. Da es hier aber nur um 



6.2 Wikipedia 163
das Modell geht, speichern wir von Zeile 35 bis 46 auch noch den Zustand des Trainers, um 
das Training von diesem Punkt an fortsetzen zu können.
Bevor wir gleich zur Evaluierung kommen, wollen wir uns noch kurz die Klasse QuestionAns weringTrainer anschauen. Diese erbt von transformers.Trainer und implementiert die Funktion 
evaluate(). Diese berechnet eine Metrik zum Fortschritt des Trainings und der Güte des 
Modells. In Bild 6.9 ist zu sehen, welche Aufgaben der Trainer übernimmt. In erster Linine 
geht es mir jedoch in der Grafik darum, zu zeigen, welche Komponenten (gelbe Kästen oben) 
beim Training eines Modells eine Rolle spielen. In einigen Beispielen, die man online findet, 
beinhaltet die Klasse ebenfalls die Methode predict(), die für uns aber hier keine Rolle 
spielt, da wir nicht jedes Mal einen Trainer initialisieren wollen, um das Modell anzuwenden. 
Wir sehen gleich in Listing 6.15, wie wir das Modell für eine Vorhersage nutzen können.
QuesonAnsweringTrainer
Data Collator
Tokenizer Test Data
Validaon Data
Training Data
Hyperparameters Model
Metrics
Training Evaluaon Predicon
Bild 6.9 QuestionAnsweringTrainer als zentrales, koordinierendes Objekt unseres Trainings
Zuvor werfen wir jedoch noch einen Blick in die Evaluation des Modells, die wir über trainer.
evaluate() starten. Im Beispiel auf Github logge ich die Ergebnisse zusätzlich in eine CSV Datei, in Listing 6.14 begnügen wir uns dem Regenwald zuliebe mit der verkürzten Fassung, 
die die Ergebnisse lediglich ins Log schreibt.
Listing 6.14 Evaluation des fertigen Modells
1. # Evaluation
2. logger.info("Evaluiere Modell...")
3. results = {}
4. results = trainer.evaluate()
5.
6. if trainer.is_world_process_zero():
7. logger.info("Evaluationsergebnisse:")
8. for key, value in sorted(results.items()):
9. logger.info(f" {key} = {value}")
10. logger.info("Ergebnis: {}", results)



164 6 Intents entwickeln
In Zeile 6 prüfen wir, ob dieser Prozess der Hauptprozess ist (ein Trainer könnte nämlich 
das Training und die Evaluierung auch auf mehreren Maschinen gleichzeitig ausführen), 
und wenn ja, dann geben wir die Ergebnisse, wie in Bild 6.10 zu sehen, aus. Das Kürzel 
HasAns steht für alle Samples der Evaluierung, die eine Antwort vorweisen. Normalerweise 
würde es noch Metriken für NoAns geben, doch da unser Datensatz vollständig ist und zu 
allen Fragen eine Antwort definiert ist, gibt es diese Metriken nicht. Die Metrik Exact gibt 
in Prozent an, wie oft die Vorhersage der Antwort genau gepasst hat. Mit genau ist gemeint, 
dass der Satz bis auf einige Stopwords und Satzzeichen übereingestimmt hat. Der F1-Score 
ist sicher vielen bekannt, die sich schon mal mit Statistik beschäftigt haben. Hier misst die ser die durchschnittliche Überlappung der vorgegebenen richtigen Antwort (auch Ground 
Truth genannt) und der vorhergesagten Antwort (Rajpurkar, Zhang, Lopyrev, & Liang, 2016).
Bild 6.10 Ergebnis der Evaluation des Modells
Listing 6.15 zeigt, wie wir das Model anwenden und sollte Ihnen bekannt vorkommen. Tat sächlich unterscheidet sich dieses inhaltlich kaum von Listing 6.9. Die Besonderheit liegt 
darin, dass in den Zeilen 8 und 9 nun unser eigenes Modell geladen wird, nicht mehr das, 
das die API online herunterlädt.
Listing 6.15 Anwendung des Modells über transformers.pipeline
1. from transformers import pipeline
2. from multiprocessing import freeze_support
3.
4. if __name__ == '__main__':
5. freeze_support()
6. qa_pipeline = pipeline(
7. "question-answering",
8. model="./models",
9. tokenizer="./models"
10. )
11.
12. contexts = ['''Obamas Vater, Barack Hussein Obama Senior (1936–1982), stammte
13. aus Nyang’oma Kogelo in Kenia und gehörte der Ethnie der Luo an. Obamas
14. Mutter, Stanley Ann Dunham (1942–1995), stammte aus Wichita im US-
15. Bundesstaat Kansas und hatte irische, britische, deutsche und Schweizer
16. Vorfahren. Obamas Eltern lernten sich als Studenten an der University of
17. Hawaii at Manoa kennen. Sie heirateten 1961 in Hawaii, als Ann bereits
18. schwanger war. Damals waren in anderen Teilen der USA Ehen zwischen
19. Schwarzen und Weißen noch verboten. 1964 ließ sich das Paar scheiden. Der



6.2 Wikipedia 165
20. Vater setzte sein Studium an der Harvard University fort. Obama sah ihn
21. als Zehnjähriger zum letzten Mal.''']*2
22.
23. questions = ["Woher kommt Obamas Vater?",
24. "Wann sah Obama seinen Vater zum letzten Mal?"]
25.
26. print(qa_pipeline(context=contexts, question=questions))
Damit haben wir unser erstes, eigenes Modell trainiert. Basismodell und Trainingsdaten 
waren zwar bereits vorbereitet, aber die erbrachte Transferleistung befähigt uns nun, auf 
eigenen Daten zu trainieren und vor allem besser zu verstehen, wie die Intelligenz hinter 
dem Prozess aussieht.
Prima, nun können wir Fragen zu beliebigen, deutschsprachigen Texten beantworten. Woher 
aber bekommen wir die Texte? Nehmen wir mal an, wir verwenden Wikipedia bequemerweise 
wieder als Quelle. Dann stellt sich aber die nächste Frage: Wie ermitteln wir das Thema, zu 
dem ein Artikel herausgesucht werden soll?
Beschaffung des Kontexts über Named Entity Recognition
Ziel dieses Abschnitts ist es, aus einer Frage das themenbestimmende Objekt zu extrahieren. 
Aus der Frage Wie alt ist Obama? Würde sich beispielsweise Obama als Thema ergeben, oder 
Arc de Triomphe aus der Frage Wo steht der Arc de Triomphe?
Für diese Aufgabe ist die klassische NER (Named Entity Recognition) prädestiniert, eine 
Disziplin aus der Domäne der Informationsextraktion, die dafür da ist, Begriffe aus unstruk turierten Texten einer oder mehrerer vordefinierter Kategorien zuzuweisen. Das passende 
Beispiel finden Sie unter 100_extras\100_10_ner_tf-idf\main_net.py. Dort nutzen wir erneut 
SpaCy, um die Entitäten aus einem vorgegebenen Satz extrahieren zu lassen.
Listing 6.16 Namend Entity Recognition am Beispiel
1. import spacy
2. from spacy import displacy
3.
4. nlp = spacy.load('de_core_news_sm')
5. doc = nlp("Wann hat Barack Obama Geburtstag?")
6.
7. print(nlp.get_pipe('ner').labels)
8.
9. for ent in doc.ents:
10. print(ent.text, ent.label_)
11.
12. displacy.serve(doc, style="ent")
Das Ergebnis manifestiert sich einerseits in einer Log-Ausgabe Barack Obama PER sowie 
der optionalen Visualisierung über displacy, die in Bild 6.11 zeigt, wie die detektierte Entität 
hervorgehoben und mit der Klasse PER versehen wird. Displacy erreichen Sie, wenn Sie in 
Ihrem Browser die Adresse Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichlocalhost:5000 aufrufen. Dahinter verbirgt sich ein kleiner 
Webserver, der sich wieder beendet, wenn Sie die Python-Anwendung mit STRG+C beenden.



166 6 Intents entwickeln
Bild 6.11 Visualisierung der erkannten Entitäten über displacy
Neben der Klasse PER existieren noch weitere Label, die nicht global in SpaCy definiert sind, 
sondern abhängig vom geladenen Modell (hier de_core_news_sm) sind. Diese Label sehen 
Sie über den Aufruf nlp.get_pipe('ner').labels:
 LOC (Ort)
 MISC (Verschiedene)
 ORG (Organisation)
 PER (Person)
Das Ergebnis ist in unserem Fall absolut zufriedenstellend, denn mit der gefundenen Enti tät konnten wir genau das Thema extrahieren, das wir für ein Nachschlagen auf Wikipedia
benötigen.
NER funktioniert allerdings nur für Fragen nach Entitäten, für die wir einen Kontext haben. 
Bei logischen Entscheidungen hingegen, ist sie weniger hilfreich. Ein Beispiel:
Justus ist 5 Jahre alt, Elisa ist 2 Jahre alt. Wer ist älter?
Der Kontext ist hier im ersten Satz enthalten, kann aber nicht in Form eines signifikanten 
Schlagworts per NER extrahiert werden. Tatsächlich begeben wir uns hier erneut in die Do mäne des Reasoning, also in den Aufgabenbereich, Schlussfolgerungen aus Informationen 
aus einer vorliegenden Situation zu ziehen. Diese Definition sollte uns wieder an das Question 
Answering aus Abschnitt 6.2.1 erinnern und tatsächlich, wenn wir dieselbe Frage an ein 
Enterprise-Modell (hier gelectra-base-germanquad von deepset.ai) weiterleiten, erhalten wir 
die passende Antwort (siehe Bild 6.12).
Bild 6.12 Reasoning für logische Fragen auf Basis eines Kontexts
Und damit stehen wir vor einem Problem, mit dem sich die Forschung in der KI ganz aktuell 
beschäftigt, um dem Ziel einer allgemeinen KI näher zu kommen: Welches Modell oder welche 
Methode verwende ich für welche Aufgabe? Wann müssen mehrere Modelle oder Methoden 
in Reihe geschaltet werden, wie wird das Ergebnis abschließend bewertet, und woher weiß 
die KI, wann sie aufhören kann, nach einem besseren Weg zu suchen, um die Aufgabe zu 
lösen? Statt diese Fragen zu beantworten, wollen wir uns noch schnell anschauen, wie wir 
generative Modelle nutzen können, um Fragen zu beantworten. Danach widmen wir uns 
dann dem nächsten Intent.



6.2 Wikipedia 167
Andere Möglichkeiten des Question Answerings ohne Kontext
Haben Sie immer noch Lust zu lernen, wie eine KI Fragen zu beliebigen Themen beantwor ten kann? Egal ob Sie nun mit ja oder nein antworten, ein Buch ist ein absolut asynchrones 
Medium und Ihnen bleibt gar keine Wahl. Ich fasse mich aber kurz, versprochen. Haben Sie 
von dem GPT-{2,3}-Modell gehört? Anders als unser bisheriges Question-Answering-Modell 
ist GPT (Generative Pre-Trained Transformer) darauf ausgelegt, Texte zu generieren bzw. vor gegebene Textanfänge fortzuschreiben. Bild 6.13 zeigt am Beispiel, wie GPT-2 den Satz L’Arc 
de Triomphe stands in vervollständigen würde. Statt nun jedoch Paris vorzuschlagen, wird 
eine gänzlich andere Aussage getroffen, nämlich dass dieser im Schatten der Kathedrale 
von Notre Dame stehe (Sie sehen, das Modell wurde wahrscheinlich vor dem Brand im April 
2019 trainiert).
Bild 6.13 Write With Transformer erlaubt das Ausprobieren von GPT-2 und anderen generativen Model len im Netz und vervollständigt einmal angefangene Sätze mit den statistisch wahrscheinlichsten Worten.
Die Funktionalität lokal zu implementieren, kommt Ihnen sicher nun schon fast trivial vor, 
ist der Arbeitsablauf doch immer derselbe. Wir erstellen eine Pipeline, laden ein vortrai niertes Modell, entweder lokal oder wie hier wieder aus dem Model Hub von Hugging Face, 
und wenden dieses auf einen Satzanfang an. Das Modell german-gpt2 wird übrigens von der 
bayerischen Staatsbibliothek bereitgestellt, der ich für die Arbeit sehr dankbar bin.
Listing 6.17 Vervollständigen von Texten mithilfe von GPT-2
1. from transformers import pipeline
2. text2text_generator = pipeline(
3. "text-generation",
4. model="dbmdz/german-gpt2",
5. tokenizer="dbmdz/german-gpt2"
6. )
7. print(text2text_generator("Der L'Arc de Triomphe steht in"))
Die Antwort, die das Modell gibt, sieht so aus:
Der L’Arc de Triomphe steht in Verbindung mit der „Rue des Gêneries“. Er befindet sich am süd lichen Ende von Paris-Nord und wird von der Rue Lamarck, der Straße "Rue des Rosiers" und
Zugegeben, ein etwas abruptes Ende, aber nichts, was sich mit ein bisschen Textschneiderei 
zu einem vernünftigen Satz kürzen ließe. Das Problem, aus einer Frage einen Satzbeginn zu 
formen, bleibt jedoch bestehen. Allerdings könnte ich mir auch vorstellen, dass wir, ähnlich 
Jeopardy, lernen können, Fragen an den Sprachassistenten anders zu richten, als wir es bis her tun. Zum Beispiel indem wir sagen: Bumblebee Frage: Obamas Alter ist …



168 6 Intents entwickeln
Wie immer überlasse ich die Entscheidung Ihnen, ob Sie nun die recht starre Methode 
verwenden, die nur Fragen wie Was ist? Wer ist? zulässt, das Question Answering, in dem 
Sie einen Kontext beschaffen müssen, oder ob Sie das generative Modell nutzen, das ohne 
Internetanbindung klarkommt, aber dessen Antworten manchmal eher kreativ als faktisch 
korrekt sind.
6.2.2 Exkurs: Transformer-Modelle und Self-Attention
Falls ich Sie mit der Einführung in die Sequence-To-Sequence-Modelle aus Kapitel 3 direkt 
hierhergetrieben habe, freue ich mich. Wenn Ihnen der Teil jedoch damals schon zu theoretisch 
war, können Sie den Abschnitt hier auch gerne überspringen und auf einer längeren Bahnfahrt 
oder im Wartezimmer darauf zurückkommen. Als Exkurs ist er optional. Wir werden uns 
auch nur einen Überblick über die Kernkomponenten und deren Funktionsweise verschaffen 
und nicht die Architektur bis ins letzte Detail durchsprechen. Für einen vollumfänglichen 
Einblick empfehle ich das Buch Natural Language Processing with Transformers (Tunstall, von 
Werra, & Wolf, 2022) oder die hervorragende Seite Transformers from Scratch (Rohrer, 2021).
Wir waren dabei stehen geblieben, dass RNNs nicht parallelisiert trainiert werden können 
und der Trainingsprozess deswegen bei größeren Datenmengen schleichend langsam ist4
. 
Besonders, wenn man daran denkt, dass für aktuelle Sprachmodelle wie GPT-3 etwa 570 
Gigabyte Trainingsdaten Verwendung finden, wird man sich der Notwendigkeit bewusst, 
den Prozess, so gut es geht, auf verschiedene, zur selben Zeit rechnende GPUs zu verteilen.
In 2017, zwanzig Jahre nach dem Paper zu LSTMs, stellten Vaswani et al. in ihrem Paper 
Attention is All You Need die Transformer-Architektur vor (Vaswani, et al., 2017). Wie der Name 
vermuten lässt, transformiert ein solcher gleich dem bereits betrachteten Seq2Seq-Modell 
eine Datensequenz in eine andere und besteht weiterhin aus Encodern und Decodern – mit 
dem Unterschied, dass keine RNNs mehr darin Verwendung finden.
Für einen Überblick über die Modell-Architektur verwende ich die Grafik aus dem Original Paper in Bild 6.14, da diese durch ihre Omnipräsenz im Internet an vielen Stellen erklärt 
wird und Sie zu jedem einzelnen Baustein Detailinformationen finden, falls Sie hier und da 
etwas tiefer in die Materie eintauchen möchten. Als Beispiel für unsere Erklärung soll weiter hin die Textübersetzung dienen, die wir ja auch schon bei den RNNs herangezogen haben.
Wir orientieren uns am Beispiel einer Textübersetzung, die ja bereits in Abschnitt 3.3.2 bei der 
Einführung in Sequence-To-Sequence-Modelle hergehalten hat, konkret an der Übersetzung 
Ich lese gerne ins Englische I like to read. Wir werden nun in den nächsten Schritten zuerst 
den Encoder anschauen, dessen Aufgabe es ist, eine Eingabesequenz bestehend aus Tokens 
in eine Sequenz von Embeddingvektoren zu konvertieren. Wie auch beim RNN bezeichnen 
wir diese auch als Hidden State oder Context. Der Decoder nimmt, abermals vergleichbar mit 
einem RRN, diesen Kontext entgegen und generiert in mehreren Iterationen eine Ausgabe sequenz, die am Ende per Klassifikation in für uns lesbare Tokens umgewandelt wird. Dabei 
wird dem Encoder die Eingabesequenz nur einmal übergeben, der Decoder hingegen übergibt 
sich seinen Output immer wieder als Input, bis er ein End-Of-Sequence-Token (EOS) erreicht.
4 Tatsächlich werden wir später noch sehen, dass sich LSTMs auch in Sekunden trainieren lassen, wenn die zugrunde 
liegenden Daten nicht allzu umfangreich sind. Es ist also nicht so, dass RNNs völlig veraltet sind und gar keine 
Verwendung mehr finden. Eher gilt das Gegenteil, denn sie sind noch sehr präsent in Forschung und Industrie.



6.2 Wikipedia 169
Bild 6.14 Die Architektur des Transformer-Modells besteht aus Encoder (links) und Decoder (rechts), 
verzichtet auf RNNs und ist in der Lage, Eingabedaten nicht sequenziell, sondern parallel zu verarbei ten (Vaswani, et al., 2017). In dieser Abbildung wurden gegenüber dem Original lediglich die Residual 
Connections (RC) ergänzt.
6.2.2.1 Der Encoder
Wir beginnen, indem wir uns den Encoder auf der linken Seite zuerst anschauen. Zu Beginn 
erzeugen wir ein Embedding, das wir bereits in Kapitel 3 kennengelernt haben. Dort reprä-
sentiert ein Embedding eine Stimme in Form einer Reihe von numerischen Werten. Was in 
Kapitel 3 galt, nämlich dass neuronale Netze nicht direkt mit Audiodateien umgehen können, 
gilt auch für Texte. Der Transformer erwartet, dass Texte zuerst in ein Embedding überführt 
werden. Der erste Schritt dorthin ist, sie zu tokenisieren. Eine solche Tokenisierung kann 
auf Buchstaben-, Wort- oder Sub-Wort-Ebene erfolgen. Wir bleiben bei unserem Beispiel bei 
der Word Tokenization, da diese am anschaulichsten ist. Der nächste Schritt ist nun, jedem 
Token eine eindeutige ID zuzuordnen, so wie wir es schon praktisch in Abschnitt 6.2.1 erprobt 
haben. Mit dieser Art Daten kann unser Computer nun etwas anfangen, sodass wir darauf folgend ein Embedding erzeugen können. Ziel dieses Embedding ist es, die Wörter auf einer 
Art Landkarte anzuordnen, auf der ähnliche Wörter nahe beieinanderliegen (siehe Bild 6.15).



170 6 Intents entwickeln
Hund
Katze
Maus
Blau
Lila
Bus
Auto
Haus
Schreiner
Tischler
Phänomen
Flugzeug
Bild 6.15 In einem Embedding Space sind inhaltlich ähnliche Wörter räumlich nah zueinander an geordnet.
Transformer haben Word Embeddings allerdings keinesfalls eingeführt, diese gibt es schon 
sehr lange, ebenso wie vortrainierte Modelle, um Embeddings zu erzeugen. Lassen Sie uns 
mal einen kurzen Blick auf das Beispiel in 100_extras/100_16_embeddings werfen. In Lis ting 6.18 möchte ich Ihnen zeigen, wie wir eine numerische Repräsentation eines Wortes 
(hier vector genannt) abrufen. Wir laden zuerst das vortrainierte Modell de_core_news_md
in Zeile 3, wählen einen Beispielsatz und können dann über doc auf die einzelnen Wörter 
in diesem Satz zugreifen (Zeilen 7 und 8). Word Embeddings sind übrigens nicht in den klei nen, vortrainierten Modellen von SpaCy (mit der Endung sm) sinnvoll befüllt, aber dennoch 
abrufbar. Erst ab md sind die hinterlegten Werte hinter der Eigenschaft vector brauchbar.
Listing 6.18 Abrufen von Embeddings über Spacy und Gensims word2vec
1. import spacy
2. # python -m spacy download de_core_news_md
3. nlp = spacy.load('de_core_news_md')
4. doc = nlp("Der Affe suchte ein schattiges Plätzchen, um seinen Keks zu essen.")
5. #doc = nlp("Ich backe leckere Sachen wie Plätzchen oder einen schokoladigen
6. Keks.")
7. plaetzchen = doc[5]
8. keks = doc[9]
9. print("Spacy - Plätchen-Embedding: ", plaetzchen.vector)
10. p_sim = plaetzchen.similarity(keks)
11. print("Spacy - Ähnlichkeit der Begriffe Plätzchen und Keks: ", p_sim)
12.
13. from gensim.models import Word2Vec, KeyedVectors
14. # Model von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichdevmount.github.io/GermanWordEmbeddings/ herunterladen
15. model = KeyedVectors.load_word2vec_format("german.model", binary=True)
16. auto = model['Auto']
17. print("Gensim word2vec - Auto-Embedding: ", auto)
18. auto_synonyms = model.most_similar('Auto', topn=10)
19. print("Gensim word2vec - Synonyme zu 'Auto' sind: ", auto_synonyms)
Das Embedding der einzelnen Begriffe ist nun über die Eigenschaft .vector abrufbar. Wenn Sie 
das Embedding nun ausgeben lassen, sehen Sie einfach nur einen sehr langen NumPy-Array.



6.2 Wikipedia 171
HINWEIS: Vector und Embedding sind laut SpaCy-Dokumentation5
 Synonyme. 
Da der Begriff Vector in Python inflationär Verwendung findet, bleibe ich bei der 
Bezeichnung Embedding.
In Zeile 10 sehen wir dann aber gleich, was wir mit einem Embedding anfangen können, 
nämlich die Ähnlichkeit eines Begriffs mit einem anderen zu prüfen. Bild 6.16 zeigt die Aus gabe des Vergleichs zwischen dem Begriff Plätzchen und Keks aus Zeile 10 und berechnet 
eine Ähnlichkeit von ~56 %.
Bild 6.16 Ausgabe der Vergleiche bzw. des Abrufs ähnlicher Begriffe über Embeddings
TIPP: Die Berechnung der Ähnlichkeit zweier Begriffe funktioniert in SpaCy über die 
Kosinusähnlichkeit, wobei der Kosinus des Winkels zwischen zwei Vektoren bestimmt 
wird (siehe Bild 6.17).
x
y
θ1
θ2
Hase
Kaninchen
Weer
Bild 6.17 Die Kosinusähnlichkeit zum Winkel q1 zwischen den Vektoren der Begriffe 
Hase und Kaninchen ist höher, da der Winkel kleiner ist. Die Ähnlichkeit zwischen den 
Begriffen Hase und Wetter hingegen ist geringer, da der Winkel q2 zwischen den reprä­
sentierenden Vektoren größer ist.
Liegen zwei ein Wort repräsentierende Vektoren nahe beieinander, so ist der Win kel gering, im Optimalfall 0 Grad. Das führt zu einer Ähnlichkeit von 1. Beträgt der 
Winkel 180 Grad, ergibt sich eine Ähnlichkeit von –1, d. h. die Begriffe sind im Em bedding Space so entgegengesetzt angeordnet, wie nur möglich. Die Formel zur 
Berechnung sähe in Python wie folgt aus, was auch der Implementierung in SpaCy
entspricht.
from numpy import dot
from numpy.linalg import norm
cos_sim = dot(a, b)/(norm(a)*norm(b))
Das Skalarprodukt aus den beiden Vektoren a und b wird durch das Produkt bei der Vektorbeträge geteilt.
5 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichspacy.io/usage/linguistic-features#vectors-similarity



172 6 Intents entwickeln
Begibt man sich in Python auf die Suche nach Embeddings, stößt man auch schnell auf die 
Bibliothek Gensim (Generate Similar), die für semantische Textanalysen geschrieben wurde 
und Fähigkeiten besitzt, wie Ähnlichkeiten von Dokumenten oder Begriffen zu ermitteln 
oder Themen aus Dokumenten zu extrahieren.
In Zeile 15 von Listing 6.18 laden wir ein freundlicherweise von Andreas Müller trainiertes 
und bereitgestelltes Word2Vec-Modell6
, prüfen die Ausgabe eines Embeddings für den Begriff 
Auto und lassen uns die zehn ähnlichsten Begriffe ausgeben. Diese sehen Sie in Bild 6.17 mit 
den entsprechenden Wahrscheinlichkeiten. Die drei ähnlichsten Begriffe sind:
 Motorrad
 Wagen
 Fahrzeug
Es ist also mit vorhandenen Mitteln recht einfach, den Embedding Space aus Bild 6.15 zu er zeugen. Kommentieren Sie nun, wie in Listing 6.18, Zeile 4 aus und Zeile 5 ein, und führen 
Sie die Anwendung erneut aus. Sie werden sehen, dass der Kontext von Plätzchen ein ganz 
anderer ist. Wir haben hier also eine Mehrdeutigkeit, mit der wir irgendwie umgehen müs sen. Dummerweise ist die Ähnlichkeit, die SpaCy liefert, noch genau die gleiche, egal ob mit 
Plätzchen ein Sitzplatz oder Gebäck gemeint ist. Hier kommen wir zum nächsten Baustein 
der Architektur, dem Positional Encoding.
Positional Encoding
Kommen wir erneut darauf zurück, warum wir keine RNNs einsetzen. Sie waren zu lang sam, weil sie Daten nur sequenziell verarbeiten konnten. Eine Sequenz hat aber einen ent scheidenden Vorteil: Ich weiß, welches Wort im Satz an welcher Stelle steht. Den Vorteil der 
Parallelisierbarkeit haben wir uns also teuer erkauft und als Ersatz muss eine andere Methode 
her. Das Positional Encoding (auch als Positional Embedding bezeichnet) springt hier ein und 
versieht unsere einfachen Token Embeddings mit Informationen über deren Position im Satz. 
Aus diesem neuen Embedding-Vektor lernen die zwei nachfolgenden Komponenten (Atten tion Heads- und Feed Forward-Schichten), diese Position und die Reihenfolge der einzelnen 
Tokens in den Gesamtkontext mit einzubeziehen. Denken wir daran, welche Bedeutung die 
Wortreihenfolge für die Grammatik und die Semantik eines Satzes hat, ist das auch absolut 
sinnvoll. Alleine die Position eines Kommas kann die Aussage eines ganzen Satzes komplett 
verändern. Ein Beispiel:
Computer arbeitet, nicht neu starten!
Computer arbeitet nicht, neu starten!
Es liegt nahe, für das Speichern der Wortpositionen einfach einen fortlaufenden Index als Posi tionsinformation für jedes Wort zu generieren, also das erste Wort bekommt eine 0 zugeordnet, 
das zweite eine 1 und so weiter Das kann allerdings dazu führen, dass sehr große Zahlen verwendet 
werden, sollte der Trainingstext lang sein. Und was wäre, wenn ein Eingabetext länger ist 
als jeder beim Training verwendete Satz? So würde das Modell Indizes über der angelernten 
Länge nicht kennen. Wie wäre es also, die Positionsinformationen zu normalisieren? Eine 
Normalisierung auf einen Wert zwischen 0.0 und 1.0 kann ebenfalls bei langen Sequenzen 
zu unterschiedlichen Abständen zwischen den einzelnen Positionsinformationen führen, 
6 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichdevmount.github.io/GermanWordEmbeddings



6.2 Wikipedia 173
sodass man zum Beispiel nicht mehr bestimmen könnte, wie viele Wörter zwischen Abschnitt 0.1 
und 0.7 liegen würden. Eine andere Methode muss also her. Die Autoren von Attention is All 
You Need schlagen vor, die Positionen anhand von trigonometrischen Funktionen, konkret 
Sinus und Kosinus, zu encodieren, wie es in Formel 6.1 und Formel 6.2 zu sehen ist.
( )
    =  
   
pos, 2 2
pos PE sin
10 000
i i
d
Formel 6.1
( + )
    =  
   
pos, 2 1 2
pos PE cos
10 000
i i
d
Formel 6.2
Darin ist pos die Position des jeweiligen Wortes bzw. Tokens in der Eingabesequenz, für das wir 
das Positions-Embedding erzeugen möchten, d die Dimension unserer Embedding-Vektoren 
und i der Zähler, der über jede Dimension eines solchen Vektors iteriert (siehe Bild 6.18). Die 
Besonderheit ist, dass die Methode, wenn pos gerade ist, die Formel 6.1 anwendet und falls 
nicht, auf Formel 6.2 zurückgreift. Für Embedding 0 und 2 in der Abbildung würde also die 
Sinusfunktion verwendet werden, für Embedding 1 die Kosinusfunktion.
0,83
0,02
0,05
0,1
Ich
0,22
0,42
0,01
0,35
lese
0,06
0,42
0,11
0,41
d = 4 +
i = 0
i = 1
i = 2
i = 3
pos = 0 pos = 1 pos = 2
0,83
0,2
0,05
0,1
0,22
0,42
0,01
0,35
0,06
0,42
0,11
0,41
0,13
–0,6
0,33
0,16
0,03
0,22
0,58
0,61
0,7
0,02
0,58
0,23
gerne
+ +
sin() cos() sin()
0,96
-0,4
0,38
0,26
0,25
0,64
0,59
0,96
0,76
0,44
0,69
0,64
pe0 pe1 pe2
Bild 6.18 Ein erzeugtes Embedding mit den einzelnen Werten d, i und pos, die für die Erzeugung 
des Positional Encodings notwendig sind (links). Einmal berechnet, wird die Positionsinformation 
(Mitte) zu den Embeddings hinzuaddiert. Gerade Positionsindizes (hier 0 und 2) werden über die 
Sinusfunktion aus Formel 6.1 und ungerade Indizes (hier 1) über die Kosinusfunktion aus Formel 6.2
berechnet. Rechts sind die berechneten Positional Encodings zu sehen.
TIPP: Die Dimensionen eines Embeddings werden häufig auch als Linguistic 
Feature bezeichnet. Anders als ein Feature aus dem klassischen Machine 
Learning, haben diese eine uns unbekannte Aussage. Das Feature sagt also nicht 
unbedingt einen Beruf, einen Namen, eine Jahreszeit oder eine andere, uns zu gängliche Eigenschaft aus.



174 6 Intents entwickeln
Würden wir uns nun die Mühe machen und anhand von erdachten Embeddings die ersten 
vier Positional Encodings ausrechnen, würden wir feststellen, dass sich durch den Nenner 
im Bruch unserer beiden Formeln mit ansteigendem i auch die Frequenz der Kurven ändert. 
So erhalten wir auch wirklich einzigartige Positionsinformationen. Würde die Frequenz der 
Kurven nämlich gleich bleiben, liefen wir Gefahr, selbst bei kurzen Sätzen dieselben Posi tionsinformationen zweier Wörter zu berechnen, selbst wenn diese weit auseinanderlägen. 
Auch wenn dieser Ansatz wesentlich weniger greifbar ist, als die Position der Wörter ein fach mit einer fortlaufenden Nummer zu indizieren, schlagen wir doch damit zwei Fliegen 
mit einer Klappe: Die Positionsinformationen steigen nicht mit wachsendem Index an, und 
sie sind, unabhängig von der Länge der Eingabesequenz, an jeder Position gleich. Mit der 
Addition der jeweiligen Word Embeddings und der darauf berechneten Positional Encodings 
ist dieser Schritt abgeschlossen.
Multi-Head Attention
Den Begriff Attention haben wir ebenfalls bereits in Kapitel 3 kennengelernt. In diesem 
Abschnitt wollen wir uns ganz konkret mit den Begriffen Self-Attention und Multi-Head 
Attention auseinandersetzen. Ganz gemäß des Prinzips First Come, First Served beginnen wir 
mit der Self-Attention. Wenn Ihr erster Eindruck ist, als käme der Begriff aus dem Milieu der 
Influencer oder er wäre der neuste Trend auf TikTok, liegen Sie vielleicht gar nicht so falsch. 
Es geht nämlich darum, nicht anderen Aufmerksamkeit zu schenken, sondern sich selbst. 
Erinnern Sie sich, dass Attention damals im Decoder der RNNs Anwendung fand, als den 
verschiedenen Hidden States selektiv Aufmerksamkeit geschenkt wurde? Es wurde also auf 
einen anderen Satz geblickt, einen anderen Kontext. Self-Attention richtet die Aufmerksam keit nicht nach außen, sondern auf denselben Satz und gewichtet die Beziehung zwischen 
dessen Wörtern. Kommen wir auf das Beispiel aus Listing 6.18 zurück, das ich in Bild 6.19
visuell aufbereitet habe. Es ähnelt ein wenig dem Dependency Parsing aus Kapitel 4, nicht 
wahr? Allerdings macht die Self-Attention keine semantischen Unterschiede und berechnet 
die Beziehung jedes Wortes, unabhängig von der dem Satz zugrunde liegenden Grammatik.
Der Affe suchte ein schages Plätzchen, um sein schokoladiges Plätzchen zu essen.
Bild 6.19 Self-Attention bestimmt die gewichteten Beziehungen zwischen Wörtern eines Satzes und 
schafft es so, zwischen doppeldeutigen Begriffen zu unterscheiden.
Das Beispiel zeigt eine klassische Doppeldeutigkeit des Begriffs Plätzchen, die aber durch 
den Kontext recht schnell aufgelöst werden kann. Wie aber identifizieren wir diesen Kontext 
ohne einen regelbasieren Ansatz, der sich über die Bedeutung, Art und grammatikalische 



6.2 Wikipedia 175
Funktion im Klaren ist? Machen wir uns gedanklich auf den Weg zur Multi-Head Attention, 
wozu wir uns den gleichnamigen Baustein aus unserem Architekturschaubild in Bild 6.20
im Detail ansehen.
Linear Q Linear K Linear V
MatMul
Scale
Somax
MatMul
Concat
Linear
Query Key Value
pe0 pe1 pe2 pe0 pe1 pe2 pe0 pe1 pe2 pe0,0 pe0,1 pe0,2 pe0,3
Neuron 1 Neuron 2 Neuron 3
pe0‘ pe1‘ pe2‘
1x3
1x4 Dimensionsredukon
pe0
T=
Bild 6.20 Die Komponenten der „Multi-Head Attention“-Schicht (links) beginnen mit drei linearen 
Schichten, die jeweils unsere Positional Embeddings entgegennehmen, einmal als Query, einmal 
unter der Bezeichnung Key und einmal als Value. Rechts ist ein Linear Layer zu sehen, der eine Di mensionsreduktion der Embeddings von vier Inputs auf drei Outputs bewirkt – wohlgemerkt nur für 
dieses eine Embedding.
Wir finden darin ganz unten drei Linear Layers (lineare Schichten), die dazu dienen, die 
Eingabe-Embeddings mit der Dimension 4 auf eine Ausgabe der Dimension 3 zu reduzieren. 
Dazu ziehen wir als Beispiel das Positional Embedding pe0 heran und transponieren es zu 
pe0
T
, wie es in der Abbildung durch den schwarzen Pfeil unten rechts angedeutet wird.
HINWEIS: Ein hochgestelltes T symbolisiert das Transponieren einer Matrix oder 
eines Vektors. Dabei werden Spalten zu Zeilen und Zeilen zu Spalten. Aus einer 
Matrix mit fünf Zeilen und einer Spalte wird also eine Matrix mit fünf Spalten und 
einer Zeile (siehe Bild 6.21).
1 7
2 8
3 9
1 2 3
7 8 9
A = AT =
Bild 6.21 Beim Transponieren der Matrix A, wird die erste Zeile von A zur ersten Spalte 
von AT
. Gleiches gilt für die zweite und alle folgenden Zeilen.



176 6 Intents entwickeln
Jede der vier Dimensionen wird nun in jedes der drei Neuronen eingespeist und jeder Pfad 
von Dimension x zu Neuron y bekommt ein eigenes Gewicht, das via Back Propagation (siehe 
Abschnitt 3.3.6) optimiert wird. Die Reduktion der Dimensionen von vier auf drei passiert auto matisch durch das Setzen der drei Neuronen als Ausgabe. Ein guter Grund, die Dimensionen 
zu reduzieren, ist, die Berechnungs- respektive die Trainingszeit des Netzes zu reduzieren. 
Das Resultat ist eine Matrix, die die Gewichte nach dem Training enthält.
Nun haben wir aber drei Linear Layers, warum das? Jede dieser Schichten hat eine eigene 
Funktion, die durch die Begriffe Query (Q), Key (K) und Value (V) dargestellt werden. Ich habe 
in den letzten Jahren viele verschiedene Versuche gehört und gelesen, die diese Begriffe 
entweder mathematisch oder via Analogie zu erklären versuchen. Für mich war am Ende 
Folgendes am verständlichsten:
Stellen Sie sich das Szenario einer Suche vor, in der ein Benutzer den Begriff Transformers
in das Suchfeld eintippt. Dieser Suchbegriff ist unsere Query. Der Suchalgorithmus geht 
nun alle indizierten Seitentitel durch und präsentiert die, die er für am passendsten hält. 
Diese Titel sind die Keys. Für den Abgleich des Suchbegriffs und der Seitentitel benutzt 
das Verfahren eine Art Ähnlichkeitsverfahren (kleiner Spoiler: Sie erinnern sich bestimmt 
an die Kosinusähnlichkeit aus Bild 6.16), bei dem der Titel zurückgeliefert wird, dem der 
Suchalgorithmus gemäß der Anfrage am meisten Aufmerksamkeit schenkt. Laden wir 
nun eine Seite zu einem passenden Titel, erhalten wir mit dem Seiteninhalt den Value.
Kommen wir also darauf zu sprechen, wie wir diese Ähnlichkeit zwischen Query und Key
bestimmen. Wir nutzen dazu die Formel aus der Kosinusähnlichkeit, ändern sie aber leicht 
ab, damit wir sie für unsere Matrizen Query und Key verwenden können.
( ) =  T Q K Ähnlichkeit Q, K Betrag
Formel 6.3
Formel 6.3 zeigt, dass die erste Änderung ist, die Matrix K zu transponieren (denn sonst 
könnten wir aufgrund nicht übereinstimmender Dimensionen kein Skalarprodukt daraus 
bilden). Die zweite Änderung ist, dass wir den Skalierungsfaktor, der zuvor durch Multiplika tion der beiden Beträge von Q und K berechnet wurde, ersetzen – wodurch sehen wir gleich.
Kommen wir zurück zu den Matrizen, die die Gewichte der Linear Layers enthalten, die wir 
eben auf Basis unserer Embeddings trainiert haben. Dieselben Embeddings multiplizieren 
wir nun mit den optimierten Gewichten, die wir über eine Matrix abbilden. Zuvor müssen 
Ich lese gerne T
Ich
lese
gerne
x
Gewichte von 
Linear Layer Q
=
Posional
Embeddings
Query Matrix
Bild 6.22 Multiplikation der Positional Embeddings mit den Gewichten der Linear Layer Q, K bzw. V 
ergibt die entsprechende Matrix.



6.2 Wikipedia 177
wir die Embedding-Matrix transponieren (siehe Bild 6.22), da sonst die Dimensionen der 
Matrizen nicht für eine Matrixmultiplikation geeignet sind).
HINWEIS: Bei einer Matrixmultiplikation werden die Werte der ersten Zeile der 
Matrix A mit den jeweiligen Werten der ersten Spalte der Matrix B multipliziert und 
aufaddiert. Derselbe Prozess wird für alle Zeilen und Spalten der beiden Matrizen 
wiederholt. Eine Darstellung wie Bild 6.23, in der die Matrizen diagonal zueinander 
angeordnet werden und die Ergebnismatrix mittig zu beiden platziert wird, hilft da bei, die miteinander zu verrechnenden Spalten bzw. Zeilen zu identifizieren.
A0,0 A1,0 A2,0 A3,0
A0,1 A1,1 A2,1 A3,1
A0,2 A1,2 A2,2 A3,2
B0,0
B0,1
B0,2
B0,3
B1,0
B1,1
B1,2
B1,3
B2,0
B2,1
B2,2
B2,3
Matrix A
Matrix B
A0,0*B0,0 + 
A1,0*B0,1 + 
A2,0*B0,2 + 
A3,0*B0,3
Ergebnis
Bild 6.23 Die Multiplikation der Matrix A und der Matrix B führt zu einer Matrix mit den 
Dimensionen „Spalten Matrix B × Zeilen Matrix A“.
Das Ergebnis dieser Multiplikation ist für den Linear Layer Q die sogenannte Query Matrix. 
Die Multiplikation wiederholen wir für Linear Layer K und V mit denselben Embeddings und 
erhalten die Key Matrix und die Value Matrix.
Damit können wir uns in Bild 6.20 eine Ebene höher bewegen, konkret zu MatMul, wo wir 
Query und Key Matrix multiplizieren; analog zum Zähler der Formel für die Ähnlichkeits berechnung. Dazu, so haben wir ja bereits erfahren, müssen wir die Key Matrix erneut 
transponieren. Das Ergebnis ist der sogenannte Attention Filter (siehe Bild 6.24).
Query Matrix Key MatrixT
x =
96
65
25
67
93
85
13
74
99
Aenon Filter
Ich
lese
gerne
Ich
lese
gerne
Bild 6.24 Das Ergebnis der Multiplikation der Query mit der transponierten Key Matrix ist der Atten tion Filter, der bereits Aussagen darüber treffen kann, welche Begriffe einen starken, semantischen 
Bezug zueinander haben.



178 6 Intents entwickeln
Im nächsten Schritt Scale wird jede Zelle im Attention Filter durch die Wurzel der Dimensionen 
der Queries und der Keys geteilt (siehe Bild 6.25). Das wäre in unserem Fall die Wurzel aus 3. 
Dadurch glätten wir unsere Werte im Attention Filter, da diese durch eine Multiplikation sehr 
groß werden können und so die Aufmerksamkeit überproportional hoch auf bestimmte Be griffe gelenkt werden können, während andere keine Aufmerksamkeit bekommen. Es folgt 
die zeilenweise Anwendung der Softmax-Funktion, um die Wahrscheinlichkeitsverteilungen 
zu berechnen. Softmax sorgt dafür, dass die Werte sich in jeder Zeile des Attention Filters zu 
1,0 aufaddieren.
96
65
25
67
93
85
13
74
99
/ 3 =
55,4
37,5
14,4
38,7
53,7
49,1
7,5
42,7
57,2
55,4
37,5
14,4
38,7
53,7
49,1
7,5
42,7
57,2
somax ( ) =
0,9
0,1
0.0
0,1
0,8
0.1
0,0
0,1
0.9
Bild 6.25 Links werden die Werte der Attention Matrix durch die Wurzel aus dk geteilt. 
Rechts wird das Ergebnis dieser Division durch die Softmax-Funktion in eine Wahrscheinlichkeits verteilung zeilenweise umgerechnet (die Werte nach der Softmax-Berechnung habe ich der 
Anschaulichkeit halber leicht angepasst).
In der Diagonalen sehen wir, dass jeder Begriff sich selbst erst mal hohe Aufmerksamkeit 
schenkt. Jedoch auch die Beziehungen zwischen Ich und lese bzw. zwischen lese und gerne
haben einen wahrzunehmenden Attention Score. Der nächste Schritt gemäß Bild 6.20 ist, 
den berechneten Attention Filter mit der Value Matrix zu multiplizieren. Erst mal möchte 
ich Ihnen jedoch eine fantastische Analogie vorstellen, die es uns erleichtern wird zu ver stehen, was das Multi in der Multi-Head Attention zu suchen hat. Dazu verlassen wir aber 
die Domäne des Natural Language Processing und begeben uns in die Tiefen der Computer 
Vision. Bild 6.26 zeigt einen Jungen, der einen Tiger berührt. Ein Attention Filter hebt nun in 
dem Bild die Bereiche hervor, auf die die Aufmerksamkeit gelenkt werden soll. Nehmen wir 
einmal an, die Eltern suchen ihr Kind, dann wäre wohl wie oben der Junge hervorzuheben. 
Alle anderen Bereiche würden in den Hintergrund rücken oder ganz ausgeblendet werden.
Das Gleiche passiert auch mit unserem Text, wenn wir die Attention Filter Matrix mit der 
Value Matrix multiplizieren. Wir bekommen einen Filtered Value, in der der Fokus auf den 
linguistischen Features liegt, die in diesem Kontext im gleichen Satz eine Rolle spielen 
(Bild 6.27 links).
Formel 6.4, über die wir an die Multi-Head Attention gelangen, sollte uns jetzt nicht mehr 
erschrecken. Wir wissen, dass wir in der Klammer im Prinzip nur eine Kosinusähnlichkeit 
zwischen Query Matrix und Key Matrix berechnen und deren Wahrscheinlichkeitsverteilung 
(Softmax), wie wir eben erfahren haben, per Skalarprodukt mit der Value Matrix multiplizieren.
( )   =      
 
T
k
Q K Attention Q, K, V softmax V
d
Formel 6.4
Nun sind wir endlich bei dem Begriff Multi angekommen. Sie haben sicher gemerkt, dass 
in Bild 6.26 nicht nur ein Attention Filter angewandt wird, sondern mehrere, der Tiger und 
der Rhododendron rücken ebenfalls in den Fokus. Sollte der Tierpfleger seinen Tiger suchen 
oder der Gärtner noch zu gießende Pflanzen, ist das natürlich auch rechtens. Auch in unse rem Modell berechnen wir mehrere Attention Heads (bisher haben wir nur einen berechnet, 



6.2 Wikipedia 179
unseren ersten Filtered Value). Das Original-Paper berechnet acht, die den Fokus auf verschie dene linguistische Features legen. In unserer Visualisierung in Bild 6.26 sind es drei. Diese 
drei bzw. acht Attention Heads werden im vorletzten Baustein vertikal aneinandergehangen 
(Concat), sodass ein Attention Head eine 3 × 3-Matrix ergibt, der zweite eine 6 × 3-Matrix 
daraus macht, der dritte eine 9 × 3-Matrix und so weiter (siehe Bild 6.26 rechts).
Aenon Filter Aenon
Input
x
x
x
=
=
=
Bild 6.26 Die Anwendung eines Attention Filters auf ein Bild sorgt dafür, dass verschiedene Objekte 
in dem Bild fokussiert werden bzw. Aufmerksamkeit bekommen.
Aenon Filter
x =
Value Matrix Filtered Value
Aenon 
Head 1
Aenon 
Head 2
Aenon 
Head 3
Concat
Linear
Mul-Head
Aenon
Iteraon 1..3
Bild 6.27 Die Multiplikation des Attention Filters und der Value Matrix, die lediglich über einen 
Linear Layer in die „Multi-Head Attention“-Architektur eingeflossen ist, führt dazu, dass im Resultat 
ein Fokus auf den Features liegt, die der Attention Filter ausprägt (links). Sind die Attention Heads 
berechnet, werden sie konkateniert und über einen letzten Linear Layer auf die Dimension der ur sprünglichen Embeddings gebracht (rechts).



180 6 Intents entwickeln
Ein letzter Linear Layer in der Multi-Head Attention-Architektur aus Bild 6.20 macht aus 
dieser Matrix wieder eine Matrix in der Ursprungseingangsform unseres Embeddings von 
4 × 3. Damit haben wir den Part Multi-Head Attention abgeschlossen.
Residual Connections
Bevor wir nun die Add & Norm-Schicht anschauen, müssen wir kurz klären, was es mit den 
Pfeilen auf sich hat, die nach dem Positional Encoding an der Multi-Head Attention vorbeigehen 
und Informationen scheinbar ungefiltert weitergeben. Diese, in Bild 6.14 mit RC (Residual 
Connection) betitelten, Pfade dienen der Informationserhaltung. Denn es kann durchaus 
vorkommen, dass Bruchstücke unserer Informationen über die vielen Schichten, die diese 
durchlaufen, vergessen werden. Dem wirkt man nun über die Residual Connections ent gegen, indem das Positional Embedding nicht nur in die Multi-Head Attention eingeschleust 
wird, sondern auch, als Bypass wenn man so möchte, direkt an Add & Norm weitergegeben 
wird. Bei einem genaueren Blick auf die Gesamtarchitektur werden Sie feststellen, dass die 
Residual Connections an vielen Stellen erneut auftauchen.
Add & Norm
Der Part Add ist der einfache, denn hier wird die Matrix, die aus der Multi-Head Attention Schicht ausgegeben wird, mit dem Positional Encoding addiert, das die Schicht über die Resi dual Connection erreicht (Bild 6.28 oben). Da wir ja dafür gesorgt haben, dass die Multi-Head 
Attention-Schicht eine gleichdimensionierte Matrix ausgibt, wie sie eingegeben bekommt, 
steht dieser Addition nichts im Wege und wir müssen nicht zusätzlich einen Linear Layer
zur Anpassung der Dimension einziehen.
Auch der Norm-Part (Bild 6.28 unten), der für Normalization steht, ist denkbar einfach zu 
verstehen. Aus der Matrix, die wir durch die Addition erhalten, berechnen wir entlang 
der horizontalen Achse zeilenweise den Mittelwert aller Werte in dieser Achse sowie die 
Standardabweichung.
Posional Embedding
aus Residual Connecon
Mul-Head
Aenon
+
0,82
0,76
0,96
0,31
1,25
2,67
0,14
1,13
0,13
1,3
0,77
0,44
=
Mielwert (µ) Standardabweichung (σ)
0,73 0,57
1,58 0,98
0,62 0,43
0,63 0,44
0,82
0,76
0,96
0,31
1,25
2,67
0,14
1,13
0,13
1,3
0,77
0,44
= −
2 +
=
0,16
–0,84
0,35
–0,73
0,91
1,11
–0,5
1,14
–1,05
–0,29
0,16
–0,43
Ad
d Norm
Bild 6.28 Die Schicht Add & Norm addiert zuerst die „Multi-Head Attention“-Matrix und das Positional 
Embedding aus der Residual Connection (oben) und normiert dann die Matrix mithilfe des Mittelwerts 
und der Standardabweichung (unten).



6.2 Wikipedia 181
Eine Beispielrechnung für die Normalisierung der ersten Zelle ist in Formel 6.5 zu sehen, 
in der wir Gebrauch von dem vorberechneten Mittelwert und der Standardabweichung für 
eben diese Zeile machen. Das e bewahrt uns davor, dass der Nenner 0 wird und durch 0 sollte 
man ja bekanntlich nicht teilen.
− =
+ 0 2
0,82 0,73
0,57 0,0001
x Formel 6.5
Diesen Schritt wiederholen wir nun für alle Zellen und erhalten eine normalisierte Matrix.
Feed Forward Layer
Die Feed-Forward-Schicht am Ende des Encoders und des Decoders besteht aus zwei Linear 
Layers, die durch eine Activation Function verbunden werden. Eine Activation Function ent scheidet, einfach gesagt, ob und in welcher Ausprägung die Ausgabe des vorherigen Neurons 
an das nächste Neuron weitergeleitet wird. Stellen Sie sich als Analogie den Empfang einer 
Notaufnahme vor, die entscheidet, ob ein Patient behandelt werden muss. Bei einem ab gebrochenen Fingernagel ist die Priorität der Aufnahme sicherlich gering, bei Fieber höher 
und bei einer Platzwunde am Kopf noch höher. Abweisen darf der Empfang jedoch keinen, 
er kann jedoch die weiterbehandelnden Pfleger und Ärzte darauf hinweisen, dass es hier 
wahrscheinlich, überspitzt gesagt, mit einem Benjamin-Blümchen-Pflaster getan ist.
Zwei der am häufigsten verwendeten Activation Functions sind tanh (Tangent Hyperbolic 
Function) und ReLU (Rectified Linear Unit), die Sie sowohl in Formel 6.6 und Formel 6.7, als 
auch in Bild 6.29 sehen.
( ) − = − + 2
2 tanh 1
1 e x x Formel 6.6
ReLU max 0, ( x x ) = ( ) Formel 6.7
–6 2 4 6
2
4
6
x
y
–4 –2 2 4
0,5
1
–0,5
–1
x
y
tanh(x) ReLU(x)
–4 –2
Bild 6.29 Die Aktivierungsfunktionen tanh und ReLU
Die grafische Darstellung zeigt die Unterscheidung zwischen der sigmoiden Aktivierungs funktion tanh und der linearen Aktivierungsfunktion ReLU. Die Ausgabe der Funktion kann 
nun leicht über die gegebenen Formeln berechnet werden, wenn die Ausgabe des vorherigen 
Neurons als x gesetzt wird.



182 6 Intents entwickeln
Neben tanh und ReLU existieren noch einige weitere Funktionen, wie etwa die Sigmoid Funktion, die Aufgrund einiger Unzulänglichkeiten nicht mehr eingesetzt wird, oder GELU
(Gaussian Error Linear Units), die in Transformern mit am häufigsten zum Einsatz kommt.
Kurz gesagt, die Feed-Forward-Schicht besteht aus zwei linearen Schichten, die durch eine 
Aktivierungsfunktion voneinander getrennt werden. Durch diese wird einem neuronalen 
Netz die Linearität genommen, was es diesem erlaubt, komplexere Beziehungen und Muster 
in den Daten zu modellieren.
HINWEIS: Eine Besonderheit der Feed-Forward-Schicht ist, dass nicht alle 
Embeddings als Matrix eingespielt werden, sondern jedes Embedding von den 
anderen unabhängig. Man spricht von einer position-wise Feed-Forward-Schicht.
Mit der Ausgabe der Feed-Forward-Schicht und der anschließenden Normalisierung errei chen wir nun endlich das Ende des Encoders. Der Encoder Stack, also alle Schichten, die der 
mit Nx betitelte Rahmen in Bild 6.14 umfasst, werden nun mehrmals ausgeführt und die 
Embeddings aktualisiert, bis diese das gewünschte Maß an Kontextinformationen enthalten. 
Dabei werden die resultierenden Embeddings eines Durchlaufs wieder als Eingabe verwendet.
6.2.2.2 Der Decoder
Wie schon in der Einleitung beschrieben, ist der Decoder dafür verantwortlich, eine Sequenz 
unter Zuhilfenahme der Eingabe des Encoders iterativ zu generieren, die dann am Ende das 
englische I like to read beinhaltet. Dabei wird ihm initial ein Start-Token mitgegeben, das nach 
und nach um das jeweils neue, vorhergesagte Wort ergänzt wird. Nach und nach bedeutet, 
dass der Decoder für jedes vorhergesagte Token erneut aufgerufen werden muss und nicht 
eine ganze Sequenz in einem Durchlauf generiert. Die Dauer einer Übersetzung ist also auch 
abhängig von der Länge der zu übersetzenden Sequenz. Bei jedem Durchlauf wird ihm die 
bisher generierte Sequenz samt des Start-Tokens übergeben, bis er das End-Of-Sequence-Token 
generiert und selbstständig merkt, dass der Decoding-Prozess abgeschlossen ist.
Dankenswerterweise enthält der Decoder nur wenige Komponenten, die wir noch nicht 
kennengelernt haben. Der Hauptunterschied zwischen Encoder und Decoder ist tatsächlich, 
dass letzterer zwei Attention-Schichten aufweist:
 Masked Multi-Head Attention und
 Multi-Head Attention.
Die einfache Multi-Head Attention ist uns ja schon zur Genüge bekannt, was aber hat es mit 
der hinzugefügten Maskierung im ersten Attention-Block im Decoder auf sich? Mit dieser 
Maskierung stellen wir sicher, dass die Tokens, die im Decoder erzeugt werden, nur auf 
die vergangenen Ausgaben achten, jedoch nicht auf die zukünftigen, die ihnen ja ebenfalls 
bekannt sind. Das Konzept ist vergleichbar mit Aufgaben einer Englischklausur, für die Sie 
die Ergebnisse kennen. In diesem Fall würden Sie die Lösung eins zu eins kopieren, um eine 
möglichst gute Note zu bekommen. Durch die Maskierung erreichen wir, dass die Lösung 
eben nicht bereitsteht, indem wir sie auf 0 setzen. Nur so lernen wir etwas und erzielen 
nicht einfach nur eine gute Note. Und genau das ist es doch, wofür Schule und Studium da 
sind, nicht wahr?



6.2 Wikipedia 183
Nach der Skalierung in der Multi-Head Attention führen wir eine Maskierung für alle Tokens 
aus, die nach diesem Token in der Eingabe auftreten (siehe Bild 6.30). Das erreichen wir, in dem wir eine Look-Ahead-Matrix erzeugen, die für alle Wörter, die dem aktuellen Wort folgen 
(und von denen der Decoder nichts wissen darf), mit einem negativen Unendlichkeitswert 
belegen. Das Wort Ich darf zum Beispiel von lese und gerne nichts wissen, da das Modell ja lernen soll, 
den nächsten Begriff selbstständig zu finden. Das Wort lese darf nur von gerne nichts wissen. 
Addiert man nun die Look-Ahead-Maskierung auf das Zwischenergebnis der Attention, wird 
automatisch eine maskierte Matrix erzeugt, die für alle noch nicht bekannten Begriffe den 
negativen Unendlichkeitswert beinhaltet. Durchläuft diese Matrix dann wiederum Softmax, 
ergeben all diese maximal negativen Werte 0.
Ich
lese
gerne
Ich
lese
gerne
55,4
37,5
14,4
38,7
53,7
49,1
7,5
42,7
57,2
Skaliertes
Produkt aus 
Query und Key 
Matrix
0
0
0
–inf
0
0
–inf
–inf
0
+ =
Look Ahead
Mask
55,4
37,5
14,4
–inf
53,7
49,1
–inf
–inf
57,2
Maskierte
Matrix
Bild 6.30 Bei der Maskierung wird die skalierte Matrix mit einer Look Ahead Mask aufsummiert. 
All die Begriffe, die dem aktuellen Wort folgen, werden durch einen negativen Unendlichkeitswert 
„entwertet“. Sie ergeben nach der Anwendung von Softmax 0.
TIPP: Beim Training eines Transformers haben wir das Konzept des Maskierens 
zwar angewandt, aber später wieder aufgehoben, warum? Der Decoder muss 
während des Trainings erfahren, ob er ein Token richtig oder falsch vorhergesagt 
hat. Deswegen wird ihm nach einem Durchlauf das richtige Token (Ground Truth) 
übergeben, sodass dieses der Vorhersage gegenübergestellt werden kann. Damit 
ist der Transformer in der Lage, die Verlustrate (Loss) zu berechnen, um zu ent scheiden, wie lange das Netz noch trainiert werden muss.
Kommen wir nun zur zweiten Multi-Head Attention-Schicht, in die die Eingaben teilweise aus 
dem Encoder und dem Decoder fließen (siehe Bild 6.31). Konkret werden Query und Key vom 
Encoder geliefert und der Value vom Decoder. Bei Query und Key handelt es sich um eine 
exakte Kopie, die das Resultat des gesamten Encoders ist.
Das erste Token, das der Decoder erhält, ist in der Regel ein Start-Token ohne inhaltliche Be deutung. Dieses durchläuft nun die Masked Multi-Head Attention-Schicht und wird als Value 
Matrix an die Multi-Head Attention-Schicht nach der Normalisierung übergeben (dargestellt 
wie üblich in Bild 6.14). Mit Query und Key aus dem Encoder und Value aus dem Decoder kann 
letzterer nun weiterarbeiten, bis wir oben beim Linear Layer ankommen. Dieser ist nun etwas 
anders als die anderen, denn seine Ausgabeschicht ist wesentlich komplexer, auch wenn er 
weiterhin ein einfacher Dense Layer ist, in dem alle Neuronen miteinander verbunden sind. 



184 6 Intents entwickeln
Wie komplex? Nun, er hat so viele Ausgaben wie unser Vokabular Wörter hat, denn was wir 
hier tatsächlich tun, ist, eine Klassifikation durchzuführen. Wir klassifizieren nämlich unsere 
Matrix und erhalten als Ausgabe ein Token, das wieder von einem Menschen gelesen und 
verstanden werden kann. Und dieser Klassifikator hat nun so viele Klassen, wie wir Wörter 
in unserem Vokabular haben.
TIPP: Jetzt macht auch die Subword Tokenization vom Beginn wieder Sinn, denn 
wenn jedes noch so komplex zusammengesetzte Wort, und darin ist ja die deut sche Sprache durch kaum eine andere zu schlagen, eine eigene Klasse bekäme, 
würde der Klassifikator mit ebenso vielen Klassen aufwarten müssen. Dadurch, 
dass wir seltene zusammengesetzte Begriffe in mehrere Tokens aufbrechen, 
schränken wir unser Vokabular ein und verringern damit die benötigte Rechen leistung.
Der Vorgang dazu ist in Bild 6.32 abgebildet. Die Ausgabe der Feed Forward-Schicht im Decoder 
wird transponiert und flachgeklopft, das heißt, die Matrix wird Zeile für Zeile konkateniert. 
Dadurch wird aus der transponierten 3 × 4- eine 1 × 12-Matrix. Die Abstände in Bild 6.32
unten habe ich lediglich aus Gründen der Lesbarkeit gelassen.
Nehmen wir an, wir hätten bereits die drei Tokens I like to generiert. Bei jedem Durchlauf 
hängt der Decoder das aktuelle Positional Encoding an eine Liste aller bisherigen Positional 
Encodings an. Diese werden dann wieder und wieder als Input für den Decoding-Prozess 
verwendet. Auf die bisherige, berechnete Ausgabe <start> I like to folgt mit der größten 
Wahrscheinlichkeit das Wort read, denn der Softmax der klassifizierten Begriffe in unserem 
Vokabular ist für diesen Wert mit 0,78 am höchsten. Ist das Modell ausreichend trainiert 
worden, ist das nächste vorhergesagte Token das EOS-Token und der Decoder beendet seine 
Arbeit.
Mul-Head
Aenon
Encoder
Query
Key
Value
Duplikat
…
…
Bild 6.31 Der Multi-Head Attention Layer des Decoders nimmt Query und Key als Resultat des 
Encoders entgegen und stellt nur die Value Matrix selber.



6.3 Erinnerungsfunktion 185
Linear ...
Umfang des Vokabulars
like I summer r car to ead
0,16 –0,84 0,35 –0,73
0,91 1,11 –0,5 1,14
I –1,05 –0,29 0,16 –0,43
like
to
Flaen() –1,05 –0,29 0,16 –0,43 0,91 1,11 –0,5 1,14 0,16 –0,84 0,35 –0,73
<start> I like to ...
Somax
0,11 0,21 0,35 0,57 0,26 0,78
read
Bild 6.32 Die letzten beiden Schichten des Decoders dienen dazu, zu prognostizieren, welches Token 
am wahrscheinlichsten auf die bisherigen Tokens folgt.
■ 6.3 Erinnerungsfunktion
Ob es Geburtstage sind oder die Erinnerung daran, dass ein Arzttermin ansteht: Zeitlich fest gelegte Ereignisse vergessen wir Menschen gerne, haben wir doch meist viele verschiedene 
Dinge im Kopf, nur nicht den Geburtstag der Tante oder der Kollegin. Was liegt also näher, als 
unseren Assistenten mit einer Erinnerungsfunktion auszustatten? Technisch liegt die Heraus forderung sicher einmal im Parsing des Befehls, müssen wir uns doch wieder einmal mit 
dem Satzbau im Deutschen herumschlagen. Andererseits nutzen wir diesen Anwendungsfall, 
um Callbacks in unser Framework zu integrieren, d. h. Funktionen, die zu einem späteren 
Zeitpunkt von einer anderen Funktion aufgerufen werden, im schwierigsten Fall auch noch 
parametrisiert. Denn bisher hat unser Assistent immer direkt eine Antwort geliefert, im Falle 
einer Erinnerung soll er es aber später zu einem vorher definierten Zeitpunkt tun. Weiterhin 
schauen wir uns an, wie wir mit Datumswerten arbeiten, was zumindest aus meiner Sicht 
in jeder Programmiersprache immer wieder eine Herausforderung ist, denn die APIs dafür 
sind fast so vielfältig wie Datumswerte selbst. Und wenn wir schon mit Zahlen hantieren, 
können wir uns im selben Atemzug auch anschauen, wie wir zwischen ausgeschriebenen 
Zahlen, etwa einer Zehn und dem numerischen Wert 10 hin und her konvertieren. Denn 
wenn Sie bereits ein wenig mit Vosk experimentiert haben, werden Sie festgestellt haben, 
dass Zahlen im Text-To-Speech-Framework unserer Wahl immer ausgeschrieben zurückge 


186 6 Intents entwickeln
geben werden, man aber mit Worten nicht rechnen kann. Eine Transformation ist zwingend 
notwendig, wenn wir Befehle wie Erinnere mich in zehn Minuten ans Kochen in einen Aufruf 
mit einem konkreten Datumswert umwandeln möchten. Denn nur einen numerischen Wert 
können wir für eine zeitliche Berechnung hernehmen. Zu guter Letzt werden wir das erste 
Mal die Lautstärke unseres Assistenten anpassen lassen, denn wenn ein Stream oder eine 
Audiodatei gespielt wird und gleichzeitig ein Befehl eingesprochen wird, soll die aktuelle 
Wiedergabe kurze Zeit leiser abgespielt werden, um den Befehl gut verstehen zu können. 
Der Lautstärkeregelung werden wir im Folgekapitel dann noch einen Feinschliff verpassen. 
Sie sehen, es geht diesmal sehr stark um die Weiterentwicklung unseres Frameworks.
6.3.1 Anpassen der Lautstärke
Das zu diesem Kapitel passende Beispiel finden Sie im Ordner 10_c_reminder. Passen Sie 
darin zunächst die config.yml so an, dass sie zusätzlich folgende Information über die zu 
Beginn gesetzte Lautstärke enthält:
assistant:
 volume: 0.5
Die Lautstärke soll zu Beginn mit einem Wert von 0,5 initialisiert werden und zur Laufzeit 
zwischen 0,0 und 1,0 rangieren. Sie zu setzen ist die nächste Aufgabe. In der main_10_c.py
wird in der __init__()-Methode der mixer initialisiert. Diesem fügen wir noch zwei Zeilen 
hinzu, in denen wir die Lautstärke aus der Konfiguration lesen und setzen (siehe Listing 6.19).
Listing 6.19 Initiales Setzen der Lautstärke
mixer.init()
self.volume = self.cfg["assistant"]["volume"]
mixer.music.set_volume(self.volume)
Nun begeben wir uns in Listing 6.20, genauer in die Methode run(). Dort wollen wir, sollte 
unser Assistent im Moment auf eine Spracheingabe hören, die Lautstärke herunterdrehen, 
sollte nebenbei auch noch eine Audiodatei abgespielt werden. Nach der Abfrage, ob derzeit 
etwas gespielt wird, fügen wir die bedingte Anpassung der Lautstärke hinzu, wie in den 
Zeilen 9 und 10 zu sehen ist. Der Wert 0,1 deutet darauf hin, dass die derzeitige Widergabe 
so leise wie möglich geschehen soll.
Listing 6.20 Anpassung der Lautstärke bei Abspielen einer Audiodatei während der Verarbeitung 
einer Eingabe
1. def run(self):
2. try:
3. while True:
4.
5. ...
6.



6.3 Erinnerungsfunktion 187
7. if global_variables.voice_assistant.is_listening:
8.
9. if mixer.music.get_busy():
10. mixer.music.set_volume(0.1)
11. ...
12. else:
13. mixer.music.set_volume(global_variables.voice_assistant.volume)
In Zeile 13 wird die Lautstärke wieder auf den vom Benutzer definierten Pegel zurückgesetzt, 
falls der Sprachassistent auf keinen Befehl mehr lauscht.
6.3.2 Verarbeiten eines Erinnerungsbefehls
Das Parsing einer Erinnerungsaufforderung kann wieder etwas komplexer werden, da die 
Möglichkeiten, eine solche zu äußern, vielfältig sind. Das deutet immer darauf hin, dass wir 
keine starren Muster parsen, sondern ein Modell anlernen, um den Intent zu verstehen. Es 
folgt konsequenterweise die Anlage eines Snips-NLU Intents, das auf einem Trainingsdaten satz basieren soll, den wir in intents/snips-nlu/reminder_dataset.yaml anlegen und mit dem 
Inhalt von Listing 6.21 befüllen.
Listing 6.21 Trainingsdaten für den Intent Reminder
1. # action_an
2. ---
3. type: entity
4. name: reminder_action_to
5. automatically_extensible: true
6. use_synonyms: false
7. matching_strictness: 0.8
8. values:
9. - an den Geburtstag von Peter
10. - an den Wocheneinkauf
11. - ans Kochen
12. - ans Wäschewaschen
13.
14. # action_infinitiv
15. ---
16. type: entity
17. name: reminder_action_infinitive
18. automatically_extensible: true
19. use_synonyms: false
20. matching_strictness: 0.8
21. values:
22. - die Zähne zu putzen
23. - die Fenster zu schließen
24. - Peter ein Geschenk zu kaufen
25. - zu putzen
26.
27. # reminder intents



188 6 Intents entwickeln
28. ---
29. type: intent
30. name: reminder
31. utterances:
32. - Erinnere mich [time:snips/datetime](um 10 Uhr)
33. [reminder_to:reminder_action_to](an meine Fernsehsendung)
34. - Erinnere mich [time:snips/datetime](um 10 Uhr)
35. [reminder_to:reminder_action_to](an das Essen)
36. - Erinnere mich [reminder_to:reminder_action_to](an den Geburtstag von
37. Peter) [time:snips/datetime](um 10 Uhr)
38. - Erinnere mich [time:snips/datetime](um 10 Uhr)
39. [reminder_to:reminder_action_to](meine Fernsehsendung)
40. - Erinnere mich [time:snips/datetime](um 10 Uhr)
41. [reminder_to:reminder_action_to](an das Essen)
42. - Erinnere mich [reminder_to:reminder_action_to](an den Geburtstag von
43. Peter) [time:snips/datetime](um 10 Uhr)
44. - Erinnerung [time:snips/datetime] (in 10 Minuten)
45. [reminder_infinitive:reminder_action_infinitive](den Kuchen aus dem
46. Kühlschrank zu nehmen)
47. - Erinnerung [time:snips/datetime] (um 8 Uhr) daran
48. [reminder_infinitive:reminder_action_infinitive](die Fenster zu
49. schließen)
50. - Erinnere mich [time:snips/datetime] (um 16 Uhr)
51. [reminder_infinitive:reminder_action_infinitive](zu kochen)
Wir beginnen mit der Definition unseres Intents in Zeile 27. Diesen nennen wir, nicht 
unbedingt überraschenderweise, reminder und erinnern uns daran, dass wir in der Liste 
Utterances ein paar Beispielformulierungen angeben müssen, damit das Framework anhand 
derer lernen kann, wie ein Erinnerungsbefehl in etwa klingt. Ich habe hier zwischen zwei 
möglichen Formulierungen unterschieden:
a) Erinnere mich an etwas
b) Erinnere mich, etwas zu tun
Beide sind in unserem Sprachgebrauch gängig und wir könnten sie getrost mischen, wäre 
da nicht der Fall, dass wir das Ereignis der Erinnerung auch wiedergeben müssten! Die ent sprechenden Antworten sähen nämlich verschieden aus:
a) Ich soll dich an das Einkaufen erinnern
b) Ich soll dich daran erinnern, zu kochen
In Beispiel a) wird an ein Nomen bzw. an die Substantivierung eines Verbs erinnert, in b) 
an einen Infinitiv mit einem zu. Unterscheiden wir zwischen diesen beiden, können wir 
daraus leicht Antwortsätze generieren, weswegen wir uns gerne den Aufwand machen und 
die zwei Entitäten reminder_action_infinitive (Zeile 14, Listing 6.21) und reminder_action_to
(Zeile 1, Listing 6.21) anlegen und diese in den in Utterances gelisteten Beispielen angeben.
Im nächsten Schritt legen wir die Konfiguration für unseren Intent im zu erstellenden Ordner 
intents/functions/reminder unter dem Namen config_reminder.yml an und befüllen sie entspre chend dem Text in Listing 6.22. Wie auch in den vorherigen Beispielen, möchte ich zeigen, 
wie eine Erinnerung in englischer und deutscher Sprache erfolgen könnte, was in diesem 
Falle auch sicherlich dem Verständnis der zu konstruierenden Antworten zugutekommt.



6.3 Erinnerungsfunktion 189
Listing 6.22 Konfiguration für den Reminder Intent
1. intent:
2. reminder:
3. de:
4. reminder_same_day_infinitive: ["Ich werde dich um {} Uhr {} erinnern {}."]
5. reminder_infinitive: ["Ich erinnere dich am {} {} {} um {} Uhr {} {}."]
6. reminder_same_day_to: ["Ich erinnere dich um {} Uhr {} {}."]
7. reminder_to: ["Ich erinnere dich am {} {} {} um {} Uhr {} {}."]
8. reminder_same_day_no_action: ["Ich erinnere dich um {} Uhr {}."]
9. reminder_no_action: ["Ich erinnere dich am {} {} {} um {} Uhr {}."]
10. no_time_given: ["Ich habe das Erinnerungsdatum nicht verstanden."]
11.
12. execute_reminder_infinitive: ["Ich soll dich daran erinnern {}."]
13. execute_reminder_to: ["Ich erinnere dich {}."]
14. execute_reminder: ["Ich soll dich jetzt erinnern."]
15. en:
16. reminder_same_day_infinitive: ["I'll remind you at {} {} to {}."]
17. reminder_infinitive: ["I'll remind you on the {} at {} {} to {}."]
18. reminder_same_day_to: ["I'll remind you at {} {} to {}."]
19. reminder_to: ["I'll remind you on the {} at {} {} to {}."]
20. reminder_same_day_no_action: ["I'll remind you at {} {}."]
21. reminder_no_action: ["I'll remind you on the {} at {} {}."]
22. no_time_given: ["I could not understand the time to remind you."]
23.
24. execute_reminder_infinitive: ["I want to remind you to {}."]
25. execute_reminder_to: ["I want to remind you to {}."]
26. execute_reminder: ["This is a reminder."]
Anhand der Konfiguration sehen Sie bereits, welche Arten von Erinnerungen wir zulassen 
werden. Diese Sätze bestätigen, dass die Erinnerung erfolgreich erstellt wurde:
 Eine Erinnerung des Typs b) zu einer bestimmten Uhrzeit ohne eine Datumsangabe (Zeile 4)
 Eine Erinnerung des Typs b) mit Datums- und Uhrzeitangabe (Zeile 5)
 Eine Erinnerung des Type a) zu einem bestimmten Zeitpunkt ohne Datumsangabe (Zeile 6)
 Eine Erinnerung des Typs a) mit Datums- und Uhrzeitangabe (Zeile 7)
 Eine Erinnerung ohne Aktion am selben Tag (Zeile 8)
 Eine Erinnerung ohne Aktion mit Datums- und Uhrzeitangabe (Zeile 9)
Anhand der Platzhalter sehen Sie, dass wir in der bald folgenden Implementierung recht viele 
Felder ausfüllen müssen, etwa für Tag, Monat, Jahr, Stunde und Minute. In Zeile 10 definieren 
wir, welcher Satz gesprochen wird, wenn der Erinnerungszeitpunkt nicht verstanden wurde 
und in den Zeilen 12–14 sind die Sätze, die gesprochen werden, wenn der Callback an dem 
gewünschten Zeitpunkt aufgerufen wird und an das jeweilige Ereignis erinnert. Ergänzen Sie 
nun im Ordner intents/functions/reminder die Datei requirements.txt und fügen Sie folgende 
zwei Anhängigkeiten hinzu.
python-dateutil==2.8.1
num2words==0.5.10



190 6 Intents entwickeln
python-dateutil hilft uns bei der Arbeit mit Datumswerten und num2words übernimmt die 
Umwandlung von Zahlen in Wörter.
Nun müssen wir jedoch noch definieren, wo wir unsere ganzen Erinnerungseinträge spei chern möchten, denn sicher macht es keinen Sinn, diese nur im Speicher vorzuhalten, denn 
da sind sie nicht persistent und weg, sollte der Sprachassistent mal aus irgendeinem Grund 
neu gestartet werden. Da wir bereits mit TinyDB gearbeitet haben, wollen wir diese auch hier 
einsetzen. Erstellen Sie dafür im selben Ordner die Datei reminder_db.json und fügen Sie 
folgende Zeile ein, um die initiale Struktur der Einträge festzulegen.
{"reminder": {}}
Beginnen wir nun, die Logik in intent_reminder.py zu implementieren, die wir ebenfalls in 
intents/functions/reminder anlegen. Zu Beginn legen wir in Listing 6.23 die Imports fest, 
erstellen eine Reminder-Datenbank samt zugehöriger reminder-Tabelle und laden die Intent Konfiguration aus config_reminder.yml.
Listing 6.23 Initialisieren des Reminder-Intents
1. from dateutil.parser import parse
2. from num2words import num2words
3. from tinydb import TinyDB, Query
4.
5. # Initialisiere Datenbankzugriff auf Modulebene
6. reminder_db_path =
7. os.path.join('intents','functions','reminder','reminder_db.json')
8. reminder_db = TinyDB(reminder_db_path)
9. reminder_table = reminder_db.table('reminder')
10.
11. # Lade die Config global
12. CONFIG_PATH =
13. os.path.join('intents','functions','reminder','config_reminder.yml')
Bevor wir nun zur Hauptfunktion kommen, wenden wir uns in Listing 6.24 jener mit Namen 
spoken_date zu, die ein Datum der gesprochenen Sprache anpasst. Dazu gehört, dass wir zur 
vollen Stunde keine Minuten nennen, das Jahr nicht sprechen, wenn das Datum im aktuellen 
Jahr liegt und den Tag und Monat eines Datums sprachlich durch Ordinalia (erster, zweiter …) 
ausdrücken. Tag und Monat werden dabei über die Bibliothek num2words in ausgeschriebene 
Worte umgewandelt, damit unsere Text-To-Speech-Implementierung nicht in Versuchung 
kommt, Punkte von Ordinalzahlen als Satzzeichen zu interpretieren.
Listing 6.24 Umwandeln eines numerischen Datums in einen ausgeschriebenen Satz
1. def spoken_date(datetime, lang):
2. hours = str(datetime.hour)
3. minutes = "" if datetime.minute == 0 else str(datetime.minute)
4. day = num2words(datetime.day, lang=lang, to="ordinal")
5. month = num2words(datetime.month, lang=lang, to="ordinal")
6. year = "" if datetime.year == datetime.now().year else str(datetime.year)



6.3 Erinnerungsfunktion 191
7.
8. if lang == 'de':
9. day += 'n'
10. month += 'n'
11.
12. return hours, minutes, day, month, year
spoken_date() findet in der Funktion reminder() Verwendung, der wir uns nun zuwenden.
HINWEIS: Um die einzelnen Konfigurationsparameter auszulesen, sind manchmal 
recht viele verschachtelte Zugriffe auf die YAML-Datei notwendig. Um das Lesen 
etwas zu erleichtern, habe ich diese in den folgenden Listings per […] abgekürzt. 
Ersetzen Sie diesen Platzhalter gedanklich durch ['intent']['reminder'][LANGUAGE]
bzw. in allen folgenden Kapiteln mit anderem Intent-Namen anstelle von reminder.
In Zeile 3 von Listing 6.25 ermitteln wir zuerst den Namen des aktuellen Sprechers, denn 
wir wollen neben der eigentlichen Erinnerung auch abspeichern, wer diese erstellt hat. Den 
Sprecher bekommen wir über dessen stimmlichen Fingerabdruck heraus, den wir ja, falls 
erkannt, nach jeder Spracheingabe global in die Variable current_speaker schreiben und so 
im Zugriff haben. Es folgt das übliche Auslesen der Konfiguration, darunter einiger starrer 
Sätze, wie etwa NO_TIME_GIVEN, den wir zurückliefern, wenn eine Erinnerung ohne eine 
zeitliche Angabe eingegeben wurde.
In Zeile 26 greifen wir auf eine Wunderwaffe der Zeitformatierung zurück, denn parse()
aus dem Modul dateutil.parser kann Datumswerte ohne Angabe eines Formats (mit wenigen 
Ausnahmen) richtig einlesen und nimmt uns so eine Menge Arbeit ab. Die Funktion ist immer 
dann sinnvoll, wenn wir nicht selber die Hoheit über das Datumsformat haben, das wir lesen 
wollen, was recht häufig bei Schnittstellen der Fall ist, von denen unbekannte Daten eingehen.
Nun rufen wir spoken_date() auf, um die einzelnen Bausteine des Datums so zu formatieren, 
dass sie unserem menschlichen Sprachgebrauch entsprechen. Es folgt eine, zugegebener maßen etwas unschöne, Reihe von If-Else-Verzweigungen, die den Rückgabewert der Funktion 
so formatiert, dass folgende Fälle berücksichtigt werden:
 Es wurde eine Erinnerung für denselben Tag eingestellt.
 Formulierung mit erinnere mich, etwas zu tun.
 Formulierung mit erinnere mich an etwas.
 Formulierung einer einfachen Erinnerung ohne eine bestimmte Tätigkeit.
 Es wurde eine Erinnerung für einen anderen Tag eingestellt.
 Die Formulierungsunterschiede sind analog zum ersten Punkt.
In jedem Fall wird am Ende ein Eintrag per Insert in die Reminder-Tabelle geschrieben, der 
die Erinnerungszeit, die Art der Formulierung, die Nachricht selbst und den Sprechernamen 
beinhaltet. Das Resultat der Funktion reminder() ist nun nicht die Erinnerung, sondern 
eine Bestätigung, dass eine solche angelegt wurde.



192 6 Intents entwickeln
Listing 6.25 Die Funktion reminder() legt neue Erinnerungen in der reminder-Datenbank an
1. def reminder(time=None, reminder_to=None, reminder_infinitive=None):
2.
3. speaker = global_variables.voice_assistant.current_speaker
4.
5. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
6. cfg = None
7. with open(CONFIG_PATH, "r", encoding='utf8') as ymlfile:
8. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
9.
10. if not cfg:
11. logger.error("Konnte Konfigurationsdatei für reminder nicht lesen.")
12. return ""
13.
14. NO_TIME_GIVEN =
15. random.choice(cfg['...']['no_time_given'])
16.
17. result = ""
18. if speaker:
19. result = speaker + ', '
20.
21. # Wurde keine Uhrzeit angegeben?
22. if not time:
23. return result + NO_TIME_GIVEN
24.
25. # Parse das Datum über die Hilfsfunktion parse
26. parsed_time = parse(time)
27.
28. # Generiere das gesprochene Datum
29. hours, minutes, day, month, year = spoken_date(parsed_time, LANGUAGE)
30.
31. # Am selben Tag wie heute?
32. if datetime.now().date() == parsed_time.date():
33. if reminder_to:
34. SAME_DAY_TO =
35. random.choice(cfg['...']['reminder_same_day_to'])
36. SAME_DAY_TO = SAME_DAY_TO.format(hours, minutes, reminder_to)
37. result = result + " " + SAME_DAY_TO
38. reminder_table.insert({'time':time, 'kind':'to', 'msg':reminder_to,
39. 'speaker':speaker})
40. elif reminder_infinitive:
41. SAME_DAY_INFINITIVE =
42. random.choice(cfg['...']['reminder_same_day_infinitive'])
43. SAME_DAY_INFINITIVE = SAME_DAY_INFINITIVE.format(hours, minutes,
44. reminder_infinitive)
45. result = result + " " + SAME_DAY_INFINITIVE
46. reminder_table.insert({'time':time, 'kind':'inf',
47. 'msg':reminder_infinitive, 'speaker':speaker})
48. else:
49. # Es wurde nicht angegeben, an was erinnert werden soll
50. SAME_DAY_NO_ACTION =
51. random.choice(cfg['...']['reminder_same_day_no_action'])
52. SAME_DAY_NO_ACTION = SAME_DAY_NO_ACTION.format(hours, minutes)
53. result = result + " " + SAME_DAY_NO_ACTION



6.3 Erinnerungsfunktion 193
54. reminder_table.insert({'time':time, 'kind':'none', 'msg':'',
55. 'speaker':speaker})
56. else:
57. if reminder_to:
58. TO = random.choice(cfg['...']['reminder_to'])
59. TO = TO.format(day, month, year, hours, minutes, reminder_to)
60. result = result + " " + TO
61. reminder_table.insert({'time':time, 'kind':'to', 'msg':reminder_to,
62. 'speaker':speaker})
63. elif reminder_infinitive:
64. INFINITIVE = random.choice(cfg['...']['reminder_infinitive'])
65. INFINITIVE = INFINITIVE.format(day, month, year, hours, minutes,
66. reminder_infinitive)
67. result = result + " " + INFINITIVE
68. reminder_table.insert({'time':time, 'kind':'inf',
69. 'msg':reminder_infinitive, 'speaker':speaker})
70. else:
71. # Es wurde nicht angegeben, an was erinnert werden soll
72. NO_ACTION =
73. random.choice(cfg['...']['reminder_no_action'])
74. NO_ACTION = NO_ACTION.format(day, month, year, hours, minutes)
75. result = result + " " + NO_ACTION
76. reminder_table.insert({'time':time, 'kind':'none', 'msg':'',
77. 'speaker':speaker})
78.
79. logger.info("Reminder mit Inhalt {} erkannt.", result)
80.
81. return result
Wie aber fragen wir nun die Datenbank ab, ob eine Erinnerung vorliegt? Sie erinnern sich an 
den Begriff des Callbacks. Einen solchen schreiben wir nun ebenfalls in die intent_reminder.py, 
zu sehen in Listing 6.26.
Damit wir später dynamisch in allen neuen Intents prüfen können, ob diese eine 
Callback-Funktion verwenden, führen wir per Definition ein, dass eine solche immer 
callback heißen und über einen Parameter processed vom Typ Boolean verfügen muss.
Der Parameter processed kann genutzt werden, um dem Callback mitzuteilen, dass ein 
vorheriger Callback erfolgreich bearbeitet wurde. Wann wäre das der Fall? Nun, wenn eine 
Erinnerung ausgesprochen werden konnte. Der Aufruf desselben Callbacks mit processed 
= True sorgt dann dafür, dass die Erinnerung aus der Reminder-Datenbank gelöscht und 
nicht erneut gefunden wird.
Nach dem bekannten Einlesen der Konfiguration werden in Zeile 11 alle gespeicherten Er innerungen ausgelesen und eine nach der anderen auf ein Fälligkeitsdatum geprüft (siehe 
Zeile 17). Ist das der Fall, wird der zu sprechende Erinnerungstext als Template aus der 
Konfiguration gelesen, mit Sprecher, Formulierung und Text befüllt und am Ende als zu spre chender Satz zurückgeliefert. Wichtig ist hier, dass immer nur die jeweils erste Erinnerung 
zurückgeliefert wird. Erst wenn diese abgearbeitet ist, nimmt sich die Callback-Funktion die 



194 6 Intents entwickeln
nächsten Erinnerungen vor, falls diese fällig sind. Findet die Funktion keine aufzurufende 
Erinnerung, gibt sie lediglich None zurück.
Zu erwähnen ist noch Zeile 44, in der der Parameter processed, der dafür sorgt, dass die 
erste gefundene, fällige Erinnerung aus der Datenbank gelöscht wird, auf True geprüft wird.
Listing 6.26 Die Callback-Funktion im reminder() überprüft regelmäßig auf fällige Erinnerungen.
1. def callback(processed=False):
2. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
3. cfg = None
4. with open(CONFIG_PATH, "r", encoding='utf8') as ymlfile:
5. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
6.
7. if not cfg:
8. logger.error("Konnte Callback nicht aufrufen, Konfiguration nicht lesbar.")
9. return None
10.
11. all_reminders = reminder_table.all()
12. for r in all_reminders:
13.
14. parsed_time = parse(r['time'])
15. now = datetime.now(parsed_time.tzinfo)
16. # Ist der aktuelle Datenbankeintrag vor/gleich der jetzigen Zeit?
17. if parsed_time <= now:
18. res = ''
19.
20. # Ist der Sprecher bei Eingabe des Reminders bekannt gewesen?
21. if r['speaker']:
22. res += r['speaker'] + ' '
23.
24. # Wie ist der Satz formuliert worden?
25. if r['kind'] == 'inf':
26. REMINDER_TEXT =
27. random.choice(cfg['...']['execute_reminder_infinitive'])
28. REMINDER_TEXT = REMINDER_TEXT.format(r['msg'])
29. res += REMINDER_TEXT
30. elif r['kind'] == 'to':
31. REMINDER_TEXT = random.choice(cfg['...']['execute_reminder_to'])
32. REMINDER_TEXT = REMINDER_TEXT.format(r['msg'])
33. res += REMINDER_TEXT
34. else:
35. REMINDER_TEXT = random.choice(cfg['...']['execute_reminder'])
36. res += REMINDER_TEXT
37.
38. # Lösche den Eintrag, falls die Erinnerung gesprochen werden konnte
39. if processed:
40. logger.info('Reminder für {} am {} mit Inhalt {} wird nun gelöscht.',
41. r['speaker'], r['time'], r['msg'])
42. Reminder_Query = Query()
43. reminder_table.remove(Reminder_Query.speaker == r['speaker'] and
44. Reminder_Query.timt == r['time'] and Reminder_Query.msg == r['msg']
45. and Reminder_Query.kind == r['kind'])
46. return None



6.3 Erinnerungsfunktion 195
47. else:
48. # Gib den zu sprechenden Satz zurück
49. return res
50.
51. # Kein Reminder gefunden?
52. return None
Und wieder bleibt eine Frage offen: Wie registrieren wir diesen Callback, also wie sorgen 
wir für dessen Aufruf? Das wollen wir zentral im Intent Management abhandeln, weswegen 
wir uns in die Datei intentmgmt.py aus dem Hauptverzeichnis unseres Sprachassistenten 
begeben. In der Klasse IntentMgmt fügen wir nun die Funktion register_callbacks()
hinzu. Wie in Listing 6.27 dargestellt, tut diese nichts anderes, als alle Funktionsordner zu 
durchsuchen (also diejenigen, in der die Logik unserer Intents liegen) und über hasattr()
in die einzelnen Module zu schauen, ob diese über eine Funktion callback() verfügen. Nun 
wird auch klar, warum wir darauf bestanden haben, dass die Funktion callback() heißt, 
denn nur so ist sie eindeutig auffindbar.
Listing 6.27 Registrieren eines Callbacks eines Intents in der Klasse IntentMgmt
1. def register_callbacks(self):
2. # Registriere alle Callback Funktionen
3. logger.info("Registriere Callbacks...")
4. callbacks = []
5. for ff in self.functions_folders:
6. module_name = "intents.functions." + Path(ff).name + ".intent_" +
7. Path(ff).name
8. module_obj = sys.modules[module_name]
9. logger.debug("Verarbeite Modul {}...", module_name)
10. if hasattr(module_obj, 'callback'):
11. logger.debug("Callback in {} gefunden.", module_name)
12. logger.info('Registriere Callback für {}.', module_name)
13. callbacks.append(getattr(module_obj, 'callback'))
14. else:
15. logger.debug("{} hat kein Callback.", module_name)
16. return callbacks
Wurde ein Callback gefunden (Zeile 10), fügen wir dieses in eine Liste ein und liefern diese 
Liste mit allen Callbacks all unserer Intents als Resultat der Funktion register_callbacks()
zurück. Was in anderen Programmiersprachen Funktions-Pointer heißen würde, erlaubt es 
uns, die Callbacks der einzelnen Intents sehr komfortabel aufzurufen.
Lassen Sie uns nun in die main_10_c.py wechseln, um in der __init__()-Methode der 
Klasse VoiceAssistant direkt nach der Initialisierung des Intent Managements das Auslesen 
der Callbacks vorzunehmen.
self.callbacks = self.intent_management.register_callbacks()
Damit haben wir alle Zeiger auf Callback-Funktionen in unserer VoiceAssistant-Instanz vor liegen und können diese nun in der Hauptschleife abfragen. Das tun wir jedoch nur, wenn 



196 6 Intents entwickeln
der Sprachassistent nicht gerade einen Befehl entgegennimmt. Damit Sie sehen, wo genau 
der relevante Code aus Listing 6.28 eingefügt wird, gebe ich ein paar bereits geschriebene 
Zeilen Code an. Ab Zeile 8 erfolgt die besagte Überprüfung, falls derzeit nicht mit dem 
Assistenten gesprochen wird. Es werden in Zeile 10 alle Callbacks auf einen Rückgabewert 
geprüft. Wird ein solcher geliefert, wird weiterhin in Zeile 16 geprüft, ob der Sprachassistent 
gerade spricht. Falls nicht, wird sämtliche möglicherweise gespielte Musik leiser gemacht 
und der Text des Callbacks gesprochen (Zeile 23).
Listing 6.28 In der Hauptschleife des Assistenten, in der Methode run(), wird auf Callbacks geprüft
1. if global_variables.voice_assistant.is_listening:
2. ...
3. # Wird derzeit nicht zugehört?
4. else:
5. # Setze die Lautstärke auf Normalniveau zurück
6. mixer.music.set_volume(global_variables.voice_assistant.volume)
7.
8. # Prozessiere alle registrierten Callback-Funktionen, die die
9. # jeweiligen Intents in jeder Iteration benötigen
10. for cb in global_variables.voice_assistant.callbacks:
11. output = cb()
12.
13. # Gibt die Callback Funktion einen Wert zurück? Dann versuche,
14. # ihn zu sprechen.
15. if output:
16. if not global_variables.voice_assistant.tts.is_busy():
17.
18. # Wird etwas abgespielt? Dann schalte die Lautstärke runter
19. if mixer.music.get_busy():
20. mixer.music.set_volume(0.1)
21.
22. # Spreche das Ergebnis des Callbacks
23. global_variables.voice_assistant.tts.say(output)
24.
25. # Wir rufen dieselbe Funktion erneut auf und geben mit,
26. # dass der zu behandelnde Eintrag abgearbeitet wurde.
27. # Im Falle der Reminder-Funktion wird dann zum Beispiel der Datenbankeintrag
28. # für den Reminder gelöscht
29. cb(True)
30.
31. # Reset Volume
32. if mixer.music.get_busy():
33. mixer.music.set_volume(global_variables.voice_assistant.volume)
Gleich im Anschluss in Zeile 29 wird der Callback nun mit processed=True aufgerufen, 
um die Erinnerung zu löschen. Das können wir getrost tun, wurde diese doch soeben mit geteilt. Damit wären wir fertig! Versuchen Sie sich doch mal nach dem Start des Assistenten 
an Aufrufen wie:
 Erinnere mich in 30 Minuten ans Einkaufen.
 Erinnere mich in 2 Stunden daran, zu bügeln.
 Erinnere mich in 5 Tagen.



6.4 Steuern der Lautstärke 197
All diese Befehle sollte der Assistent nun entgegennehmen. Schauen Sie weiterhin mal in die 
reminder_db.json im Ordner intents\functions\reminder, um zu sehen, wie die Erinnerungen 
abgelegt und ob Sie als Sprecher erkannt werden.
■ 6.4 Steuern der Lautstärke
Wer schon mal auf seinem kommerziellen Sprachassistenten mehrere Stunden am Stück 
Kinderlieder gehört hat, weiß die Funktion der Lautstärkesteuerung zu schätzen. Eine solche 
ist ein elementarer Bestandteil und soll auch in unserer Open-Source-Lösung nicht fehlen. 
Bisher haben wir eine rudimentäre Implementierung für Musik geschrieben. Diese soll 
nun aber auch für Text-To-Speech implementiert werden. Weiterhin soll die Lautstärke in 
der globalen Konfiguration gespeichert werden, sodass sie nicht bei jedem Neustart wieder 
gesetzt werden muss.
Dieses Projekt finden Sie im Repository des Buches im Ordner 10_d_volume. Zu Beginn 
definieren wir in der globalen config.yml unter dem Punkt assistant die Eigenschaft 
silenced_volume: 0.1. Diese Zahl definiert, auf welchen Lautstärkepegel die Ausgabe des 
Sprachassistenten gesetzt werden soll, wenn eine Ausgabe während einer Eingabe stattfindet. 
Bisher haben wir diesen Wert starr auf 0,1 gesetzt, wir möchten ihn nun aber konfigurierbar 
machen, denn er wird in diesem Kapitel noch häufiger Verwendung finden als zuvor. Wir 
möchten die Lautstärkeregelung nicht nur für Musikwiedergabe implementieren, sondern 
auch für generierte Sprache.
In der main Punkt Pei laden wir ja bereits die Lautstärke aus der Konfigurationsdatei. Da wir diese 
allerdings vor Initialisierung der TTS-Komponente benötigen, schieben wir die Zeile 1 von 
Listing 6.29 in der __init__()-Methode etwas weiter nach vorne, nämlich vor die Initiali sierung unserer TTS-Komponente, und lesen im gleichen Zug zusätzlich die silenced_volume
aus, um sie an die Instanz unseres VoiceAssistants zu hängen.
Listing 6.29 Auslesen der Lautstärkepegel aus der Konfigurationsdatei
1. self.volume = self.cfg["assistant"]["volume"]
2. self.silenced_volume = self.cfg["assistant"]["silenced_volume"]
3. self.tts = Voice()
4. self.tts.set_volume(self.volume)
In Zeile 4 rufen wir dann die noch nicht implementierte Funktion set_volume() auf der 
Voice-Instanz auf und übergeben den eben gelesenen Wert.
Da nun die geminderte Lautstärke gesetzt ist (Zeile 2), müssen wir fix alle Vorkommnisse 
von 0.1 in unserer main Punkt Pei durch self.silenced_volume ersetzen. Und um die Lautstärke beim 
Schließen des Assistenten zu speichern, fügen wir in den finally-Block der Methode run()
folgende Zeilen ein.



198 6 Intents entwickeln
Listing 6.30 Speichern der Konfiguration beim Beenden der Anwendung
1. self.cfg["assistant"]["volume"] = self.volume
2. self.cfg["assistant"]["silenced_volume"] = self.silenced_volume
3. with open(CONFIG_FILE, 'w') as f:
4. yaml.dump(self.cfg, f, default_flow_style=False, sort_keys=False)
In Listing 6.30 geschieht nichts anderes, als dass die Variablen, mit denen wir zur Laufzeit 
arbeiten und die wir nach unseren Wünschen setzen und lesen, zurück in das Konfigurations objekt geschrieben werden, nur um dieses dann in den Zeilen 3–4 wieder in der config.yml
zu speichern, sodass es beim nächsten Start von da geladen werden kann. Der Parameter 
default_flow_style gibt an, ob Sammlungen von Unterobjekten im YAML ausgeschrieben oder 
im Block formatiert werden sollen. Da wir bisher Unterelemente nicht im Block vorhalten, 
setze ich default_flow_style hier auf False. Der andere Parameter, sort_keys, gibt lediglich an, 
ob die Namen der Eigenschaften alphabetisch sortiert werden sollen. Auch das ist für uns 
nicht praktikabel, da wir ja thematisch sortieren.
Jetzt ist es an der Zeit, die Lautstärke in der TTS.py anpassbar zu machen, weswegen wir, 
wie in Listing 6.31 zu sehen, die Eigenschaft volume in Zeile 15 einführen. Diese setzen wir 
initial auf 0.5, falls zum Beispiel die Konfiguration aus irgendeinem Grund nicht gelesen werden 
kann, und ändern die Methode __speak__() so ab, dass diese nun zusätzlich einen Wert für 
volume entgegennimmt, den sie sogleich per engine.setProperty() in Zeile 3 der Eigen schaft volume der TTS-Engine zuweist. Denn diese bringt die Funktionalität, eine Lautstärke 
zu setzen, bereits mit.
Beim Aufruf von __speak__() in einem eigenen Prozess in Zeile 20 müssen wir nun diesen 
eben hinzugefügten volume-Parameter auch noch setzen. Abschließend implementieren wir 
die fehlende Methode set_volume() in Zeile 30, die wir eben in der main Punkt Pei bereits auf gerufen haben.
Listing 6.31 Anpassungen an der Voice-Klasse zum Setzen der Lautstärke
1. def __speak__(text, voiceId, volume):
2. engine = pyttsx3.init()
3. engine.setProperty('volume', volume)
4. engine.setProperty('voice', voiceId)
5. engine.say(text)
6. engine.runAndWait()
7.
8. class Voice:
9.
10. def __init__(self):
11. self.process = None
12. self.voiceId =
13. "HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Speech\Voices\Tokens\TTS_MS_DE-
14. DE_HEDDA_11.0"
15. self.volume = 0.5
16.
17. def say(self, text):
18. if self.process:
19. self.stop()



6.4 Steuern der Lautstärke 199
20. p = multiprocessing.Process(target=__speak__,
21. args=(text, self.voiceId, self.volume))
22. p.start()
23. self.process = p
24.
25. def set_voice(self, voiceId):
26. self.voiceId = voiceId
27.
28. ...
29.
30. def set_volume(self, volume=0.5):
31. self.volume = volume
Damit sind die Vorarbeiten erledigt, kommen wir zum eigentlichen Intent.
6.4.1 Intent zur Lautstärkeregelung
Wie bei jedem neuen Intent legen wir zuerst im Ordner intents\functions einen neuen Ord ner an, in diesem Fall mit Namen volume. Darin wiederum legen wir eine requirements.txt
an und fügen die Abhängigkeit text2numde==1.0.0 hinzu, denn wir werden hier den Fall 
haben, dass der Sprachassistent von unserer Speech-To-Text-Engine ausgeschriebene Zahlen 
geliefert bekommt, die wir aber intern in numerische Werte umwandeln müssen, aus einer 
Fünf muss eine 5 werden.
TIPP: text2numde ist meine erste Bibliothek, die ich auf pypi.org veröffentlicht 
habe. Sie sind herzlich dazu eingeladen, sich den Quelltext der Transformationslo gik, denn nichts anderes ist es, auf Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/padmalcom/text2numde
anzuschauen und mir haufenweise Bug-Reports zu stellen, etwa warum Halve 
Hahn nicht korrekt erkannt wird.
Die Konfigurationsdatei (zu sehen in Listing 6.32), die wir als config_volume.yml ebenfalls im 
Ordner intents\functions\volume anlegen, gibt bereits einen ersten Ausblick darauf, welche 
Funktionalitäten wir implementieren möchten. Das sind konkret die Befehle, um
 die Lautstärke um n Einheiten zu erhöhen und zu senken,
 die aktuelle Lautstärke anzusagen,
 und die Lautstärke auf einen konkreten Wert zwischen 0 und 10 zu setzen.
Listing 6.32 Konfigurationsdatei für die Lautstärkeregelung
1. intent:
2. volume:
3. de:
4. invalid_volume: ["Bitte nenne eine Lautstärke zwischen 0 und 10"]
5. volume_up: "lauter"
6. volume_down: "leiser"



200 6 Intents entwickeln
7. volume_is: "Die Lautstärke ist {} von zehn."
8. en:
9. invalid_volume: ["Please set the volume between 0 and 10"]
10. volume_up: "volume up"
11. volume_down: "volume down"
12. volume_is: "Volume is {} out of ten."
Erstellen Sie als Nächstes die intent_volume.py. Da die Lautstärkeregelung eigentlich recht 
simpel ist (lauter, leiser, Lautstärke 3 …), werden wir das ChatbotAI Framework verwenden. 
Wir beginnen mit dem Auslesen der Konfiguration und der Abfrage der aktuellen Lautstärke 
in Listing 6.33.
Listing 6.33 Auslesen der Konfiguration und Abfragen der Lautstärke
1. from loguru import logger
2. from chatbot import register_call
3. import global_variables
4. import yaml
5. import random
6. import os
7. from pygame import mixer
8. import text2numde
9.
10. def __read_config__():
11. cfg = None
12. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
13. config_path =
14. os.path.join('intents','functions','volume','config_volume.yml')
15. with open(config_path, "r", encoding='utf8') as ymlfile:
16. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
17. return cfg, LANGUAGE
18.
19. @register_call("getVolume")
20. def getVolume(session_id = "general", dummy=0):
21. cfg, language = __read_config__()
22. logger.info("Lautstärke ist {} von zehn.",
23. int(global_variables.voice_assistant.volume * 10))
24. return cfg['intent']['volume'][language]['volume_is'].format(
25. int(global_variables.voice_assistant.volume * 10))
Ich habe __read_config__() in eine eigene Methode ausgelagert, da sonst das Lesen der 
Konfiguration in jeder der folgenden vier Methoden wiederkehrend implementiert werden 
müsste. Der effiziente Programmierer ist ja von Natur aus faul, sodass er sich unnötige 
Arbeiten gerne spart. Die Methode gibt sowohl die für den Sprachassistenten global gesetzte 
Sprache als auch das nun schon altbekannte Konfigurationsobjekt zurück.
Der Aufruf von getVolume() in Zeile 20 ist ebenfalls recht überschaubar. Die Konfiguration 
wird gelesen, und ein unter dem Schlüssel volume_is zu findendes Template in Form eines 
Strings wird mit der aktuellen Lautstärke befüllt und zurückgeben, sodass der Sprachassistent 
beispielsweise antworten würde: Die Lautstärke ist fünf von zehn.



6.4 Steuern der Lautstärke 201
Interessant wird es nun beim Setzen der Lautstärke auf einen vom Benutzer kommunizierten 
Wert (Listing 6.34). Hier übergibt das Framework einen Wert volume, der in Zeile 5 auf sein 
Vorhandensein geprüft wird. Ist er vorhanden (etwa bei der Spracheingabe Lautstärke 5), 
wird die Prozedur normal durchlaufen. Ist er aber leer (mit strip() entfernen wir mögliche 
Whitespaces, also Leerzeichen, Tabs und Zeilenumbrüche), dann hat der Benutzer wahr scheinlich nur Lautstärke gesagt, was wir hier so interpretieren, als hätte er die aktuelle 
Lautstärke abgefragt und den Rückgabewert von getVolume() aus der Methode setVolume()
zurückliefern.
Listing 6.34 Setzen der Lautstärke auf einen konkreten Wert
1. @register_call("setVolume")
2. def setVolume(session_id = "general", volume=""):
3. cfg, language = __read_config__()
4.
5. if volume.strip() == "":
6. return getVolume(session_id, 0)
7.
8. # konvertiere das Zahlenwort in einen ganzzahligen Wert
9. if isinstance(volume, str):
10. try:
11. volume = text2numde.text2num(volume.strip())
12. except:
13. return random.choice(cfg['intent']['volume'][language]['invalid_volume'])
14. num_vol = volume
15.
16. # Konnte die Konfigurationsdatei des Intents geladen werden?
17. if cfg:
18.
19. if num_vol < 0 or num_vol > 10:
20. logger.info("Lautstärke {} ungültig, Werte von 0 - 10 erlaubt.", num_vol)
21. return random.choice(cfg['intent']['volume'][language]['invalid_volume'])
22. else:
23. new_volume = round(num_vol / 10.0, 1)
24. logger.info("Setze Lautstärke von {} auf {}.",
25. global_variables.voice_assistant.volume, new_volume)
26. global_variables.voice_assistant.tts.set_volume(new_volume)
27. mixer.music.set_volume(new_volume)
28. global_variables.voice_assistant.num_vol = new_volume
29. return ""
30. else:
31. logger.error("Konnte Konfigurationsdatei für Intent 'volume' nicht laden.")
32. return ""
Wurde aber eine Lautstärke übergeben, dann überprüfen wir zunächst, ob diese als Zeichen kette an die Funktion übergeben wurde (Zeile 9) und nutzen dann text2numde.text2num(), 
um die ausgeschriebene Zahl, zum Beispiel fünf, in eine 5 umzuwandeln. Der try-except-Block erlaubt 
es uns, darauf zu reagieren, wenn text2numde die Zahl nicht interpretieren konnte. Wir geben 
dann entsprechend einen vorgefertigten Satz aus der Konfiguration zurück.
In Zeile 19 erfolgt des Weiteren eine Prüfung, ob sich die Lautstärke in einer erlaubten 
Spannweite bewegt, und wenn das der Fall ist, bilden wir den Zahlenraum 0–10 auf den von 



202 6 Intents entwickeln
0,0–1,0 ab, denn wir erinnern uns, dass sowohl die TTS-Engine als auch der Mixer eine der artige Gleitkommazahl erwarten. Der zweite Parameter in der Funktion round() sagt aus, 
dass auf eine Nachkommastelle gerundet werden soll.
Nun nehmen wir alle nötigen Veränderungen an mixer und TTS-Engine vor und speichern 
die Lautstärke in den globalen Einstellungen, sodass diese beim nächsten Aufruf einer Laut stärkefunktion aktuell sind. Da man in der Regel die Lautstärke ändert, wenn man gerade 
etwas hört, etwa Musik oder einen spannenden Tierlaut, habe ich mich entschieden, keinen 
Text nach einer Änderung auszugeben, denn das würde das Hörvergnügen nur stören.
Kommen wir zur dritten Funktion in diesem Intent, der Erhöhung der Lautstärke. Statt nur 
auf ein Lauter zu reagieren, möchten wir unseren Benutzern auch anbieten, lauter lauter sagen 
zu können, um die Lautstärke um zwei Einheiten zu erhöhen. Die über den Call volumeUp
referenzierte Funktion gleichen Namens nimmt ebenfalls einen Parameter volume entgegen 
und prüft in Zeile 8 von Listing 6.35, ob es sich bei dem übergebenen Wert um einen String 
handelt. In den Zeilen 10 und 11 zählen wir darauf die Anzahl der Vorkommnisse des Be griffs lauter (definiert in der Konfigurationsdatei unter volume_up), indem wir den String 
aufsplitten und per count() zählen.
Listing 6.35 Erhöhen der Lautstärke
1. @register_call("volumeUp")
2. def volumeUp(session_id = "general", volume=None):
3. cfg, language = __read_config__()
4.
5. vol_up = 1
6. if cfg:
7.
8. if isinstance(volume, str):
9. # Erlaube etwas wie "lauter, lauter, lauter"
10. vol_up =
11. volume.split().count(cfg['intent']['volume'][language]['volume_up'])
12.
13. vol = global_variables.voice_assistant.volume
14.
15. new_volume = round(min(1.0, (vol + vol_up / 10.0)), 1)
16. logger.info("Setze Lautstärke von {} auf {}.",
17. global_variables.voice_assistant.volume, new_volume)
18. logger.debug("Setze Lautstärke auf {}.", new_volume)
19. global_variables.voice_assistant.tts.set_volume(new_volume)
20. mixer.music.set_volume(new_volume)
21. global_variables.voice_assistant.volume = new_volume
22. return ""
23. else:
24. logger.error("Konnte Konfigurationsdatei für Intent 'volume' nicht laden.")
25. return ""
Das Konstrukt in Zeile 15 berechnet die Lautstärke und sorgt mittels min() dafür, dass der 
Lautstärkepegel die 1,0 nicht übersteigt. Die Funktion nimmt, ihrem Namen alle Ehre ma chend, jeweils den Minimalwert der zwei ihr übergebenen Parameter. Da das Minimum von 
irgendeiner Zahl größer 1,0 und 1,0 immer 1,0 ist, überschreiten wir nie dieses Limit, auch 



6.4 Steuern der Lautstärke 203
dann nicht, wenn der Benutzer elf Mal hintereinander lauter sagt. Anschließend wird die neue 
Lautstärke an allen relevanten Positionen gesetzt, analog zu Listing 6.34. Das Beispiel zum 
Verringern der Lautstärke erspare ich uns, da dies fast in Gänze der Funktion volumeUp()
entspricht, bloß dass die Lautstärke eben gesenkt wird. Sie können es bei Fragen gerne im 
Repository nachvollziehen.
Was fehlt nun noch zu unserem Glück? Genau, das entsprechende Template, sodass unser 
Intent Management den Befehl auch zu verstehen lernt. Erzeugen Sie ein volume.template im 
Ordner intents\chatbotai und befüllen Sie es entsprechend Listing 6.36.
Listing 6.36 Das Template für den Lautstärke-Intent
1. {% block %}
2. {% client %}(wie ist die lautstärke|wie laut ist es){% endclient %}
3. {% response %}{% call getVolume: 0 %}{% endresponse %}
4. {% endblock %}
5. {% block %}
6. {% client %}(lautstärke|setze lautstärke)(\s*)(?P<volume>.*)?{% endclient %}
7. {% response %}{% call setVolume: %volume %}{% endresponse %}
8. {% endblock %}
9. {% block %}
10. {% client %}lauter.*{% endclient %}
11. {% response %}{% call volumeUp: %0 %}{% endresponse %}
12. {% endblock %}
13. {% block %}
14. {% client %}leiser.*{% endclient %}
15. {% response %}{% call volumeDown: %0 %}{% endresponse %}
16. {% endblock %}
Die Zeilen 1–4 reagieren auf die Nachfrage der aktuellen Lautstärke und rufen die Funktion 
getVolume() mit dem Dummy-Parameter 0 auf. Wir erinnern uns, dass wir hier eine 0 
übergeben, weil ein unparametrisierter Aufruf nicht gestattet ist. Die Zeilen 5–8 setzen die 
Lautstärke auf einen im Parameter volume übergebenen Wert. Wie eben besprochen, kann 
der Befehl Lautstärke auch dazu führen, dass die aktuelle Lautstärke abgefragt wird, falls 
der Parameter volume leer ist. Das jedoch behandeln wir bekanntermaßen in der Logik, nicht 
im Template. Es folgt die Erkennung der Befehle lauter und leiser. Sie sehen am Template, 
dass lediglich ein lauter oder ein leiser erwartet wird und den Funktionen volumeUp() und 
volumeDown() der gesamte Befehl übergeben wird (nämlich die Gruppe 0 des regulären 
Ausdrucks, welche ja immer den gesamten Eingabetext abbildet). Nur so können wir in den 
beiden Funktionen die Anzahl der Vorkommnisse von lauter und leiser auszählen. Damit 
wären wir fertig und können die Lautstärke von Sprache und Audiodatenausgaben flexibel 
per Stimme steuern.



204 6 Intents entwickeln
■ 6.5 Abspielen von Streams
Eines der sicherlich am meisten genutzten Features eines Sprachassistenten ist das Streamen 
von Musik. Ich habe hin und her überlegt, ob ich einen Dienst wie Spotify als Beispiel her nehme, fand den Weg aber für eine Open-Source-Lösung unpassend. Stattdessen möchte ich 
auf das gute, alte Radio zurückgreifen, das ja seit vielen Jahren auch über Internetsender 
erreichbar ist. Konkret schauen wir uns MP3-Streams an, die wir anhand ihres Namens 
aufrufen können. Da Sendernamen eher seltener im Sprachgebrauch einer Speech-To-Text Engine zu finden sind, beschäftigen wir uns praktischerweise in diesem Kapitel auch noch 
mit Fuzzy-Suchen, also Suchen, die eine gewisse Unschärfe zulassen. Als wäre das nicht 
schon genug an interessanten Themen, werden wir unseren Audioplayer gleich noch in 
eine eigene Klasse auslagern, denn unser mixer aus pygame, so lieb wir ihn auch gewonnen 
haben, ist nicht in der Lage, Streams abzuspielen. Das aktuelle Kapitel finden Sie im Ordner 
10_e_music_stream. Installieren Sie die bisher benötigten Abhängigkeiten und führen Sie 
danach noch ein conda install ffmpeg aus. Wer sich mal länger mit Audiobearbeitung auf dem 
PC auseinandergesetzt hat, weiß, dass ffmpeg der Standard für das Bearbeiten, Aufnehmen und 
Wiedergeben von Audio und teilweise auch Video ist. Auch wir werden uns diese fantastische 
Anwendung zunutze machen, um unseren Audioplayer darauf aufzubauen. Nun könnten wir 
die Binärdateien auch einzeln herunterladen und global auf unserem System zu Verfügung 
stellen, doch Anaconda macht es uns viel einfacher, indem es uns erlaubt, ffmpeg per Einzeiler 
nur für unsere Umgebung zu installieren und dort zu Verfügung zu stellen. Die Bibliothek 
ffmpeg-python, die wir per pip installieren und die entsprechend in der requirements.txt des 
Beispiels gelistet ist, bietet uns dann ein Interface, sodass wir aus Python heraus auf die 
Bibliotheken von ffmpeg zugreifen können. Installieren Sie diese entsprechend. Installieren 
Sie zusätzlich noch soundfile und sounddevice.
6.5.1 Die Klasse Audioplayer
Unsere Klasse Audioplayer legen wir im Hauptverzeichnis unserer Anwendung mit dem 
Namen audioplayer.py an. Aufgrund ihrer Komplexität teile ich sie in zwei Hälften. Die erste, 
welche sich mit Initialisierung, Lautstärkeregelung, Stoppen, Zustandsabfragen und dem 
Abspielen von einfachen Audiodateien beschäftigt, sehen Sie in Listing 6.37. Der Import 
von multiprocessing deutet bereits darauf hin, dass wir dafür sorgen werden, dass unser 
Sprachassistent durch die Wiedergabe nicht blockiert wird. Es wäre sicherlich nicht im Sinne 
der Benutzer, wenn man einen Sprachassistenten nach Starten eines Streams nicht unter brechen könnte; da würde nur ein beherzter Griff zum Stecker helfen.Also starten wir die 
Wiedergabe nebenläufig. Die versprochene Verwendung von ffmpeg bleibt in play_file()
und _play_file() noch aus, können wir doch einfach mit soundfile und sounddevice arbei ten, was wesentlich leserlicher ist, als ein stark parametrisierter Aufruf von ffmpeg, wie wir 
ihn gleich sehen werden. play_file() startet einen neuen Prozess und ruft die Funktion 
_play_file() auf, die für die eigentlich Wiedergabe verantwortlich ist. Die Funktion stop()
terminiert den Prozess, is_playing() fragt ab, ob der Prozess noch am Leben ist.



6.5 Abspielen von Streams 205
Listing 6.37 Die Klasse Audioplayer
1. from loguru import logger
2. import time
3. import os
4. import ffmpeg
5. import sounddevice as sd
6. import soundfile as sf
7. import multiprocessing
8. import queue
9.
10. import numpy as np
11.
12. class AudioPlayer:
13.
14. def __init__(self):
15. self._process = None
16. self._volume = 0.5
17.
18. def play_file(self, file):
19. if self._process:
20. self.stop()
21.
22. self._process = multiprocessing.Process(target=self._play_file,
23. args=(file,))
24. self._process.start()
25.
26. def _play_file(self, file):
27. sd.default.reset()
28. data, fs = sf.read(file, dtype='float32')
29. sd.play(data * self._volume, fs, device=sd.default.device['output'])
30. status = sd.wait()
31. if status:
32. logger.error("Fehler bei der Soundwiedergabe {}.", status)
33.
34. def stop(self):
35. if self._process:
36. self._process.terminate()
37.
38. def is_playing(self):
39. if self._process:
40. return self._process.is_alive()
41.
42. def set_volume(self, volume):
43. self._volume = max(0.0, min(volume, 1.0))
44.
45. def get_volume(self):
46. return self._volume
Das Abspielen eines Streams ist etwas komplexer, wobei der Aufruf von play_stream() fast 
identisch zu play_file() ist (siehe Zeile 1 in Listing 6.38). _play_stream() jedoch hat es 
in sich. Wir haben zuerst eine Queue definiert, also eine Warteschlange, die 20 Datenobjekte 
der Reihe nach vorhalten kann. In _play_stream() haben wir nun eine zweite Funktion 
definiert, was syntaktisch erlaubt ist, aber nicht jedermanns Sache ist. Diese trägt den Na 


206 6 Intents entwickeln
men _callback_stream() und ist einer Callback-Funktion vergleichbar mit dem Konstrukt, 
das wir in Abschnitt 6.3 eingeführt haben. Diese Callback-Funktion übergeben wir später in 
Zeile 48 an sd.OutputStream(), das heißt, sie wird aufgerufen, solange der Ausgabestrom 
geöffnet ist und sie tut nichts anderes, als die aktuellen Streaming-Daten in Zeile 20 von der 
Queue zu holen, die entsprechende Lautstärke für die Stream-Wiedergabe zu setzen und sie 
über die Liste outdata an sounddevice zum Abspielen zurückzugeben. Warum müssen wir 
das tun? Nun, ändert der Benutzer während der Wiedergabe eines Streams die Lautstärke, 
soll diese auch gleich Verwendung finden und nicht erst greifen, wenn der Benutzer den 
Stream unterbricht und wieder startet. Diesen Eingriff in den laufenden Betrieb, wie man so 
schön in der IT sagt, können wir hervorragend über einen Callback umsetzen.
Nach der Definition des Callbacks nutzen wir ffmpeg.probe() in Zeile 28, um Informationen 
über den Stream zu sammeln, den wir abspielen möchten. Diese liegen uns als JSON im Ob jekt info vor. Wir stellen darüber sicher, dass dieser nur einen Stream beinhaltet (Zeile 33), 
holen uns diesen Stream und prüfen, ob es sich um einen Audio-Stream handelt (Zeile 38).
Listing 6.38 Abspielen von Streams in der Klasse Audioplayer
1. def play_stream(self, source):
2. if self._process:
3. self.stop()
4. self._process = multiprocessing.Process(target=self._play_stream, args=(source, ))
5. self._process.start()
6.
7.
8.
9. def _play_stream(self, source):
10. sd.default.reset()
11. _q = queue.Queue(maxsize=20)
12. logger.info("Spiele auf Device {}.", sd.default.device['output'])
13.
14. def _callback_stream(outdata, frames, time, status):
15. if status.output_underflow:
16. raise sd.CallbackAbort
17. assert not status
18.
19. try:
20. data = _q.get_nowait()
21. data = data * self._volume
22. except queue.Empty as e:
23. raise sd.CallbackAbort from e
24. assert len(data) == len(outdata)
25. outdata[:] = data
26.
27. try:
28. info = ffmpeg.probe(source)
29. except ffmpeg.Error as e:
30. logger.error(e)
31.
32. streams = info.get('streams', [])
33. if len(streams) != 1:
34. logger.error('Es darf nur genau ein Stream eingegeben werden.')



6.5 Abspielen von Streams 207
35.
36. stream = streams[0]
37.
38. if stream.get('codec_type') != 'audio':
39. logger.error("Stream muss ein Audio Stream sein")
40.
41. channels = stream['channels']
42. samplerate = float(stream['sample_rate'])
43.
44. try:
45. process = ffmpeg.input(source).output('pipe:', format='f32le',
46. acodec='pcm_f32le', ac=channels, ar=samplerate,
47. loglevel='quiet').run_async(pipe_stdout=True)
48. stream = sd.OutputStream(samplerate=samplerate, blocksize=1024,
49. device=sd.default.device['output'], channels=channels, dtype='float32',
50. callback=_callback_stream)
51.
52. read_size = 1024 * channels * stream.samplesize
53. for _ in range(20):
54. data = np.frombuffer(process.stdout.read(read_size), dtype='float32')
55. data.shape = -1, channels
56. _q.put_nowait(data)
57. logger.info("Starte Stream...")
58. with stream:
59. timeout = 1024 * 20 / samplerate
60. while True:
61. data = np.frombuffer(process.stdout.read(read_size), dtype='float32')
62. data.shape = -1, channels
63. _q.put(data, timeout=timeout)
64. except queue.Full as e:
65. logger.error("Streaming-Queue ist voll.")
66. except Exception as e:
67. logger.error(e)
Nach ein paar weiteren Abfragen von Channels und der Sample Rate des Radio-Streams 
folgt der wie versprochen etwas klobige Aufruf von ffmpeg in den Zeilen 45–47. Als Eingabe 
nimmt das Framework die Quelle als URL entgegen und leitet dann die Ausgabe ein. Das 
pipe: deutet darauf hin, dass der Audiostream über die Standardausgabe (stdout) geleitet 
wird. Nur so können wir den Stream in Zeile 61 wieder über frombuffer() abgreifen und 
ihn gestückelt auf die Queue legen (Zeile 63). Die Alternative wäre, ihn statt in stdout in eine 
Datei zu schreiben, aber das ist ja nicht unser Ziel. Der Befehl put() wird um den Parameter 
timeout ergänzt, um einen Fehler zu produzieren, wenn die Queue lange Zeit voll ist und 
keine Daten davon abgerufen werden. In diesem Fall erfolgt die Fehlermeldung in Zeile 65. 
Die eigentliche Ausgabe, die aus unserem Lautsprecher kommt, wird in Zeile 48 initiiert, 
indem wir festlegen, dass die Daten dafür aus der Methode _callback_stream() kommen. 
Aus welchen Lautsprechern genau? Aus den für unser System festgelegten Standardlautspre chern, die wir in Zeile 49 über sd.default.device['output'] wählen. In der For-Schleife 
in Zeile 53 befüllen wir die Queue initial schon mal mit 20 Einheiten des Audiostreams, um 
später einer Unterversorgung vorzubeugen, was zu kurzen Pausen führen würde. Es folgt 
eine Endlosschleife in Zeile 60, die nun so lange Abschnitte auf die Queue stellt, die aus 



208 6 Intents entwickeln
dem stdout des ffmpeg-Prozesses gelesen werden, bis der Prozess über _play_stream() per 
terminate() unterbrochen wird.
Den Audioplayer sollten wir nun testen, bevor wir den mixer aus unserer Anwendung aus bauen. Dazu erstellen wir einen manuellen Test in der neuen Datei test.py im Hauptverzeichnis 
unserer Anwendung. Wir verifizieren damit das Abspielen von Streams, von Audiodateien, 
das Unterbrechen eines Abspielvorgangs und das Ändern der Lautstärke. Alles zusammen 
sehen Sie in Listing 6.39.
Listing 6.39 Testen des Audioplayers
1. from audioplayer import AudioPlayer
2. import time
3.
4. if __name__ == '__main__':
5.
6. ap = AudioPlayer()
7. try:
8. ap.set_volume(1.0)
9. ap.play_stream("Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichst01.dlf.de/dlf/01/64/mp3/stream.mp3")
10. #ap.play_file(r"functions\animalsounds\animals\cat.ogg")
11. time.sleep(5)
12. ap.set_volume(0.1)
13. ap.stop()
14. #ap.play_stream("Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichst01.dlf.de/dlf/01/64/mp3/stream.mp3")
15. ap.play_file(r"intents\functions\animalsounds\animals\cat.ogg")
16. time.sleep(4)
17. ap.stop()
18. finally:
19. if ap:
20. ap.stop()
In Zeile 6 erstellen wir zunächst eine Instanz der Klasse AudioPlayer, setzen die Lautstärke 
auf 1,0 (was der vollen Lautstärke entspricht, also Vorsicht) und spielen den Stream von 
Deutschlandfunk. Diesen unterbrechen wir nach 5 Sekunden, setzen die Lautstärke auf 0,1 
und spielen das Geräusch einer Katze, das wir nach vier Sekunden ebenfalls wieder unter brechen. Funktioniert alles? Wunderbar, damit sind wir hier fertig.
6.5.2 Integration des AudioPlayer
Es folgt die etwas undankbare und repetitive Aufgabe, den pygame.mixer durch unseren 
AudioPlayer zu ersetzen. Das muss nicht nur in der main Punkt Pei geschehen, sondern auch in den 
Intents, in den wir Audiodateien oder -streams abspielen. In den folgenden Sektionen zeige 
ich auf, wo die Änderungen vorzunehmen sind. Die Zeilen mit einem Minus am Beginn sind 
durch die mit einem Plus am Beginn zu ersetzen.



6.5 Abspielen von Streams 209
main Punkt Pei
Wir beginnen mit dem Import und der Initialisierung in der main Punkt Pei.
- from pygame import mixer
+ from audioplayer import AudioPlayer
Es folgt die eigentliche Initialisierung.
- mixer.init()
- mixer.music.set_volume(self.volume)
+ self.audio_player = AudioPlayer()
+ self.audio_player.set_volume(self.volume)
In der Methode run() erfolgt nun die Anpassung der Koordination der Audiowiedergabe in 
der Hauptschleife der Anwendung. Zunächst im Falle, dass der Assistent gerade auf einen 
Befehl hört und die Lautstärke reduziert werden muss, sowie wenn der Text eines Callbacks 
gesprochen und die Musik leiser gestellt werden muss.
- if mixer.music.get_busy():
- mixer.music.set_volume(self.silenced_volume)
+ if self.audio_player.is_playing():
+ self.audio_player.set_volume(self.silenced_volume)
Ebenso muss das Zurücksetzen auf Normallautstärke über den AudioPlayer erfolgen.
- mixer.music.set_volume(self.volume)
+ self.audio_player.set_volume(self.volume)
Zuletzt müssen wir im finally-Block den AudioPlayer unterbrechen, um die Wiedergabe 
sauber zu beenden.
+ if self.audio_player is not None:
+ self.audio_player.stop()
Es folgt die Anpassung der Intents aus den Unterordnern von intents\functions.
intent_animalsound.py
Im Intent Animal Sound sind folgende Änderungen vorzunehmen. Einen gesonderten Im port für den AudioPlayer benötigen wir nicht, da wir diesen ja global initialisieren und über 
global_variables.voice_assistant referenzieren.
- from pygame import mixer
- if mixer.music.get_busy():
- mixer.music.stop()
- mixer.music.load(ogg_file)
- mixer.music.play()
+ if global_variables.voice_assistant.audio_player.is_playing():
+ global_variables.voice_assistant.audio_player.stop()
+ global_variables.voice_assistant.audio_player.play_file(ogg_file)



210 6 Intents entwickeln
intent_stop.py
Im Intent Stop gilt es, die Unterbrechung auf den AudioPlayer umzumünzen.
- from pygame import mixer
- if mixer.music.get_busy():
- mixer.music.stop()
+ if global_variables.voice_assistant.audio_player.is_playing():
+ global_variables.voice_assistant.audio_player.stop()
intent_volume.py
Auch den Intent, der die Lautstärke steuert, den wir eben in mühevoller Kleinarbeit entwickelt 
haben, müssen wir nun anpassen, indem wir den Import von pygame.mixer entfernen und 
in den drei Methoden setVolume(), volumeUp() und volumeDown() das Setzen der Laut stärke anpassen.
- from pygame import mixer
- mixer.music.set_volume(new_volume)
+ global_variables.voice_assistant.audio_player.set_volume(new_volume)
6.5.3 Der Intent Webradio
Wie schon angemerkt, wird eine der größten Herausforderungen in diesem Kapitel sein, dass 
Radiosender für eine STT-Engine schwer zu verstehende Namen haben, wenn diese nicht 
explizit darauf trainiert ist – was bei uns der Fall ist. WDR 2 oder DLF sind eine etwas grö-
ßere Herausforderung in unserem Setup, der wir aber mit einigen Tricks begegnen können. 
Lassen Sie uns jedoch zunächst einmal den Datensatz für Snips-NLU anlegen, sodass klar 
wird, wie der Intent später zu bedienen ist. Dafür erzeugen wir die Datei radio_dataset.yaml
im Ordner intents\snips-nlu.
Zunächst beginnen wir in Listing 6.40 mit der Anlage einer Entität radio_station, der wir 
einige Beispielradiosender mitgeben, zum Beispiel Deutschlandfunk oder Bremen Eins.
Eins ist keine große Herausforderung, Abkürzungen wie WDR 3 oder RPR 1 sind es hin gegen schon. Hier ist es wichtig, dass Abkürzungen mit einem Leerzeichen versehen und 
die dazugehörigen Zahlen ausgeschrieben werden müssen. Letzteres haben wir ja bereits 
bei den Datumswerten erfahren und entsprechend beherzigt.
Listing 6.40 Schema und Beispiele für den Webradio Intent
1. type: entity
2. name: radio_station
3. automatically_extensible: true
4. use_synonyms: false
5. matching_strictness: 0.8
6. values:
7. - deutschlandfunk
8. - n-joy



6.5 Abspielen von Streams 211
9. - bremen eins
10. - fritz
11. - regenbogen
12. - bonn rhein sieg
13. - Radio Neunundachzig null
14. - r p r eins
15. - m d r aktuell
16. - h r 1
17. - n d r zwei
18. - m d r eins
19. - w d r drei
20.
21. ---
22. type: intent
23. name: musicstream
24. utterances:
25. - Spiele [station:radio_station](r p r eins)
26. - Spiele Radiosender [station:radio_station](n d r 2)
27. - Radio [station:radio_station](Regenbogen)
28. - Starte [station:radio_station](Bonn Rhein Sieg)
29. - Streame [station:radio_station](w d r eins)
Es folgt der Intent musicstream ab Zeile 22, für den wir eine Hand voll Beispielsätze formulie ren und jeweils auch ein Beispiel für einen Radiosender mitgeben. Dadurch stellt uns Snips NLU, wie auch in den vorherigen Intents, den Sendernamen über die Variable station bereit.
Machen wir uns nun daran, die Logik zu implementieren. Dazu erstellen Sie im Ordner 
intents\functions einen Ordner musicstream und darin zunächst eine requirements.txt mit 
dem Inhalt aus Listing 6.41.
Listing 6.41 Abhängigkeiten für den Intent Webradio
fuzzywuzzy==0.18.0
text2numde==1.0.0
sounddevice==0.4.1
Die Bibliotheken text2numde und sounddevice kennen wir bereits und müssten sie theoretisch 
nicht mehr mit aufnehmen. Da wir aber unsere Intents autark aufsetzen möchten, sodass 
sie von keinem anderen Intent abhängig sind, nehme ich alle notwendigen Bibliotheken 
redundant auf. Und wenn es nur dafür ist, dass der Intent losgelöst vom Sprachassistent 
getestet werden kann. Zur Abhängigkeit mit dem wunderbaren Namen fuzzywuzzy kommen 
wir gleich noch.
Bevor wir uns in die Tiefen der Python-Entwicklung stürzen, legen wir noch schnell die 
Konfiguration für den Intent an, analog zu Listing 6.42. Diese speichern wir in der Datei 
config_musicstream.yml im eben erzeugten Ordner musicstream.



212 6 Intents entwickeln
Listing 6.42 Konfiguration für den Intent Webradio
intent:
 musicstream:
 de:
 unknown_station: ["Den Radiosender kenne ich nicht."]
 en:
 unknown_station: ["I don't know that station."]
 stations:
 "Deutschlandfunk": "Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichst01.dlf.de/dlf/01/64/mp3/stream.mp3"
 "HR 1": "Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichhr-hr1-live.cast.addradio.de/hr/hr1/live/mp3/128/stream.mp3"
 "MDR aktuell": "Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichmdr-284340-0.cast.mdr.de/mdr/284340/0/mp3/high/stream.mp3"
 "N-JOY": "Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichndr-njoy-live.cast.addradio.de/ndr/njoy/live/mp3/128/stream.mp3"
 "Fritz": "Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichfritz.de/livemp3"
 "WDR 1": "Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrichwdr-1live-live.icecast.wdr.de/wdr/1live/live/mp3/128/stream.mp3"
 "RPR 1": "Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichstreams.rpr1.de/rpr-simulcast-128-mp3"
Nichts, vor dem man sich fürchten müsste. Zuerst definieren wir eine Liste möglicher Sätze, 
die gesprochen werden, wenn der Radiosender nicht verstanden wurde, dann folgt eine Liste 
von Schlüsselwertpaaren bestehend aus dem Namen eines Radiosenders und der dazugehö-
rigen URL des Streams.
TIPP: Ist der Radiosender Ihres Vertrauens nicht dabei? Dann werfen Sie einen Blick 
auf die Liste von Hendrik Jansen, der eine beeindruckende Sammlung auf seiner 
Website zusammengetragen hat: Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichhendrikjansen.nl/henk/streaming3.html
Der Intent selber, zu sehen in Listing 6.43, ist nun gar nicht so komplex, wie man vielleicht 
vermuten mag. Wir definieren die Funktion musicstream mit dem Parameter station und 
lesen wie gewohnt die Konfigurationsdatei, die wir zuvor angelegt haben. Wird kein oder ein 
leerer Sender übergeben (Zeile 31), dann geben wir einen zufälligen Eintrag aus der Liste 
der Sätze unter unknown_station aus der Konfiguration zurück.
Listing 6.43 Logik des Intents Webradio
1. from loguru import logger
2. from chatbot import register_call
3. import global_variables
4. import random
5. import os
6. import yaml
7.
8. import text2numde
9. from fuzzywuzzy import fuzz
10.
11. def musicstream(station=None):
12.
13. config_path =
14. os.path.join('intents','functions','musicstream','config_musicstream.yml')
15. cfg = None



6.5 Abspielen von Streams 213
16.
17. with open(config_path, "r", encoding='utf8') as ymlfile:
18. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
19.
20. if not cfg:
21. logger.error("Konnte Konfigurationsdatei nicht lesen.")
22. return ""
23.
24. # Holen der Sprache aus der globalen Konfigurationsdatei
25. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
26.
27. # Meldung, falls der Sender nicht gefunden wurde
28. UNKNOWN_STATION =
29. random.choice(cfg['intent']['musicstream'][LANGUAGE]['unknown_station'])
30.
31. if (station == None) or (station == ""):
32. return UNKNOWN_STATION
33.
34. # Radiosender haben häufig Zahlen in den Namen, weswegen wir für einen
35. # besseren Abgleich Zahlenwörter in Zahlenwerte umwandeln.
36. station = text2numde.sentence2num(station)
37.
38. # Wir entfernen alle Whitespaces, denn das Buchstabieren in VOSK bringt
39. # pro Buchstabe eine Leerstelle mit sich.
40. station = "".join(station.split())
41.
42. station_stream = None
43. for key, value in cfg['intent']['musicstream']['stations'].items():
44.
45. # Wir führen eine Fuzzy-Suche aus, da die Namen der Radiosender nicht immer
46. # perfekt von VOSK erkannt werden.
47. ratio = fuzz.ratio(station.lower(), key.lower())
48. logger.info("Übereinstimmung von {} und {} ist {}%", station, key, ratio)
49. if ratio > 70:
50. station_stream = value
51. break
52.
53. # Wurde kein Sender gefunden?
54. if station_stream is None:
55. return UNKNOWN_STATION
56.
57. global_variables.voice_assistant.audio_player.play_stream(station_stream)
58.
59. # Der Assistent muss nicht sprechen, wenn ein Radiostream gespielt wird
60. return ""
Wird ein Radiosender in dem Befehl erkannt, nutzen wir zunächst die Funktion sentence2num(), 
um alle Zahlen eines Satzes von einer ausgeschriebenen Form in einen numerischen Wert 
umzuwandeln, denn so haben wir sie ja richtigerweise in unserer Konfiguration hinterlegt. 
Dann trennen wir über station.split() den Radiosender in Buchstaben auf und fügen 
ihn ohne Leerzeichen über join() wieder zusammen. Das hat den Zweck, dass wir die neue 
Zeichenkette besser gegen die Liste der Namen aus unserer Konfiguration abgleichen können. 
Ein Beispiel für diesen ersten Schritt sähe so aus:



214 6 Intents entwickeln
R P R Eins → RPR1
Und hier sind wir auch schon beim Stichwort abgleichen, denn das ist der wahrscheinlich 
spannendste Teil in diesem Abschnitt. Mit fuzz.ratio() können wir einen Übereinstim mungsgrad zwischen zwei Zeichenketten ermitteln. Wir prüfen nicht, ob sich zwei Strings 
gleichen, sondern zu welchem Prozentsatz. Wird nun ein Radiosender angegeben, vergleichen 
wir ihn über fuzzywuzzy mit jedem uns bekannten Radiosender und sollte eine Überein stimmung von über 70 % ermittelt werden, dann wählen wir diesen Sender aus. Die 70 % 
sind übrigens kein wissenschaftlich ermittelter Wert, sondern wurden von mir nach bestem 
Wissen und Gewissen festgelegt.
Wurde kein Radiosender mit einem passenden Namen gefunden, wird genau das in Zeile 55 
zurückgemeldet. Andernfalls spielen wir den Stream mit unserer fantastischen AudioPlayer Klasse in Zeile 57 ab. Eine gesprochene Rückmeldung ist bei erfolgreichem Start des Streams 
nicht mehr notwendig, weswegen wir einen leeren String zurückliefern.
Bild 6.33 bewertet die Übereinstimmung beim Vergleich der Zeichenkette rpr1, die aus dem 
von VOSK verstandenen r p r eins aufbereitet wurde, und dem tatsächlichen Namen RPR 1
mit 89 %. Der Vergleich mit deutschlandfunk erbrachte hingegen 0 % und der mit fritz 22 %. 
Das Vergleichen zweier Strings mit einer gewissen Unschärfe kann uns das Fine Tuning 
unserer STT-Engine ersparen. Natürlich nur, weil unser Anwendungsfall es hier zulässt.
Bild 6.33 Interpretation und Fuzzy Matching der Radiosender
6.5.4 Fuzzylogik
Um dem Anspruch eines Fachbuchs gerecht zu werden, möchte ich nun mit Ihnen noch 
etwas tiefer in die Materie der Fuzzylogik einsteigen und schauen, ob wir eventuelle Ge meinsamkeiten zu KI finden. In der klassischen Informatik sind wir es gewohnt, zwischen 
zwei Zuständen zu unterscheiden, die entweder 1 oder 0, True oder False, sein können. Diese 
werden durch ein Bit abgebildet.
TIPP: Wenn von klassischer Informatik die Rede ist, gibt es dann auch eine nicht 
klassische oder moderne Informatik? Zwar ist Quantum Computing nicht als sol ches definiert, dennoch revolutionieren dessen Ansätze den Umgang mit Daten 
und Rechenoperationen in der Informationstechnologie ungemein. Auch wenn das 
Thema bereits seit 30 Jahren existiert.
Ein Computer besteht aus einer Vielzahl von Schaltkreisen, die wiederum aus Bauele menten, wie Dioden, Transistoren oder Widerständen bestehen, die mit bloßem Auge 
längst nicht mehr zu erkennen sind, weil sie durch ausgefeilte Fertigungstechniken



6.5 Abspielen von Streams 215
immer kleiner werden. Seit einigen Jahren stößt man aber bei der Herstellung auf 
physikalische Grenzen, die besonders bei der Verwendung von Transistoren eine 
Rolle spielen.
Transistoren können über ein vorgehaltenes Spannungspotenzial genau zwei Zu stände speichern, also ein Bit, 1 oder 0, abbilden. Durch deren Verknüpfung in 
Logikgattern können logische Operatoren wie AND, OR, XOR oder NAND abgebil det werden, die Sie in Tabelle 6.3 sehen. Der logische Operator OR ist zum Beispiel genau 
dann 1, wenn einer der eingegebenen Bits 1 ist, wohingegen AND 1 ist, wenn bei de eingegebenen Bits 1 sind.
Tabelle 6.3 Logische Operatoren in einem binären System
Eingabe A Eingabe B AND OR XOR NAND
0 0 0 0 0 1
0 1 0 1 1 1
1 0 0 1 1 1
1 1 1 1 0 0
Verknüpft man wiederum logische Operatoren, können komplexere mathemati sche Funktionen abgebildet werden, wie etwa die Addition über ein sogenanntes 
Addierwerk, eines der zentralen Komponenten einer CPU, das, anders als sein 
Name vermuten lässt, auch in der Lage ist, eine Subtraktion, Multiplikation und 
Division abzubilden.
Zurück zu den Transistoren: Wo liegt denn jetzt dieses physikalische Limit? 
Ab einer Transistorgröße von 10 Nanometern abwärts ist der sogenannte Tunnel effekt zu beobachten, an dem Elektronen eine Barriere im geschlossenen Tran sistor passieren, die sie nicht passieren dürften. Dieser Effekt heißt quantenme chanischer Effekt und wird dank einer beeindruckenden, positiven Einstellung der 
Wissenschaft als Potenzial für die Entwicklung von Quantencomputern gesehen.
In einem Quantencomputer werden nun nicht mehr Bits, sondern Qubits verwen det, die neben den exklusiven Zuständen 1 oder 0 auch beide Zustände gleich zeitig annehmen können. Der Knackpunkt ist, dass dieser Zustand nur so lange 
anhalten kann, wie das Qubit unbeobachtet ist. Wird es beobachtet, löst sich die ser Zustand mit einer konkreten Wahrscheinlichkeit zu 1 oder 0 auf, sodass zum Beispiel 
der Zustand 1 mit einer Wahrscheinlichkeit von 12 % und der Zustand 0 mit einer 
Wahrscheinlichkeit von 88 % zutrifft. Die Fähigkeit, zwei sich überlagernde Zustän de anzunehmen, nennt sich Superposition. Falls Sie jetzt vermuten, dass wir nach 
dem kurzen Ausflug in die Welt der Quantencomputer Parallelen zu der Fuzzylogik 
feststellen werden, liegen Sie richtig.
Ein Beispiel soll jedoch zuvor verdeutlichen, was diese Fähigkeit für Vorteile mit 
sich bringt. Nehmen wir an, wir hätten zwei Bits, mit denen wir die Werte 00, 
01, 10 oder 11 abbilden können. Mit zwei Qubits können wir nun alle vier Zahlen 
gleichzeitig abbilden. Das erste Qubit bildet dabei den Binärwert an erster, das



216 6 Intents entwickeln
zweite den an zweiter Position ab. Jedoch können wir nicht nur einen größeren 
Informationsraum abbilden, sondern durch die Superposition auch mehrere paral lele Rechenoperationen ausführen – weiterhin nur, solange das Qubit noch nicht 
gemessen und so in einen konkreten Zustand von 0 oder 1 überführt wurde.
Neben der angesprochenen Superposition kommt weiterhin die Verschränkung 
(Entanglement) zum Tragen, in dem zwei Qubits, die miteinander verschränkt 
wurden, ihr Verhalten dahingehend ändern, dass sie agieren, als würden sie eine 
ortsunabhängige Verbindung eingegangen sein. Misst man nun den Zustand eines 
Qubits, erfährt man gleichzeitig den Zustand seines verschränkten Partners.
Nun gibt es analog zu den Logikgattern auch Quantengatter, die das Rechnen mit 
Qubits ermöglichen. Und hier offenbart sich einer der größten Vorteile der Quan tenmechanik.
 Hadamard Gatter: Erzeugt eine Superposition aus einem Qubit.
 Pauli X Gatter: Dreht den Zustand eines Qubits um.
 Phase Shift Gatter: Tauscht die Wahrscheinlichkeiten eines Qubits, die bei Messung 
des Zustands für dessen Binärwert ermittelt werden.
 CNOT (Controlled Not) Gatter: Verschränkt zwei Qubits und invertiert das Target
Qubit, wenn das Control Qubit 1 ist.
Vielleicht ziehen Sie an dieser Stelle die Augenbrauen hoch, weil Sie doch etwas 
über Sprachassistenten und nicht über Quantengatter lernen wollten. Ich komme 
gleich zum Punkt, hierfür wollte ich aber vorher klarstellen, dass sich mit Quanten gattern dieselben Operationen durchführen lassen wie mit Logikgattern. Bloß dass 
alle möglichen Berechnungen auf Basis der Superposition gleichzeitig durchge führt werden können. Bild 6.34 zeigt dies exemplarisch an einer CNOT-Schaltung. 
Die Notation, bestehend aus einem senkrechten Strich, einem Symbol und einem 
Pfeil nach rechts, ist Teil der Dirac-Notation, manchmal auch als Bra-Ket-Notation 
bezeichnet, und weist darauf hin, dass es sich um einen Quantenzustand handelt.
x
x
y XOR y x
a)
|0 |0
|1 |1
0 0
0 XOR 0
b)
|x |x
|y |y x
|0 |0
|0 |0
|1 |1
|0 |1
|1 |1
|1 |0
Bild 6.34 a) zeigt das CNOT-Gatter als Beispiel analog zum logischen umgekehrten 
XOR-Gatter oben in b). Die unteren, farbig hervorgehobenen Bilder zeigen alle gleichzeitig 
durchführbaren Berechnungen für Quanten- und Logikgatter.



6.5 Abspielen von Streams 217
Nun wollte ich eigentlich nur verdeutlichen, dass im Quantum Computing ebenso 
wie in der Fuzzylogik Zustände herrschen können, die nicht binär auszudrücken 
sind und da stehen wir und reden über Quantengatter und deren Fähigkeiten. Das 
ist aber gar nicht so schlimm, denn so bekomme ich eine elegante Überleitung 
zum Quantum Machine Learning (QML) hin. QML erlaubt es Wissenschaftlerinnen, 
klassische Machine-Learning-Algorithmen in eine Folge von Quantengattern (eine 
Quantenschaltung) zu übersetzen, sodass diese effizient auf einem Quantencom puter berechnet werden können. Auch wenn Quantencomputer auf der Welt noch 
rar gestreut sind, entwickeln sich erste Frameworks, zum Beispiel TensorFlow Quantum
(TFQ) von Google, die sich den neuen Herausforderungen annehmen.
Und da das Ihnen vorliegende Werk ja ein Praxisbuch sein soll, wollen wir uns zum 
Schluss noch anschauen, wie wir die Schaltung aus Bild 6.34 in Python umsetzen. 
Zwar setzen wir hier lediglich das Framework CIRQ ein und nicht TFQ, jedoch bil det Google CIRQ die Grundlage dessen und ist somit ein valider Einstiegspunkt. Zu 
Beginn legen wir in Listing 6.44 zwei Qubits mit den Bezeichnern a und b an, die 
wir in q1 und q2 speichern. In den Zeilen 7–11 folgt der Schaltkreis, bestehend 
aus dem oben angeführten Hadamard Gatter, das zunächst das Qubit q1 in den 
Zustand der Superposition überführt. Damit hat q1 eine Fifty-fifty-Chance, 1 oder 
0 zu betragen – wohlgemerkt vor dessen Messung. Es folgt wie angekündigt das 
CNOT-Gatter mit q1 als Kontroll-Qubit, das es auf seinen Zustand zu überprüfen 
gilt, und q2 als Ziel-Qubit. Mit der Anwendung dieses Gatters sind q1 und q2 da mit verschränkt. Zuletzt müssen wir die beiden Qubits über einen Messvorgang 
in Zeile 10 in einen konkreten Zustand überführen, sodass wir diese später in der 
Messung ausgeben können.
Listing 6.44 Umsetzung des CNOT-Gatters in Google CIRQ
1. import cirq
2.
3. q1 = cirq.NamedQubit(‚a‘)
4. q2 = cirq.NamedQubit(‚b‘)
5.
6. # Lege den Schaltkreis an
7. circuit = cirq.Circuit(
8. cirq.H(q1),
9. cirq.CNOT(control=q1, target=q2),
10. cirq.measure(q1, q2)
11. )
12. print(„Circuit:“)
13. print(circuit)
14.
15. # Starte die Simulation
16. simulator = cirq.Simulator()
17. result = simulator.run(circuit, repetitions=20)
18. print(„Results:“)
19. print(result)



218 6 Intents entwickeln
Wenn Sie sich den Schaltkreis wie in Zeile 13 gezeigt ausgeben lassen, sehen sie 
diesen in der Konsole (Bild 6.35). Das H symbolisiert das Hadermard-Gatter, das @
und das X das CNOT-Gatter und die Ms die Messpunkte. In Zeile 16 erzeugen wir 
nun einen Simulator, den wir in Zeile 17 über run() aufrufen, und das 20-mal in 
Folge. Das Ergebnis zeigt für a und b die ausgegebenen Werte jedes Durchlaufs.
Bild 6.35 Ausgabe des Schaltkreises und der 20 generierten Messergebnisse
Prima, wir haben einen Schaltkreis entwickelt und getestet, wie hilft uns Machine 
Learning hier nun weiter? Immerhin ist Machine Learning ja nicht dafür da, das 
Verhalten eines Systems exakt zu simulieren, wie es CIRQ hier tut. Stattdessen 
kann es das Verhalten eines Systems erlernen und dessen Ausgabe vorhersagen. 
Und das ist genau der Mehrwert. Statt auf noch selten verfügbare, teure7
 Quan tencomputer zurückzugreifen, Schaltkreise physisch zu bauen und Experimente 
durchzuführen, wird das Verhalten eines Quantenmodells erlernt, um so Vorher sagen über dessen zukünftiges Verhalten unter Eingabe unbekannter Daten mög lichst genau treffen zu können. Damit lassen sich grobe Fehler von vornerein aus schließen, neue Algorithmen erproben und die Eingabedatenqualität verbessern, 
ohne dass zuvor ein Quantencomputer in Anspruch genommen werden muss.
uantum Machine Learning war. Fuzzylogik lässt sich auch als Unschärfelogik übersetzen 
und wurde von dem Mathematiker Lotfi Zadeh als die präzise Erfassung des Unpräzisen be zeichnet. Anders als die boolesche Logik, die, wie wir bereits erfahren haben, zwischen 0 
und 1 unterscheidet, gibt die Fuzzylogik einen Grad zwischen 0 und 1 an. Warum sollte man 
so etwas tun wollen? Nun, als Mensch nehmen wir viele Dinge subjektiv wahr: Jan ist groß, 
Elisabeth ist schlau und Sarah kann gut kochen. Wie würden Sie diese Eigenschaften mit 
den Werten 0 und 1 beschreiben? Könnten Sie das überhaupt? Schließlich haben Sie ja keine 
Referenz. Auch wenn Jan groß ist, gibt es auf der Welt sicher Millionen von Menschen, die 
noch größer sind. Und wann wäre Jan als klein zu bezeichnen? Wenn er einen Zentimeter 
schrumpft? Oder anders gefragt: Ist ein Mensch schon klein, wenn er nur einen Millimeter 
kleiner wäre, als Jan? In der booleschen Logik wäre das der Fall. In der Fuzzylogik können 
wir derartige Eigenschaften viel genauer abbilden.
7 Für einen Quantencomputer von IBM mit 27 Qubits sind Stand 2021 Monatsmieten von etwa 11.000 € im Ge spräch.



6.5 Abspielen von Streams 219
HINWEIS: Schauen Sie noch einmal in Listing 6.43, in dem wir genau vor dieser 
Frage standen. Wurde der Name eines Radiosenders, den der Benutzer gerne 
abspielen möchte, erkannt? Und wenn ja, mit welcher Wahrscheinlichkeit? 
Wir haben dort einen Schwellenwert von 70 % festgelegt, um zu bestimmen, wann 
fuzzywuzzy einen Radiosender wohl richtig zugeordnet hat. Hier sind wir nämlich 
gezwungen, von dem Unschärferaum auf den binären Raum zu mappen, denn die 
Bedingung, die prüft, ob der Radiosender nun gespielt werden soll oder nicht, eva luiert ausschließlich Binärwerte.
Bild 6.36 illustriert den Unterschied der beiden Betrachtungsweisen über ein Glas, das mit 
Wasser gefüllt ist. Die boolesche Logik erlaubt es auszudrücken, ob ein Glas leer oder voll 
ist. Mittels Fuzzylogik können wir auch die Zustände zwischen einem leeren und einem 
vollen Glas beschreiben, wobei es zwischen einem leeren und einem vollen Glas unendlich 
viele weitere Zustände geben könnte. Verbal beschreiben wir diese Zustände häufig über 
sogenannte Heckenausdrücke, wie recht voll oder ziemlich voll, die angeben, in welchem Maße 
eine bestimmte Charakteristik eines Sachstands ausgeprägt ist.
Boolesche Logik Fuzzylogik
Ist das Glas leer?
Ja Nein
Ist das Glas leer?
Leer Fast leer Halb voll Fast voll Voll
Bild 6.36 Fuzzylogik erlaubt einen Zustand genauer zu beschreiben, als es die boolesche Logik vermag.
Lassen Sie uns zum Abschluss noch einmal praktisch anschauen, wie wir die Unterschiede 
zweier Zeichenketten mit fuzzy bewerten, so wie wir es bei der Erkennung der Radiosender 
tun. Als Grundlage dafür soll uns die Levenshtein-Distanz dienen.
6.5.5 Die Levenshtein-Distanz
Die Levenshtein-Distanz (oder auch Editierdistanz) benennt die Anzahl der Operationen, 
die nötig sind, um eine gegebene Zeichenkette A in eine gegebene Zeichenkette B umzu wandeln. Diese Operationen beziehen sich immer auf ein einzelnes Zeichen und können 
die folgenden sein:
 Einfügen
 Löschen
 Ersetzen



220 6 Intents entwickeln
Neben dem uns bekannten Anwendungsfall, in dem wir Ergebnisse einer Speech-To-Text Komponente überprüfen, wird die Levenshtein-Distanz weiterhin recht häufig bei der Recht schreibprüfung eingesetzt.
Bild 6.37 zeigt ein Beispiel, in dem die Distanz zwischen den Wörtern Pferd und Herde be rechnet wird. Die erste Operation ersetzt das P durch ein H, die zweite löscht das f und die 
dritte fügt am Ende des Wortes ein e hinzu. Durch die Anzahl der Operationen ergibt sich eine 
Distanz von 3. Wie aber sieht der Algorithmus zu dieser Berechnung aus? Tatsächlich hat 
Levenshtein einen solchen in seiner Abhandlung von 1965 gar nicht bereitgestellt, sondern 
nur die Definition geliefert.
Wort 2
Herde
Wort 1
Pferd
Pferd Hferd
Ersetze P durch H
Hferd Herd
Lösche f
Herd Herde Füge e hinzu
(1)
(2)
(3)
Levenshtein-Distanz = 3
Bild 6.37 Berechnung der Levenshtein-Distanz zwischen den Wörtern „Pferd“ und „Herde“
Der Algorithmus zur Ermittlung der minimalen Editierdistanz ist ein wunderbares Beispiel 
dafür, wie eine Visualisierung der Daten zum Verständnis beitragen kann. Werfen Sie einen 
Blick auf Bild 6.38, in dem eine Matrix zu sehen ist, auf deren x-Achse das Ursprungswort, in 
unserem Fall Pferd, und auf deren y-Achse der Zielbegriff, hier Herde, aufgetragen ist. Beiden 
wird durch Anführungszeichen eine leere Zeichenkette vorangestellt, die für die Berechnung 
benötigt wird. Unser Ziel ist nun, die Matrix mit Zahlen zu befüllen, sodass wir im Kästchen 
unten rechts das Ergebnis ablesen können.
„“ P F E R D
„“ 012345
H 1 12345
E 222234
R 3 3332 3
D 4 4 443 2
E555443
Wort 1 (Ursprung) 
Pferd
Wort 2 (Ziel)
Herde
Schri 1
Schri 1
Schri 2
Ersetzen Löschen
Einfügen (i, j)
(0, 0) (1, 0) (2, 0)
(0, 1) (1, 1) (2, 1) (3, 1) (4, 1) (5, 1)
(3, 0) (4, 0) (5, 0)
(0, 2) (1, 2) (2, 2) (3, 2) (4, 2) (5, 2)
(0, 3) (1, 3) (2, 3) (3, 3) (4, 3) (5, 3)
(0, 4) (1, 4) (2, 4) (3, 4) (4, 4) (5, 4)
(0, 5) (1, 5) (2, 5) (3, 5) (4, 5) (5, 5)
Bild 6.38 Die Levenshtein-Distanz zwischen zwei Zeichenketten lässt sich über eine Matrix berechnen, 
in der der Ursprung auf der x-Achse und das Ziel auf der y-Achse aufgetragen wird.



6.5 Abspielen von Streams 221
Zur Befüllung sind zwei Schritte nötig:
 Initialisierung: Die mit Schritt 1 hervorgehobene Initialisierung betrifft die erste Spalte 
und die erste Zeile der Matrix. Dort sind inkrementell Zahlen ab der 0 einzutragen, sowohl 
horizontal als auch vertikal. Der Erklärung dafür ist recht einfach. Wir vergleichen nach 
und nach jede Unterzeichenkette aus dem Ursprungswort mit dem ersten Zeichen (also der 
leeren Zeichenkette) der ersten Spalte. Für jede überprüfen wir, wie viele Operationen wir 
vornehmen müssen, um diese Unterzeichenkette in eine leere Zeichenkette umzuwandeln. 
Das sähe konkret so aus:
 "" → "": 0 Operationen
 P → "": 1 Löschoperationen
 PF → "": 2 Löschoperationen
 PFE → "": 3 Löschoperationen
 PFER → "": 4 Löschoperationen
 PFERD → "": 5 Löschoperationen
Wie Sie wahrscheinlich richtig vermuten, wird die Anzahl der Operationen von links 
nach rechts in das jeweilige Feld der Matrix, auch bezeichnet als (i, j) und zu sehen in der 
rechten unteren Ecke jedes Feldes, eingetragen, wobei i für den aktuellen Spalten- und j
für den aktuellen Zeilenindex steht.
Das gleiche Verfahren wenden wir nun für die erste Spalte und die Umwandlung von Herde
auf eine leere Zeichenkette an und auch hier ergibt sich ein simples Inkrement.
 Berechnung: Für die Berechnung der restlichen Werte arbeiten wir uns von oben links 
zeilenweise bis unten rechts in der Matrix vor, beginnend bei Feld (1, 1), das im Ursprungs wort den Buchstaben P und im Zielwort den Buchstaben H referenziert. Wir unterscheiden 
nun zwischen zwei Fällen:
 Nichtübereinstimmung: Sind die beiden Zeichen nicht gleich, nehmen wir uns den 
weißen Kasten oben links aus Bild 6.38 und legen ihn so in unsere Matrix, dass das 
Kästchen mit (i, j) in dem Feld liegt, das wir befüllen wollen. Dann suchen wir in den 
drei anderen Kästchen, die von dem Kasten überlagert werden, nach der kleinsten Zahl. 
Falls nicht ganz klar ist, welche Kästchen gemeint sind, können sie deren Position auch 
über die Indizes (i – 1, j), (i, j – 1) und (i – 1, j – 1) ermitteln, also über die Kästchen links 
neben, links über und über dem aktuell zu berechnenden Kasten.
Für unser erstes Feld in (1, 1) ist der kleinste Wert der zu betrachtenden Nachbarn die 0 
aus Position (0, 0). Diese nehmen wir nun und addieren 1 hinzu, denn wir benötigen eine 
Operation, um aus dem P ein H zu machen. Nun bewegen wir uns horizontal zum Feld 
(2, 1). Auch hier ist eine Ersetzung notwendig, um aus dem F ein H zu machen. Von den 
eben geprüften umliegenden Feldern ist der kleinste Wert 1. Und durch die Ersetzung 
wird eine weitere Operation notwendig, weswegen sich der Wert des aktuellen Feldes 
auf 2 summiert. So gehen wir nun Reihe für Reihe weiter vor.
 Übereinstimmung: Haben wir eine Übereinstimmung zweier Zeichen, wie in den Feldern 
(3, 2), (4, 3) oder (5, 4), dann übernehmen wir den Wert des Feldes (i – 1, j – 1), gekenn zeichnet durch die Pfeile. Feld (3, 2) übernimmt beispielsweise die Werte von Feld (2, 1).
Ist die Matrix befüllt, können wir die Editierdistanz ganz bequem im Feld unten rechts ab lesen. Diese ist in Bild 6.38 durch einen grünen Kreis hervorgehoben. Der dazugehörige, recht 



222 6 Intents entwickeln
einfache Algorithmus ist in Listing 6.45 zu sehen. Wir initialisieren zunächst in Zeile 6 eine 
Matrix über NumPy, deren Dimension in x-Richtung der Länge des Ursprungsworts und in 
y-Richtung der Länge des Zielworts entspricht. In den beiden Schleifen in den Zeilen 10–14 
befüllen wir die erste Spalte und die erste Zeile initial aufsteigend, wie es in den Schritten 
zuvor beschrieben wurde.
Listing 6.45 Algorithmus zur Levenshtein-Distanz
1. import numpy as np
2.
3. def levenshtein_distance(source, target):
4. rows = len(source)+1
5. cols = len(target)+1
6. distance = np.zeros((rows,cols),dtype = int)
7.
8. print(distance)
9.
10. # Initialisiere die erste Spalte und die erste Zeile der Matrix
11. for i in range(1, rows):
12. for k in range(1,cols):
13. distance[i][0] = i
14. distance[0][k] = k
15.
16. print(distance)
17.
18. # Iteriere zeilenweise über die Matrix
19. for col in range(1, cols):
20. for row in range(1, rows):
21.
22. # Stimmen die Zeichen überein?
23. if source[row-1] == target[col-1]:
24. cost = 0
25. else:
26. cost = 1
27. distance[row][col] = min(distance[row-1][col] + 1,
28. distance[row][col-1] + 1, # Kosten für Einfügen
29. distance[row-1][col-1] + cost) # Kosten für Ersetzen
30.
31. # Berechnen der Ratio
32. ratio = ((len(source)+len(target)) - distance[row][col]) /
33. (len(source)+len(target))
34.
35. print(distance)
36.
37. return "Es werden {} Änderungen benötigt, um aus {} {} zu machen. "+
38. "Die Zeichenketten stimmen zu {}% überein.".
39. format(distance[row][col], source, target, ratio*100)
40. if __name__ == '__main__':
41. print(levenshtein_distance("Pferd", "Herde"))
In den Zeilen 19–29 erfolgt die restliche Befüllung und die eigentliche Berechnung der 
Matrix. Hier gehen wir, wie zuvor schon erläutert, Zeile für Zeile von links nach rechts über 



6.6 Wetterabfrage 223
die Felder der Matrix und befüllen diese. Haben wir eine Übereinstimmung zweier Zeichen, 
sind die Kosten für dieses Feld 0, muss eine Ersetzung stattfinden, sind die Kosten 1. In den 
Zeilen 27–29 wird von den drei benachbarten Feldern der Minimalwert ermittelt, wobei das 
Einfügen und das Löschen immer Kosten von 1 aufweisen und das Ersetzen bei einer Über einstimmung auch 0 sein kann. Dieser Minimalwert wird dann an der aktuellen Position in 
der Matrix (distance[row][col]) festgeschrieben.
Ist die Matrix befüllt, berechnen wir noch die Levenshtein-Ratio, eine Art prozentuale Über einstimmung von Source und Target wenn man so möchte, die beispielsweise bei der Über prüfung der Radiosendernamen herangezogen werden könnte. Deren Ermittlung ist relativ 
simpel, wir summieren die Länge der zwei Wörter auf, ziehen die Distanz ab und teilen 
diesen Wert erneut durch die Länge beider Wörter. So ergibt sich neben der Editierdistanz 
von 3 eine Ratio von 0.7 (siehe Bild 6.39).
Bild 6.39 Ergebnis der Berechnung der Levenshtein-Distanz und der dazugehörigen Ratio. 
Die Matrizen zeigen von links nach rechts den unbefüllten, den initialisieren und den berechneten 
Zustand.
■ 6.6 Wetterabfrage
In diesem Abschnitt wollen wir uns einem Thema widmen, das aus Sicht meiner Frau auf 
keinem elektronischen Gerät fehlen darf; der Wetteransage. Ziel ist es, auf Aufrufe wie Wie 
ist das Wetter in Landau? zu reagieren und Temperatur und Bewölkung, Regen oder Son nenschein in der jeweiligen Region zu verkünden. Wir verwenden dafür den Service OWM 
(Open Weather Map) (siehe Bild 6.40) und die dazu passende Library. Zusätzlich möchten 
wir unseren Benutzern anbieten, ihren Standort automatisch zu ermitteln, sodass sich die 
Frage nach dem Wetter auf Wie ist das Wetter? reduzieren ließe.
Da diese beiden Features durch mehr oder weniger komplexe API-Aufrufe lösbar sind, möchte 
ich mit Ihnen gleichzeitig noch unser Framework dahingehend erweitern, dass wir bestim men können, welche Intents ein Benutzer aufrufen darf. Hören die Kinder etwa zu häufig 
am Tag Tierlaute, so schränken wir den Zugriff auf diese Funktion ein, sodass nur noch die 
Erwachsenen in diesen Genuss kommen.



224 6 Intents entwickeln
Bild 6.40 Open Weather Map stellt eine API zu Verfügung, um das Wetter weltweit abzufragen.
6.6.1 Einschränken des Intent-Zugriffs
Mit letzterem Eingriff in das Framework wollen wir auch gleich beginnen. Wie immer finden 
Sie das Beispiel 010_f_weather im Github Repository, um es Schritt für Schritt nachvollziehen 
zu können. Die Anlage eines gleichnamigen Environments ist obligatorisch, Sie können aber 
auch wie immer einfach auf dem des letzten Abschnitts aufsetzen.
Die Anpassung findet im User Management statt, da wir dort pro User festlegen wollen, 
welche Intents er oder sie aufrufen darf. Das wiederum erfordert eine Anpassung der users.
json, die wir sogleich in Listing 6.46 vornehmen.
Listing 6.46 Anpassen der Benutzereigenschaften für Intent-Zugriffe
{
 "speakers": {
 "1": {
 "name": "jonas",
 "voice": [],
 "intents":["*"]
 },
 "2": {
 "name": "sarah",
 "voice": [],
 "intents":["animalsounds", "gettime"]
 }
 }
}
Ich habe hier den etwas klobigen Fingerabdruck der Stimmen entfernt und einen zweiten 
Benutzer, Sarah, eingetragen. Sehen Sie den neuen Eintrag intents? Hierbei handelt es sich 



6.6 Wetterabfrage 225
um eine Liste der Funktionen, die wir für den jeweiligen Benutzer zulassen. Jonas bekommt 
durch ein Wildcard-Sternchen die Berechtigung, alle Intents aufzurufen, Sarah schränken 
wir hingegen auf Tierlaute und Zeitansage ein (natürlich nur exemplarisch). Um diese Eigen schaft nun zu verwenden, öffnen wir die usermgmt.py und fügen der Klasse UserMgmt die 
Funktion authenticate_intent() hinzu. Diese ist in Listing 6.47 zu sehen. Darin wird 
zunächst der Eintrag für den aktuellen Sprecher aus der users.json gelesen. Ist dieser Spre cher bekannt, lesen wir in Zeile 7 alle Intents dieses Benutzers aus, die er oder sie aufrufen 
darf. Ist diese Liste in den Eigenschaften vorhanden, prüfen wir in den Zeilen 11 und 12 die 
folgenden beiden Punkte:
 Weist die Intent-Liste genau einen Eintrag auf, der ein * beinhaltet? Dann darf dieser näm lich alle Intents ausführen.
 Beinhaltet die Intent-Liste den Namen des Intents?
In allen anderen Fällen evaluiert die Methode zu False und das Ausführen des Intents wird 
untersagt.
Listing 6.47 authenticate_intent bewertet, ob ein Benutzerprofil einen bestimmten Intent aufrufen darf
1. def authenticate_intent(self, speaker, intent):
2. Speaker = Query()
3. # Hole den Eintrag für den Sprecher
4. result = self.speaker_table.get(Speaker.name == speaker)
5. if not result is None:
6. # Hole die Intents, die der Sprecher nutzen darf
7. intents = result['intents']
8. if intents:
9. # Ist ein Eintrag in der Intent-Liste mit dem Wert "*"
10. # oder der exakte Name des Intents?
11. return (((len(intents) == 1) and (intents[0] == '*')) or
12. (intent in intents))
13. return False
Zugegeben, die Methode ist noch alles andere als perfekt. Was ist, wenn ein Name zweimal in 
der users.json vorkommt? In dem Fall wird immer für den ersten Treffer auf eine Erlaubnis, 
einen Intent auszuführen, geprüft. Es würde also sicherer sein, den Fingerabdruck zu prüfen. 
Weiterhin ist unser Sprachassistent mit dieser Authentifizierung sehr stark eingeschränkt, 
denn kein anderer außer den bekannten Benutzern kann einen Befehl geben.
Das war der einfache Teil. Der schwierigere dreht sich nun darum, das Intent Management 
anzupassen. Dazu müssen wir eine bestehende Klasse erweitern, ganz konkret Chat aus der 
Bibliothek ChatbotAI. Warum müssen wir das tun? Nun, ChatbotAI bietet von sich aus keine 
Funktionalität, um den Namen der Funktion zurückzuliefern, die es zur Beantwortung einer 
Anfrage auswählt. Und genau diese gilt es nun nachzupflegen, denn wir müssen ja vor der 
Ausgabe der Antwort sichergestellt haben, dass der Benutzer diesen Befehl auch wirklich 
absetzen darf.
Listing 6.48 zeigt genau diese Erweiterung, die wir in die intentmgmt.py eintragen. Dankens werterweise ist es in Python (und vielen anderen Sprachen natürlich auch) recht einfach, 
eine bestimmte Klasse zu erweitern. Wir erstellen einfach eine zweite Klasse Chat, die auf 
der vorherigen aufbaut (siehe Zeile 2). Damit erbt diese neue Klasse alle Funktionalitäten 



226 6 Intents entwickeln
der alten und fügt lediglich eine neue hinzu, nämlich die Funktion get_intent_name(). 
Diese erwartet den Eingabetext des Benutzers als Parameter text sowie die bereits bekannte 
session_id, die wir auf dem Default general lassen, da wir ja immer nur eine Session verwen den. Der Inhalt der Funktion ist weitestgehend aus existierenden Funktionen des Frameworks 
abgeleitet. Wir evaluieren hier, nach einigen Datenbereinigungsfunktionen (Zeilen 6–13), 
welcher Intent zur Beantwortung der Anfrage in text ausgewählt werden würde (Zeilen 16–17). 
Gibt es einen Treffer (match ist gesetzt), dann wird der Response-String genauer betrachtet 
und wenn dieser einen Substring beinhaltet, der mit {% call beginnt und mit %} endet, dann 
haben wir unsere aufzurufende Funktion gefunden, schneiden sie aus dem Response-String 
heraus und liefern sie zurück. In jedem anderen Fall geben wir einen leeren String zurück.
Listing 6.48 Erweiterung der Klasse Chat um die Funktionalität, einen Intent-Namen zu erfahren
1. # Erweitere die Klasse Chat durch die Funktion get_intent_name
2. class Chat(Chat):
3. def get_intent_name(self, text, session_id="general"):
4. session = mapper.Session(self, session_id)
5.
6. text = self.__normalize(text)
7. try:
8. previous_text = self.__normalize(
9. session.conversation.get_bot_message(-1)
10. )
11. except IndexError:
12. previous_text = ""
13. text_correction = self.spell_checker.correction(text)
14. current_topic = session.topic
15.
16. match = self.__intend_selection(text, previous_text, current_topic) or
17. self.__intend_selection(text_correction, previous_text, current_topic)
18.
19. if match:
20. match, parent_match, response, learn = match
21. resp = random.choice(response)
22. response, condition = resp
23. action_start = response.find("{% call ")
24. action_end = response.find("%}")
25. if action_start >= 0 and action_end >= 0:
26. action_corpus = response[action_start + len("{% call "):action_end - 1]
27. if action_corpus.find(":") > 0:
28. action_name = action_corpus.split(':')[0]
29. return action_name
30. return ""
Mit der Funktion get_intent_name() sind wir nun in der Lage, den Namen der Funktion 
zu ermitteln, die ChatbotAI aufrufen würde.
Analog dazu müssen wir eine ähnliche Funktion für Snips-NLU formulieren. Diese platzieren 
wir ebenfalls in der intentmgmt.py, jedoch außerhalb der neuen Klasse Chat. Der Funktion 
geben wir, wie in Listing 6.49 zu sehen, den passenden Namen get_snips_nlu_intent().



6.6 Wetterabfrage 227
Listing 6.49 Intent-Namen von Snips-NLU erfahren
1. def get_snips_nlu_intent(text):
2. parsing =
3. global_variables.voice_assistant.intent_management.nlu_engine.parse(text)
4.
5. for intent in
6. global_variables.voice_assistant.intent_management.dynamic_intents:
7.
8. # Wurde überhaupt ein Intent erkannt?
9. if parsing["intent"]["intentName"]:
10.
11. # Die Wahrscheinlichkeit wird geprüft, um sicherzustellen, dass nicht
12. # irgendein Intent angewendet wird, der gar nicht gemeint war
13. if (parsing["intent"]["intentName"].lower() == intent.lower()) and
14. (parsing["intent"]["probability"] > 0.5):
15. return parsing["intent"]["intentName"]
16. return ""
Glücklicherweise müssen wir hier nicht so tief in den Code eingreifen wie zuvor, denn wir 
können einfach das Ergebnis des Parsings nehmen, das den Namen des Intents beinhaltet. 
Wir geben den Text des Benutzers ein und lassen ihn, wie in den Zeilen 2 und 3 zu sehen, von 
der Engine verarbeiten. Nun kommt es uns zugute, dass wir alle Intent-Namen beim Einlesen 
zwischengespeichert haben, denn wir können so jeden einzelnen gegen den erkannten Intent Namen aus dem Ergebnis des Parsings prüfen. Finden wir eine Übereinstimmung (Zeile 13), 
prüfen wir weiterhin, ob dieser Intent mit einer ausreichend hohen Wahrscheinlichkeit und 
nicht in Ermangelung eines Besseren ausgewählt wurde. Die 0.5 sind empirisch ermittelt, 
also durch Ausprobieren herausgefunden. Snips-NLU soll sich zu 50 % sicher sein, dass wir 
diesen Intent mit unserer Eingabe in text aufrufen wollten. Auch hier gilt wieder: Wenn kein 
Intent ermittelt wurde, der genügend gut zur Eingabe passt, geben wir einen leeren String 
zurück – symbolisch für den Fehlschlag der Ermittlung einer passenden Funktion.
Sind die zwei Funktionen zur Ermittlung der aufzurufenden Intents für unsere beiden 
Frameworks einmal geschrieben, können wir endlich der Methode process() in der Klasse 
IntentMgmt eine wirkliche Aufgabe zuweisen. Bisher rufen wir dort ja lediglich self.chat.
respond(text) auf und geben das Ergebnis zurück. In Listing 6.50 erweitern wir process()
nun um unsere Authentifizierung. Zu Beginn ermitteln wir in Zeile 4 den Intent-Namen, 
den ChatbotAI aufrufen würde. Ist dieser gefunden, speichern wir ihn in intent_name. Wir 
erinnern uns an unsere ausgeklügelte Logik, dass ChatbotAI die Anfrage an Snips-NLU über 
den default_snips_nlu_handler weiterleitet, wenn es keine passende Antwort auf die Frage 
des Benutzers finden konnte. Ist der Intent-Name auf default_snips_nlu_handler gesetzt, 
müssen wir in einem zweiten Schritt die Methode get_snips_nlu_intent() aufrufen, um 
von diesem Framework zu erfahren, welchen Intent es wählen würde.
Nun sind wir in der Lage, authenticate_intent() aus der Benutzerverwaltung zu fragen, 
ob der aktuelle Sprecher den ermittelten Intent aufrufen darf. Wenn ja, wird die Antwort in 
Zeile 13 entsprechend formuliert und später als response ausgegeben. Darf der derzeitige 
Sprecher den Intent nicht auswählen, geben wir ihm oder ihr den Hinweis, dass die passende 
Authentifizierung fehlt.



228 6 Intents entwickeln
Listing 6.50 Überprüfung der Intent-Aufrufberechtigung in der Funktion process()
1. def process(self, text, speaker):
2.
3. # Evaluiere zunächst ChatbotAI
4. intent_name = self.chat.get_intend_name(text)
5.
6. # Greift kein Intent, überprüfe Snips NLU
7. if intent_name == "default_snips_nlu_handler":
8. intent_name = get_snips_nlu_intent(text)
9.
10. # Überprüfe, ob der Benutzer diesen Intent ausführen darf
11. if global_variables.voice_assistant.user_management.
12. authenticate_intent(speaker, intent_name):
13. response = self.chat.respond(text)
14. else:
15. # In diesem Beispiel lassen wir den Assistenten antworten.
16. # In Zukunft wird er einfach nicht reagieren, um das Abspielen gar nicht
17. # erst zu unterbrechen.
18. response = speaker + " darf den Befehl " + intent_name +
19. " nicht ausführen."
20.
21. return response
Damit sind wir durch! Probieren Sie mal, Ihre Rechte ein wenig einzuschränken, sodass 
Sie nur noch einzelne Befehle ausführen dürfen. Das sollte nun mit einer entsprechenden 
Meldung quittiert werden.
6.6.2 Die Wetterabfrage
Wie auch die Male zuvor beginnen wir damit, dass wir im Ordner intents\functions einen 
neuen Ordner anlegen, diesmal mit dem Namen weather. Diesen wollen wir nicht ungenutzt 
lassen und versehen ihn mit der Konfigurationsdatei config_weather.yml, die den Inhalt von 
Listing 6.51 bekommen soll. Darin verorten wir eine Liste von Sätzen, über die wir das Wetter 
ausgeben (hier ist es erst mal nur einer), sowie ein Template, falls das Wetter für einen be stimmten Ort nicht gefunden werden konnte. Zusätzlich legen wir ein Schlagwort fest, auf 
das der Intent reagieren soll, wenn kein konkreter Ort angegeben wurde, sondern lediglich 
der aktuelle Standort des Benutzers auf das aktuelle Wetter geprüft werden soll. Im Deut schen legen wir hier fest, im Englischen here. Die letzte Eigenschaft, die wir verwenden, ist 
owm_api_key. Da Open Weather Map ein teilweise kommerzieller Service ist, der auch größeren 
Unternehmen zu Verfügung steht und viele Tausend Abfragen am Tag erlaubt, verwendet 
es einen API-Schlüssel, der an einen Benutzeraccount gebunden ist. So kann der Anbieter 
sicherstellen, dass wir als Entwickler nur derzeit 60 Abrufe pro Minute und eingeschränkte 
Wettervorhersagen abrufen. Haben Sie sich einmal auf www.openweathermap.org registriert, 
können Sie in Ihrem Profil unter My API keys Ihren persönlichen Schlüssel abrufen und in 
der Konfigurationsdatei eintragen. Der, den Sie in Listing 6.51 finden, ist frei erfunden und 
sollte nicht verwendet werden.



6.6 Wetterabfrage 229
Listing 6.51 Konfigurationsdatei für den Wetter-Intent
intent:
 weather:
 de:
 weatheris: ["Das Wetter in {} ist {} bei einer Temperatur von {} Grad Celsius."]
 location_not_found: "Ich konnte den Ort {} nicht finden."
 here: "hier"
 en:
 weatheris: ["The weather in {} is {} at a temperatur of {} degrees."]
 location_not_found: "I could not find a place called {}."
 here: "here"
 owm_api_key: " f91ade7f89f37cf932a7d1f5337ae2d4"
Es folgt die Anlage der requirements.txt im selben Ordner mit dem Inhalt von Listing 6.52.
Listing 6.52 Abhängigkeiten des Wetter-Intents
pyowm==3.1.1
geocoder==1.38.1
PyOWM erlaubt uns den Zugriff auf Open Weather Map per Python API, Geocoder hilft uns 
dabei, den Standort zu einer IP-Adresse zu ermitteln.
HINWEIS: Hier befinden wir uns natürlich in dem Dilemma, dass wir unseren 
Sprachassistenten eigentlich so entwickeln wollten, dass er auch offline funk tioniert. Der Wetter-Intent ist nun aber, wie auch das Abspielen des Webradios, 
abhängig von einer Internetverbindung. Wenn Sie den Assistenten aber wirklich 
unabhängig vom World Wide Web betreiben möchten, dann können Sie das trotz dem tun – die Grundfunktionalität bleibt Ihnen ja schließlich erhalten, bloß dass 
einzelne Dienste nicht mehr funktionieren.
Legen Sie nun die Datei intent_weather.py in intents\functions\weather an. Deren Inhalt ist 
entsprechend Listing 6.53 zu erstellen. Die Verwendung von @register_call in Zeile 12 
deutet darauf hin, dass wir ChatbotAI verwenden. In den Zeilen 15–24 lesen wir wie gewohnt 
die Konfigurationsdatei aus, die wir eben in Listing 6.51 erstellt haben, und holen dort unsere 
Zeichenketten heraus, die wir später für die Ausgabe verwenden (WEATHER_IS, HERE, 
LOCATION_NOT_FOUND), sowie unseren API_KEY, den Sie spätestens jetzt anlegen sollten.
Da die API von Open Weather Map mehrsprachig ist, müssen wir über die Funktion 
get_default_config() in Zeile 34 die Konfiguration der Bibliothek abrufen und in der 
darauffolgenden Zeile dahingehend anpassen, dass wir die Sprache auf die Sprache unseres 
Sprachassistenten setzen.



230 6 Intents entwickeln
Listing 6.53 Logik des Wetter-Intents
1. from loguru import logger
2. from chatbot import register_call
3. import global_variables
4. import random
5. import os
6. import yaml
7.
8. import pyowm
9. from pyowm.utils.config import get_default_config
10. import geocoder
11.
12. @register_call("weather")
13. def weather(session_id = "general", location=""):
14.
15. config_path =
16. os.path.join('intents','functions','weather','config_weather.yml')
17. cfg = None
18.
19. with open(config_path, "r", encoding='utf8') as ymlfile:
20. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
21.
22. if not cfg:
23. logger.error("Konnte Konfigurationsdatei für das Wetter nicht lesen.")
24. return ""
25.
26. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
27.
28. WEATHER_IS = random.choice(cfg['intent']['weather'][LANGUAGE]['weatheris'])
29. HERE = cfg['intent']['weather'][LANGUAGE]['here']
30. LOCATION_NOT_FOUND = cfg['intent']['weather'][LANGUAGE]['location_not_found']
31. API_KEY = cfg['intent']['weather']['owm_api_key']
32.
33. # Konfiguration for die Wetter API
34. config_dict = get_default_config()
35. config_dict['language'] = LANGUAGE
36.
37. owm = pyowm.OWM(API_KEY, config_dict)
38. weather_mgr = owm.weather_manager()
39.
40. location = location.strip()
41. if (location == HERE) or (location == ""):
42. g = geocoder.ip('me')
43. w = weather_mgr.weather_at_coords(g.latlng[0], g.latlng[1]).weather
44. return WEATHER_IS.format(g.city, w.detailed_status,
45. str(w.temperature('celsius')['temp']))
46. else:
47. obs_list = weather_mgr.weather_at_places(location, 'like', limit=5)
48. if len(obs_list) > 0:
49. w = obs_list[0].weather
50. return WEATHER_IS.format(location, w.detailed_status,
51. str(w.temperature('celsius')['temp']))
52.
53. return LOCATION_NOT_FOUND.format(location)



6.6 Wetterabfrage 231
Passen wir die Sprache nicht an, bekommen wir statt des Begriffs wolkig etwa cloudy aus gegeben. In Zeile 37 setzen wir den API-Key und die Konfiguration, die wir eben abgerufen 
und angepasst haben, und initialisieren eine Instanz der Klasse OWM. Diese nutzen wir, um 
einen weather_manager anzulegen, der ab jetzt für die Interaktion mit Open Weather Map
verantwortlich sein wird.
Ab Zeile 40 bereiten wir den Standort auf. Nennt der Benutzer einen solchen, zum Beispiel durch 
die Formulierung Wie ist das Wetter in Mannheim?, so wird der Standort auf Mannheim 
gesetzt, wie wir gleich beim Erstellen des ChatbotAI-Templates sehen werden. In Zeile 41 
prüfen wir jedoch, ob der Standtort nicht übergeben wurde (etwa durch die Frage Wie ist das 
Wetter?) oder ob die Frage Wie ist das Wetter hier? geäußert wurde. In diesem Fall müssen 
wir den Standort des Benutzers ermitteln, was durch die Bibliothek Geocoder relativ leicht 
ist, wie Zeile 42 zeigt. So bekommen wir einen Längen- und einen Breitengrad, den wir an 
die Funktion weather_at_coords() übergeben können (Zeile 43). Mit dem resultierenden 
weather-Objekt können wir nun unser Template WEATHER_IS befüllen und ausgeben, denn 
es beinhaltet all die Eigenschaften, die wir für die Rückgabe benötigen:
 g.city: Spezifiziert die Stadt, die Geocoder für unsere IP-Adresse ermittelt hat.
 w.detailed_status: Gibt eine Beschreibung für das aktuelle Wetter zurück, etwa sonnig, 
regnerisch oder bewölkt.
 w.temperature('celsius')['temp']: Gibt die aktuelle Temperatur in Grad Celsius zurück. Neben 
der Temperatur können Sie sich auch noch eine gefühlte Temperatur ausgeben lassen, 
schauen Sie sich einfach das Objekt mal genauer per Log-Ausgabe an.
Ab Zeile 47 behandeln wir dann den Fall, dass ein Standort vom Benutzer übergeben wurde, 
sodass wir diesen mit weather_at_place() ermitteln können. Das like ist analog zu einer 
SQL-Abfrage für den Abgleich des Standortnamens verantwortlich und das limit schränkt die 
Ergebnisliste ein. Ist mindestens ein Standort mit dem angefragten Namen gefunden worden, 
befüllen wir ebenfalls das WEATHER_IS-Template und geben es zurück. Wurde bis hierhin 
kein Standort ermittelt, geben wir diese Tatsache in Zeile 53 final zurück.
Zu unserem Glück fehlt uns lediglich noch das Template, das wir in intents\chatbotai anlegen 
und weather.template nennen.
Listing 6.54 Template für den Wetter-Intent
{% block %}
 {% client %}
 (wie ist das wetter in|wie ist das wetter)((\s*)(?P<location>.*))?
 {% endclient %}
 {% response %}{% call weather: %location %}{% endresponse %}
{% endblock %}
Listing 6.54 birgt keine großen Überraschungen, bis auf eine. Wir können die Frage nach 
dem Wetter ja auf zwei Arten stellen:
 Wie ist das Wetter?
 Wie ist das Wetter in Bonn?
Beide Sätze unterscheiden sich darin, dass der erste keinen Parameter verwendet, der zweite 
aber schon. Das bilden wir per Regular Expression so ab, dass wir eine optionale Leerstelle 



232 6 Intents entwickeln
nach Wie ist das Wetter und Wie ist das Wetter in erlauben. Das tun wir durch den String \s*, 
der so viel bedeutet wie: erlaube 0–n Whitespaces. Das Fragezeichen hinter den Klammern, 
in denen sich der Location-Parameter befindet, sagt dann zusätzlich aus, dass dieser Para meter vorkommen kann, aber nicht muss. So gehen wir sicher, dass beide Fragen durch den 
regulären Ausdruck erkannt und vom Framework verarbeitet werden können.
Damit sind wir auch schon wieder fertig und Sie können den Intent gerne auf Herz und Nieren 
testen. Schade ist, dass Open Weather Map eine Wettervorhersage nur zulässt, wenn man der 
Firma etwas Geld dafür gibt (was natürlich absolut verständlich ist, für die tolle Arbeit, die 
dort geleistet wird). Aber warum bauen wir uns nicht einfach unsere eigene Wettervorhersage?
6.6.3 Time Series Forecasts mit Wetterdaten
Wir Menschen besitzen eine Fähigkeit, die wir, wie so viele andere auch, einer Maschine erst 
mühevoll beibringen müssen. Vervollständigen Sie doch mal diese Zahlenreihen:
0, 1, 4, ?
0, 1, 1, 2, 3, ?
Sie werden schnell darauf gekommen sein, dass es sich in der ersten Zeile um die Quadrierung 
der Zahlen von 0 bis 2 handelt und der nächste Wert demnach 9 betragen müsste. Wer Dan 
Browns Sakrileg gelesen hat oder sich aus mathematischem Interesse mit der Wachstums lehre beschäftigt, merkt weiterhin, dass die zweite Zahlenreihe die Fibonacci-Folge darstellt 
und folglich der Summand aus den zwei vorhergehenden Zahlen für den letzten Wert 5 lauten 
muss. Beide Reihen folgen einer festen Regel, einer Formel, die wir auf unbekannte Zahlen 
anwenden und somit die nächsten Werte präzise vorhersagen können. Jetzt fällt dem ein oder 
anderen sicher ein, dass gängige Tabellenkalkulationswerkzeuge ebenfalls ein derartiges 
Feature besitzen. Microsoft Excel ist in der Lage, einfache Zahlenreihen fortzusetzen, etwa 
ein fortlaufendes Datum oder eine Kette von Integer-Werten, die jeweils um einen festen 
Summanden erhöht werden. Wie sich Excel jedoch bei komplexeren Formeln verhält, ist in 
Bild 6.41 zu sehen.
x x2 x [–1] + x
Bild 6.41 Excel vervollständigt einfache, fortlaufende Zahlenreihen korrekt (Spalte links), 
scheitert aber an der Erkennung komplexerer Berechnungen (mittlere und rechte Spalte).
Wir sind der Maschine in diesem Fall also überlegen, können wir doch anhand einer Hand 
von Werten schon ab dem Kindesalter vorhersagen, wie eine Zahlenreihe fortzusetzen ist, 
wenn wir denn erst einmal die Formel ermittelt haben (denn genau das ist es, was wir tun).



6.6 Wetterabfrage 233
6.6.3.1 Ausflug in die Welt der Regression
Das Finden einer Formel, die eine gegebene Anzahl von Wertpaaren repräsentiert, ist nicht 
nur dem menschlichen Verstand vorbehalten, sondern schon seit langem in der Statistik 
und der Domäne des maschinellen Lernens verankert. Bei der linearen Regression etwa 
wird eine Geradengleichung ermittelt, die eine Punktwolke so genau wie möglich abbildet. 
Bild 6.42 zeigt eine solche exemplarisch für eine zwölfstündige Temperaturkurve im Monat 
März und benennt die abgeleitete Geradengleichung f (x) = 0,01868 · x – 4,42857, die in Blau 
dargestellt wird. Dadurch sind wir in der Lage, für beliebige Werte von x (in der Regression als 
Regressand bezeichnet) den unbekannten Wert y (Regressor) näherungsweise zu bestimmen. 
Die drei grünen Punkte in der Abbildung zeigen etwa die Werte für die Stunden 4, 5 und 6 
nach Ende der Messung – vorausgesagt von einem sehr einfachen Modell.
Bild 6.42 Die lineare Regression bildet eine Gerade ab, die eine Menge von Punkten so gut wie 
möglich anhand der vorliegenden Daten repräsentiert – hier zu sehen in Form von Temperaturdaten 
der letzten zwölf Stunden, dargestellt durch rote Kreuze. Die ermittelte Geradengleichung ist in Blau 
dargestellt, die vorhergesagten Temperaturwerte hingegen als grüne Punkte.
Schauen wir uns das Vorgehen mal etwas genauer an. Das passende Beispiel, das in 
Listing 6.55 gezeigt wird, finden Sie im Ordner 100_extras\100_14_regression. Dank Scikit Learn ist eine lineare Regression denkbar einfach umzusetzen. Wir beginnen damit, dass 
wir uns drei Testszenarien in Form von jeweils drei NumPy-Arrays kreieren, basierend auf 
den Formeln:
 y = x
 y = x2
 y = x [–1] + x



234 6 Intents entwickeln
Diese geben wir nacheinander in die Methode train_and_predict(), die auf Basis von 
x und y ein Modell auf Basis einer linearen Regression trainiert, zu erkennen an der Ver wendung der Funktion fit() (Zeile 26). Diese tut nichts anderes, als die Koeffizienten der 
Geradengleichung zu optimieren.
HINWEIS: Warum müssen die Arrays für x immer zweidimensional sein? Das li neare Regressionsmodell von Scikit-Learn ist so implementiert, dass es ebenfalls 
eine Multiple Linear Regression (MLR) abbilden kann, die nichts anderes tut, als 
ein Modell zu erzeugen, in dem y von mehreren bekannten Variablen in x abhängig 
ist. Ein solches Beispiel sehen wir nachher bei unserer Wettervorhersage. Hier 
kann die Temperatur von Morgen von der heutigen Temperatur abhängen sowie 
gleichzeitig vom heutigen Luftdruck.
In Zeile 28 besorgen wir uns den Determinationskoeffizienten (auch Bestimmtheitsmaß 
oder R2
), über den wir prüfen, wie gut das trainierte Modell bei unseren, ihm unbekannten 
Evaluationsdaten (val_x und val_y) performt.
Listing 6.55 Lineare Regression für Summen, Quadrate und Fibonacci
1. import numpy as np
2. from sklearn.linear_model import LinearRegression
3.
4. # x
5. x1 = np.array([[0], [1], [2], [3], [4], [5]]) # x must be two dimensional
6. y1 = np.array([0, 1, 2, 3, 4, 5])
7. val_x1 = np.array([[6], [7], [8]]) # x must be two dimensional
8. val_y1 = np.array([6, 7, 8])
9.
10. # x^2
11. x2 = np.array([[0], [1], [2], [3], [4], [5]])
12. y2 = np.array([0, 1, 4, 9, 16, 25])
13. val_x2 = np.array([[6], [7], [8]])
14. val_y2 = np.array([36, 49, 64])
15.
16. # Fibonacchi
17. x3 = np.array([[0], [1], [2], [3], [4], [5]])
18. y3 = np.array([0, 1, 1, 2, 3, 5])
19. val_x3 = np.array([[6], [7], [8]])
20. val_y3 = np.array([8, 13, 21])
21.
22. test = np.array([[6], [7], [8]])
23.
24. def train_and_predict(x, y, val_x, val_y, test):
25. model = LinearRegression()
26. model.fit(x, y)
27.
28. cod = model.score(val_x, val_y)
29. print('Coefficient of determination: ', cod)
30.
31. predictions = model.predict(test)
32. print('Predictions: ', predictions, sep='\n')



6.6 Wetterabfrage 235
33.
34. if __name__ == '__main__':
35. print("x")
36. train_and_predict(x1, y1, val_x1, val_y1, test)
37. print("\n")
38.
39. print("x^2")
40. train_and_predict(x2, y2, val_x2, val_y2, test)
41. print("\n")
42.
43. print("Fibonacci")
44. train_and_predict(x3, y3, val_x3, val_y3, test)
45. print("\n")
Im Ergebnis in Bild 6.43 sehen wir, dass der Koeffizient für den ersten Datensatz 1,0 beträgt, 
für den zweiten etwa –1,88 und für den dritten etwa –1,91. Das lässt sich so interpretieren, 
dass das erste Modell perfekt auf die Validierungsdaten passt (ein besserer Wert als 1,0 kann 
nicht erzielt werden). Die negativen Ausprägungen von Modell zwei und drei bedeuteten das 
genaue Gegenteil: Das Modell sagt Werte voraus, die eine sehr hohe Distanz zu den Werten 
in den Validierungsdaten aufweisen.
Bild 6.43 Ausgabe der Berechnungen der linearen Regression
HINWEIS: Wenn der Determinationskoeffizient auch als R2
 bezeichnet wird, wäre 
doch eigentlich davon auszugehen, dass er niemals negativ ist, oder? Tatsächlich 
behaupten viele Quellen im Internet, dass R2
 immer zwischen 0,0 und 1,0 liegen 
muss, wobei 0,0 darauf hindeutet, dass das Modell so schlecht wie nur möglich 
evaluiert wird. Die Dokumentation von Scikit-Learn hingegen weist ausdrücklich 
darauf hin, dass der r2_score auch negativ sein kann, da Modelle beliebig schlecht
sein können8
. Die genaue Berechnung des Determinationskoeffizienten erspare 
ich Ihnen hier, da gibt es tausende Statistikbücher, die die Formel hervorragend 
erklären, und die gängigen Statistikbibliotheken wie NumPy oder Scikit-Learn bie ten alle vorgefertigten Funktionen dafür.
8 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichscikit-learn.org/stable/modules/model_evaluation.html#r2-score



236 6 Intents entwickeln
Neben der Validierung erstellen wir noch einige Testdaten in der Liste test (Zeile 22), die wir 
für unsere eigene, wenn man so will menschliche, Validierung des Modells verwenden. Auch 
hier zeigt sich, dass die Vorhersagen von Modell 1 perfekt (sie sagen genau 6, 7, 8 voraus) 
und für Modell 2 und 3 nicht zu gebrauchen sind.
Was ist nun, wenn eine Gerade unseren Datensatz nicht ausreichend repräsentiert, zum Beispiel im 
Falle der Formel x = x2
? Klar, eine Gerade würde diese Formel tendenziell korrekt abbilden 
(die Gerade würde in Richtung des Wachstums der Kurve zeigen, aber im Unendlichen 
eine maximale Distanz erreichen), jedoch wäre ein Polynom passender, da wir den Verlauf 
der Zahlen dadurch wesentlich genauer annähern könnten. Schlauen Mathematikern zum 
Dank gibt es dafür die polynomiale Regression, die wir sogleich ausprobieren wollen, um zu 
schauen, ob wir die Güte unserer Modelle erhöhen können.
Listing 6.56 zeigt diese und ist zu weiten Teilen ähnlich dem vorherigen Listing 6.55, mit 
dem einzigen Unterschied, dass wir unsere Features, die wir in das LinearRegression-Modell 
eingeben, über PolynomialFeatures.for_transform() in den Zeilen 27–29 anpassen – 
oder genauer gesagt, anpassen lassen.
Listing 6.56 Polynomiale Regression zur Vorhersage von x
2
 und Fibonacci
1. import numpy as np
2. from sklearn.linear_model import LinearRegression
3. from sklearn.preprocessing import PolynomialFeatures
4.
5. # x
6. x1 = np.array([[0], [1], [2], [3], [4], [5]]) # x must be two dimensional
7. y1 = np.array([0, 1, 2, 3, 4, 5])
8. val_x1 = np.array([[6], [7], [8]]) # x must be two dimensional
9. val_y1 = np.array([6, 7, 8])
10.
11. # x^2
12. x2 = np.array([[0], [1], [2], [3], [4], [5]])
13. y2 = np.array([0, 1, 4, 9, 16, 25])
14. val_x2 = np.array([[6], [7], [8]])
15. val_y2 = np.array([36, 49, 64])
16.
17. # Fibonacchi
18. x3 = np.array([[0], [1], [2], [3], [4], [5]])
19. y3 = np.array([0, 1, 1, 2, 3, 5])
20. val_x3 = np.array([[6], [7], [8]])
21. val_y3 = np.array([8, 13, 21])
22.
23. test = np.array([[6], [7], [8]])
24.
25. def train_and_predict(x, y, val_x, val_y, test):
26. transformer = PolynomialFeatures(degree=2) # please try degree=3
27. x_ = transformer.fit_transform(x)
28. val_x_ = transformer.fit_transform(val_x)
29. test_ = transformer.fit_transform(test)
30. print(x_)
31.
32. model = LinearRegression()
33. model.fit(x_, y)



6.6 Wetterabfrage 237
34.
35. cod = model.score(val_x_, val_y)
36. print('Coefficient of determination: ', cod)
37.
38. predictions = model.predict(test_)
39. print('Predictions: ', predictions, sep='\n')
40.
41. if __name__ == '__main__':
42. print("x")
43. train_and_predict(x1, y1, val_x1, val_y1, test)
44. print("\n")
45.
46. print("Square")
47. train_and_predict(x2, y2, val_x2, val_y2, test)
48. print("\n")
49.
50. print("Fibonacci")
51. train_and_predict(x3, y3, val_x3, val_y3, test)
52. print("\n")
In Zeile 26 stellen wir einen Grad über den Parameter degree bereit, den der Transformator 
dafür verwendet, den Array x um n Features zu erweitern. Wie in Bild 6.44 a) zu sehen, wird 
bei einer polynomialen Regression zweiten Grades eine Spalte eingefügt, die das Quadrat 
x2
 der ursprünglichen Zahl in x beinhaltet. In b) kommt bei einem dritten Grad noch x3
 als 
Feature hinzu. Diese Transformationslogik müssen wir nun auf unsere Trainings-, Validie rungs- und Testdaten anwenden, da das Modell eine einheitliche Form der Daten erwartet.
Bild 6.44 Ergebnisse der polynomialen Regression zweiten (a) und dritten (b) Grades für x, x
2
 und 
die Fibonacci-Folge



238 6 Intents entwickeln
Die Ergebnisse für die polynomiale Regression zweiten Grades sehen Sie in Bild 6.44 a), 
die des dritten Grades in Bild 6.44 b). Der Determinationskoeffizient verbessert sich in a) 
für die Funktion x2
 drastisch und auch unsere Testdaten zeigen, dass das Modell nun in der 
Lage ist, x2
 verlässlich vorherzusagen. Die Fibonacci-Folge wird erst bei Hinzunahme eines 
dritten Grades besser und liegt für die Testdaten nur noch etwa ein Drittel daneben. Es 
sieht so aus, als würden unsere Modelle mit einem steigenden Grad immer besser werden. 
Warum nehmen wir dann nicht einfach immer einen möglichst hohen Grad und sparen uns 
die inkrementelle Suche nach einem passenden Modell?
Nehmen wir mal an, wir hätten ein Modell, das eine Punktwolke wie in Bild 6.42 abbildet, 
dann würden wir mit einer Regression zehnten Grades (wir benötigen zehn, denn wir haben 
ja auch zehn Trainingspunkte) eine Kurve erzeugen können, die all die Punkte einfängt, die 
in den Trainingsdaten vorhanden sind. Wie aber könnte diese ganz genau passende Kurve 
Daten abbilden, die das Modell noch nicht kennt? Wahrscheinlich gar nicht, unterliegt sie 
doch dem, was in der Statistik als Overfitting bezeichnet wird. Das Modell ist dabei den Trai ningsdaten zu genau angepasst, bildet jeden Ausreißer und jeden Messfehler ab, sodass es 
für den tatsächlichen Verlauf der Werte blind wird.
Kommen wir nun zurück zu unserem Bestreben, Wetterdaten vorherzusagen. Bild 6.42 zeigt, 
dass wir mit einer einfachen linearen/polynomialen Regression schon sehr weit kommen, 
jedoch weist die Temperatur eine Besonderheit auf. Sie ist saisonal verschieden und sinkt und 
steigt zyklisch, abhängig von Uhr- und Jahreszeit. Was denken Sie, wie eine Steigungsgerade 
aussähe, würden wir ein ganzes Jahr betrachten? Wahrscheinlich wäre diese relativ flach, 
würde sie doch den Durchschnitt durch alle Messpunkte abbilden und lediglich aufgrund 
des Effekts der globalen Erwärmung kaum merklich steigen. Die polynomiale Regression 
hingegen würde uns eine Funktion bilden, die zwar einen zeitlichen Ausschnitt unserer 
historischen Wetterdaten optimal abbildet, aber ihre Genauigkeit in ferner oder schon in 
naher Zukunft verlieren. Es muss also eine andere Methode her.
6.6.3.2 Beschaffung von Wetterdaten
Eine Regression lässt sich noch leicht mit einigen erdachten Daten erklären. Bevor wir aber 
in medias res gehen und Methoden betrachten, die auf realen und optimalerweise umfangrei cheren Trainingsdaten arbeiten, wollen wir uns doch echte Wetterdaten besorgen. Einerseits 
erhöht die Arbeit mit Realdaten die Motivation (zumindest meine) und andererseits können 
wir unsere Modelle auch zielgerichtet auf Fehler überprüfen. Na gut, das konnten wir bei 
der Fibonacci-Reihe eben auch, aber wir wollen ja einen naturgegebenen, atmosphärischen 
Zustand vorhersagen und keine Zahlenreihe. Der Unterschied ist, dass es für ersteren noch 
keine Formel gibt, um ihn zu berechnen.
Bei der Beschaffung von Wetterdaten sind wir glücklicherweise nicht mehr darauf angewiesen, 
das Internet nach mehr oder weniger aktuellen CSVs zu durchforsten, sondern können mitt lerweile auf APIs zurückgreifen, die uns genau die Daten in der Granularität zurückliefern, 
die wir benötigen. So bietet etwa der Deutsche Wetterdienst (DWD) seine Daten über einen 
Open Data Server9
 zum Download an, der wiederum von einigen Python Packages gekapselt 
wird, um diese Daten direkt in einer für uns nutzbaren Form in unserer Anwendung zu 
verarbeiten. Wir nutzen dafür das passend benannte Package wetterdienst, das sich einfach 
über pip installieren lässt.
9 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichwww.dwd.de/EN/ourservices/opendata/opendata.html



6.6 Wetterabfrage 239
Bild 6.45 Eine Vielzahl von Messstationen liefern ihre Daten an den Deutschen Wetterdienst und 
sind für uns über das Package wetterdienst abrufbar.
Öffnen Sie das Projekt 100_extras/100_14_arima und darin die Datei main_get_data.py, die, 
wie der Name schon andeutet, dafür verantwortlich sein wird, für unsere nächsten Projekte 
die Daten zu beschaffen, die wir für Training und Test benötigen. Die API von wetterdienst
ist so aufgebaut, dass wir zuerst eine Liste all der Messstationen ermitteln, für die wir dann 
die entsprechenden Messpunkte laden. Eine gute Anlaufstation ist dafür der Wetterdienst 
Explorer, der über pip install wetterdienst[explorer] installiert und über wetterdienst explorer
aufgerufen wird. Neben den entsprechenden Auswahlfeldern sehen Sie darin eine Karte aller 
Messstationen in Deutschland (siehe Bild 6.45), deren Daten der DWD aggregiert und zur 
Verfügung stellt. Die Kunst im Explorer sowie gleich auch bei der Verwendung der API ist 
es, die Parameter so zu wählen, dass sie von der Messstation auch geliefert werden.
Schauen wir uns aber zunächst an, wie wir die Stationen ermitteln, die in der Nähe des Ortes 
liegen, für den wir das Wetter vorhersagen möchten. Listing 6.57 zeigt das exemplarische 
Vorgehen. Wir definieren zuerst einen DwdObservationRequest, in dem wir angeben, dass 



240 6 Intents entwickeln
wir die Lufttemperatur mit einer Granularität von 10 Minuten abfragen möchten. Über die 
Parameter start_date und end_date können wir zwar einschränken, dass wir nur Stationen 
genannt bekommen, die in dieser Zeitspanne aktiv waren, das heißt jedoch noch nicht, 
dass diese auch die entsprechenden Daten liefern. Anschließend filtern wir die gefundenen 
Stationen nach ihrer Geokoordinate, sodass wir nur diejenigen 20 behalten, deren Distanz 
zu dem übergebenen Längen- und Breitengrad am geringsten ist. Das df am Ende des ersten 
Befehls konvertiert das Resultat in einen Pandas DataFrame, mit dem wir hervorragend 
weiterarbeiten können.
TIPP: Das Modul tabulate ist sinnvoll, wenn es darum geht, tabellarische Daten 
formatiert im Terminal auszugeben, ohne dass die Tabellen ihre Leserlichkeit ver lieren. Dessen Verwendung ist allerdings nicht verpflichtend.
Listing 6.57 Ermitteln aller Wetterstationen in der Nähe einer Koordinate
def get_station(start_date, end_date):
 stations = DwdObservationRequest(
 parameter=[DwdObservationDataset.TEMPERATURE_AIR],
 resolution=DwdObservationResolution.MINUTE_10,
 start_date=start_date,
 end_date=end_date
 ).filter_by_rank(49.19780976647141, 8.135207205143768, 20).df
 logger.info("\n"+tabulate(stations, headers='keys'))
 if (len(stations)) > 0:
 return stations.iloc[0]
 return None
Am Ende prüfen wir, ob eine Station gefunden wurde. Falls ja, geben wir die erste in der Liste 
zurück, falls nicht, None, um darauf hinzuweisen, dass die Parameter des Requests angepasst 
werden müssen oder eine andere Station ausgesucht werden muss.
Im nächsten Schritt beschaffen wir uns die Rohdaten für die ermittelte Station. In Listing 6.61
nutzen wir exakt denselben Request, mit dem wir eben die Stationsliste angefragt haben, 
bloß dass wir sie hier auf die ID der in Listing 6.57 gewählten Station filtern und per 
values.all().df alle Messdaten als DataFrame abrufen.
Listing 6.58 Beschaffung der nicht aufbereiteten Wetterdaten und der verfügbaren Messparameter
def get_data(station, start_date, end_date):
 logger.info("start date: {} ", start_date)
 logger.info("end date: {}", end_date)
 request = DwdObservationRequest(
 parameter=[DwdObservationDataset.TEMPERATURE_AIR],
 resolution=DwdObservationResolution.MINUTE_10,



6.6 Wetterabfrage 241
 start_date=start_date,
 end_date=end_date
 )
 station_data = request.filter_by_station_id(station['station_id']).values.all().df
 parameters = pd.unique(station_data['parameter']).tolist()
 return station_data, parameters
In der vorletzten Zeile verwende ich pd.unique(), um die einzigartigen Parameternamen 
herauszufinden, die im DataFrame mitgeliefert werden. Diese wollen wir später verwenden, 
um die Temperaturdaten zu filtern, die die Grundlage unserer Vorhersage bilden. Im letzten 
Schritt, zu sehen in Listing 6.59, löschen wir alle Datensätze, deren Spalte value fehlende 
Werte enthalten, denn diese würden im schlimmsten Fall unser Ergebnis verfälschen.
Es folgt die Konstruktion einer Aggregationsfunktion (die hinter den Kulissen nichts an deres ist als ein Dictionary), die für die jeweilige Spalte eine Funktion festlegt, die bei der 
Aggregation ausgeführt werden soll. Da wir mehrere Einträge mit einem gleichen Wert in 
der Spalte date haben können (zum Beispiel eine Zeile für den Parameter Temperatur, eine für den 
Parameter Luftfeuchtigkeit), legen wir zuerst fest, dass wir über die Funktion first() nur 
das erste Datum für alle nach Datum gruppierten Werte behalten wollen. Am Beispiel von 
Tabelle 6.4 werden zum Beispiel später die Zeilen 144 und 434 anhand des Datums gruppiert. Zeile 0 
wurde bereits gelöscht, da der Wert NaN und somit nicht vorhanden ist.
Tabelle 6.4 Wetterrohdaten vor Aggregation und Bereinigung
Id Stations-ID Dataset Parameter Datum Wert
0 00377 temperature_air pressure_air_site 2021-06-01 
00:00:00+00:00
NaN
1 00377 temperature_air pressure_air_site 2021-06-01 
00:10:00+00:00
NaN
2 00377 temperature_air pressure_air_site 2021-06-01 
00:20:00+00:00
NaN
…
144 00377 temperature_air temperature_air_
mean_200
2021-06-01 
00:00:00+00:00
287.25
145 00377 temperature_air temperature_air_
mean_200
2021-06-01 
00:10:00+00:00
287.25
146 00377 temperature_air temperature_air_
mean_200
2021-06-01 
00:20:00+00:00
287.15
…
432 00377 temperature_air humidity 2021-06-01 
00:00:00+00:00
55.3
433 00377 temperature_air humidity 2021-06-01 
00:10:00+00:00
55.8
434 00377 temperature_air humidity 2021-06-01 
00:20:00+00:00
55.8



242 6 Intents entwickeln
Ist das geschehen, werden alle Parameter für jedes Datum ermittelt, in unserem Falle tempe rature_air_mean_200 und humidity. Die Funktion sum, die in der Aggregationsfunktion für 
diese Felder gesetzt wird, summiert nicht wirklich Werte auf, sondern setzt diese vielmehr, 
denn im Kontext von beispielsweise humidity ist temperature_air_mean_200 gleich 0, da 
nicht vorhanden.
Listing 6.59 Vorverarbeitung der Wetterdaten
def preprocess_data(station_data, parameters):
 unique_dates = pd.unique(station_data['date'])
 logger.info("Unique dates: {} ", len(unique_dates))
 # drop nan
 station_data.dropna(subset=['value'])
 aggregegation_functions = {}
 aggregegation_functions['date'] = 'first'
 for p in parameters:
 station_data.loc[station_data['parameter'] == p, p] = station_data['value']
 aggregegation_functions[p] = 'sum'
 logger.info("Aggregation function: {}", aggregegation_functions)
 station_data = station_data.groupby('date').aggregate(aggregegation_functions)
 return station_data
Nach Ausführen der Vorverarbeitung haben die Daten die folgende Form: Alle Parameter 
bekommen eine eigene Spalte samt der gruppierten Werte nach Datum. Damit haben wir 
die Daten in Tabelle 6.5 in einer Form, mit der wir gut arbeiten können.
Tabelle 6.5 Wetterdaten nach der Gruppierung und Bereinigung
Datum temperature_
air_mean_200
temperature_
air_mean_5
humidity temperature_
dew_point_mean_200
2021-06-01 
00:00:00+00:00
287.25 284.85 55.3 278.45
2021-06-01 
00:10:00+00:00
287.25 284.85 55.8 278.55
2021-06-01 
00:20:00+00:00
287.15 284.85 55.8 278.45
Nun gilt es noch, die gesehenen Funktionen in der richtigen Reihenfolge wie in Listing 6.60
auszuführen. Unsere Anwendung lädt folgerichtig die Wetterdaten für die selektierte Sta tion im Zeitraum vom 01.01.2021 bis zum 01.06.2021 herunter und speichert diese als 
weather_data.csv im Verzeichnis des Skripts.



6.6 Wetterabfrage 243
Listing 6.60 Ausführen der Wetterdatenbeschaffung
import os
from wetterdienst.provider.dwd.observation import
 DwdObservationRequest,
 DwdObservationResolution,
 DwdObservationDataset
from datetime import datetime
import pandas as pd
from tabulate import tabulate
from loguru import logger
WEATHER_DATA = os.path.join("weather_data.csv")
if __name__ == '__main__':
 start_date = datetime.fromisoformat('2021-01-01 00:00:00')
 end_date = datetime.fromisoformat('2021-06-01 23:59:59')
 station = get_station(start_date, end_date)
 logger.info("Found station {}.", station)
 data, parameters = get_data(station, start_date, end_date)
 logger.info("Read data with {} entries.", len(data))
 logger.info("Parameters: {}", parameters)
 data = preprocess_data(data, parameters)
 data.to_csv(WEATHER_DATA, index=False)
 logger.info("Weather file {} written.", WEATHER_DATA)
Sie sind nun in der Lage, für beliebige Messstationen und Zeiträume Wetterdaten herunter zuladen. Achten Sie jedoch immer darauf, dass die Stationen die gewünschten Daten auch 
liefern. Da wir uns nun bereits um die Datenbeschaffung gekümmert haben (was in einem 
Data-Science-Projekt häufig ca. 80 % der Arbeit ausmacht), können wir uns frohen Mutes 
in die Analyse stürzen.
6.6.3.3 Autoregression, Moving Average Model und ARIMA
Die Regressionsanalyse, die wir eben kennengelernt haben, kann mit wenigen Worten so 
beschrieben werden, dass sie statistisch ermittelt, ob eine bestimmte Variable – in unserem 
Fall die Temperatur – von einer anderen Variablen – bei uns die Zeit – abhängt. Was ist 
aber, wenn wir keinen Zusammenhang zweier Variablen herstellen können oder uns nur 
ein Wert zu Verfügung steht? Hier kommt die Autoregression ins Spiel. Diese ist speziell für 
Zeitreihenanalysen gedacht und kann einen Wert auf Basis dessen Historie vorhersagen. 
Wenn wir zum Beispiel wissen, wie die Temperatur im letzten Monat war, können wir aus 
diesen historischen Daten per Autoregression auf die Temperatur morgen schließen. Das geht 
allerdings nur, wenn die Werte korrelieren, also wenn ein Zusammenhang zwischen ihnen 
besteht. Die Lottozahlen vorherzusagen wäre demnach kein Anwendungsfall für eine Auto regression, denn die wöchentlich gezogenen Zahlen hängen nicht voneinander ab, sondern 
jede Ziehung unterliegt dem Zufallsprinzip. Das Wetter, Aktienkurse oder Absatzzahlen 
hingegen sind schon voneinander abhängig.



244 6 Intents entwickeln
Die Idee, die hinter der Autoregression steckt, ist denkbar einfach. Ebenso, wie wir die Va riablen einer linearen Regression optimieren (siehe Formel 6.8), können wir diese Technik 
auch bei Zeitreihenanalysen einsetzen.
y c bx = +⋅ Formel 6.8
So machen wir in Formel 6.9 den vorhergesagten Wert xt
 abhängig von den zwei vorherge henden Beobachtungen xt–1 und xt–2, wobei diese durch b1 und b2 unterschiedlich gewichtet 
werden und durch c einen konstanten Ausgangswert, analog zum y-Achsenabschnitt der 
linearen Regression, zugewiesen bekommen. Diese jeweiligen Parameter zu bestimmen ist 
Aufgabe des Trainingsalgorithmus. Das Epsilon bezeichnet einen möglichen Fehler.
e − − =+ + + ⋅ ⋅ 1 12 2 t t tt x cbx bx Formel 6.9
TIPP: Wir sind frei in der Anzahl der Werte (Lag Variables) und deren Faktoren (bx), 
die wir rückwirkend betrachten können, um den nächsten Wert der Zeitreihe vor auszusagen. Die Betrachtung der zwei vergangenen Werte dient hier lediglich als 
Beispiel und es wäre valide, einen dritten, vierten oder fünften historischen Wert 
zu betrachten – oder auch nur einen. Die Anzahl dieser Werte bezeichnet man 
als Ordnung p. Wenn Sie in Ihr Statistikbuch oder auf Wikipedia schauen, werden 
Sie die Formel mit einem Summenzeichen vorfinden, über das das Produkt der 
jeweiligen Ordnung von 1 bis p aufsummiert wird. Ich bin aber absolut kein Freund 
von mathematischen Notationen, da Code vermutlich für 99 % aller Menschen ein facher lesbar ist.
In Python-Code ausgedrückt könnte die Funktion wie in Listing 6.61 gezeigt aussehen. Be halten Sie bei der Betrachtung im Hinterkopf, dass unser Ziel ist, die Liste b per Training zu 
bestimmen und nicht wie gezeigt vorzugeben.
Listing 6.61 Berechnung der Autoregression anhand fiktiver Werte
x = [10.2, 20.1, 15.5, 11.8, 18.1, 9.9]
b = [0.1, 0.2, 0.9, 0.3, 0.3, 0.2] # die per Training zu bestimmenden Parameter
et = 0.2 # optionales Rauschen
order = 5 # Ordnung des Modells
c = 12.0
result = c + et
for i in range(1, order+1):
 result += b[i-1]*x[-i]
print(result)
Haben wir die Werte in b durch ein trainiertes Modell berechnen lassen, könnten wir unsere 
Funktion verwenden, um Voraussagen für den nächsten Wert in x nach 9.9 treffen zu lassen.



6.6 Wetterabfrage 245
Moving Average Model
Im Kontext von Zeitreihenanalysen wird die Autoregression häufig im selben Atemzug mit 
dem Moving Average Model genannt. Das Moving Average Model macht Gebrauch von ver gangenen Fehlern, um Vorhersagen für zukünftige Werte zu treffen.
HINWEIS: Ich erinnere mich noch daran, dass ich zu Beginn meiner Arbeit mit 
Zeitreihen ein Verständnisproblem hatte, das mich einige Tage beschäftigt hat. 
Und zwar war mir nicht bewusst, dass das Moving Average Model (oder Moving 
Average Process) mit dem statistischen Verfahren Moving Average nichts (oder 
nur wenig) zu tun hat. Vielleicht lächeln sie jetzt, weil Ihnen diese Tatsache schon 
längst bekannt war. In dem Fall können Sie diesen Kasten getrost überspringen. 
Für alle anderen möchte ich eine kurze Erklärung zum Moving Average liefern, um 
beide Vorgehensweisen differenzieren zu können.
Schwierig ist nämlich, dass beide Begriffe häufig im selben Kontext genannt wer den. Das Moving Average (gleitender Mittelwert oder gleitender Durchschnitt) kann 
jedoch recht einfach berechnet werden und ist demnach nicht konkret als Modell 
zu betrachten.
Dessen Verwendung ist vor allem bei der Analyse von Finanzdaten interessant, 
erfüllt es doch folgende Aufgaben:
 Glättung von Zeitreihen und Filtern von hohen Frequenzanteilen
 Indikator für Trends, der Signale für Käufe und Verkäufe am Finanzmarkt geben 
kann
Bei der Bildung besagten Durchschnitts wird jedoch nicht der Mittelwert aller 
Werte für die Glättung verwendet, sondern nur ein Ausschnitt der vergangenen 
Werte, ein gleitendes Fenster. Wir summieren die letzten n Werte unserer Zeit reihe auf und teilen sie durch deren Anzahl (der Order). Dank Pandas können wir 
Moving-Average-Werte mit schwindend geringem Aufwand erzeugen.
Listing 6.62 Bilden des Moving Average für eine Reihe von Temperaturwerten
series[‚moving average‘] = series[‚temperature‘].rolling(window=10).mean()
Listing 6.62 verwendet die Funktion rolling() auf dem Pandas DataFrame 
series, um ein Fenster der letzten 10 Werte zu bekommen und wendet darauf 
die Funktion mean() an, um den Mittelwert zu bilden. Die Notation ist für Nicht Python-Entwickler etwas gewöhnungsbedürftig, denn es ist auf den ersten Blick 
nicht zu erkennen, dass diese Operation für jedes Objekt im DataFrame ausge führt wird.
Bild 6.46 zeigt das Ergebnis der Berechnung. Warum aber bekommen wir für die 
ersten zehn Werte kein Ergebnis? Nun, es kann schlicht kein Fenster von zehn vor herigen Werten ermittelt werden, da diese nicht verfügbar sind. Erst ab dem elften 
Wert stehen zehn vorherige Werte zu Verfügung. Eine Visualisierung des Moving 
Average sehen Sie in Bild 6.47.



246 6 Intents entwickeln
Bild 6.46 Moving Average für eine Liste von 20 Temperaturmessungen, berechnet über 
ein gleitendes Fenster von zehn Werten.
Bild 6.47 Betrachtung von ca. 52.000 Temperaturmessungen und dem Moving Average 
mit einer Fenstergröße von 1000. Es ist gut zu sehen, dass die Kurve durch das Ent fernen von hohen Abweichungen des gleitenden Mittelwerts geglättet wird. Je größer die 
Fenstergröße, desto glatter die Kurve.
Ich hoffe, dass dieser kurze Exkurs für Klarheit und nicht etwa für Verwirrung 
sorgt. Im Folgenden werden wir nur noch vom Moving Average Model sprechen. 
Als kleine Gedankenstütze kann man sich merken: Wenn von Fehlern oder Order
die Rede ist, handelt es sich um das Moving Average Model.



6.6 Wetterabfrage 247
Das Moving Average Model lässt sich ebenso wie die Autoregression über eine Formel abbil den. Formel 6.10 zeigt wieder exemplarisch ein Modell der Ordnung 2. Die Implementierung 
in Python spare ich mir hier, da diese der Autoregression fast in Gänze gleicht, bloß dass 
die Summanden aus Produkten von Parametern und Fehlern bestehen und nicht wie in der 
Autoregression aus Parametern und vergangenen Werten.
ee e − − =+ + + ⋅ ⋅ 1 12 2 t tt t xc d d Formel 6.10
Auch hier bezeichnet c einen konstanten Wert und Epsilon einen Fehler. Zu abstrakt? Hier 
ein kleines Beispiel aus dem Leben: Meine Kinder nehmen seit kurzem gerne Müsli mit in 
den Kindergarten, das meine Frau und ich akribisch vorbereiten. Nun unterliegt aber die 
Menge an Müsli einem sich ständig ändernden Fehler. Wir sind zu Beginn der Müslizuberei tung davon ausgegangen, dass wir mit c = 40 Gramm Müsli gut fahren und alle satt werden. 
Nun kommen aber die Kinder am ersten Tag zurück und sagen, dass das viel zu wenig war, 
60 Gramm wären gut gewesen. Unser Fehler lag bei et = 20. Nun wenden wir Formel 6.10 an, 
allerdings der Einfachheit halber erster Ordnung (das d2 · et–2 lassen wir weg, ebenso et
, da 
noch nicht bekannt), und gehen davon aus, dass wir die Müslimenge erst mal um den Faktor 
d1 = 0.5 anpassen, falls die Kinder morgen doch nicht so viel Hunger haben. Am nächsten 
Tag präparieren wir die Menge:
x =+ ⋅= 40 0.5 20 50
Am nächsten Tag versuchen wir unser Glück mit 50 Gramm und siehe da, die Kinder essen 
nur 40 Gramm. Unser Fehler lag bei et = –10. Wir wenden erneut die Formel an und geben 
ihnen diesmal x = 40 + 0.5 · –10 = 35 Gramm mit. Nehmen wir an, wir hätten Zeit, die 
Nahrungsaufnahme unserer Kinder statistisch über fünf Tage zu erfassen, dann würde sich 
Tabelle 6.6 abbilden lassen.
Tabelle 6.6 Beispielhaftes Moving-Average-Modell der Ordnung 1
t xt εt xt – εt
1 40 20 60
2 50 –10 40
3 35 0 35
4 40 –10 30
5 35 25 60
Visualisieren wir xt – et
, wie in Bild 6.48 geschehen, wird klar, wie das Modell zu seinem 
Namen kam. Die Werte, die wir durch dessen Anwendung ermitteln, bewegen sich nämlich 
immer um den Wert von c, in unserem Fall 40. Man sagt, das Modell ist stationär, dazu später 
mehr. Natürlich kann es Fälle geben, in denen die Kurve lange Zeit über bzw. unter c ver weilt, jedoch entfernt sie sich nicht maßgeblich von diesem Wert, da c in jeder Iteration den 
Ausgangspunkt bestimmt. Die Betrachtung in unserem Beispiel basiert auf einem Modell 
der Ordnung 1 (abgekürzt MA(1)). Diese Ordnung ließe sich nun erhöhen, indem wir nicht 
nur den Fehler der letzten Iteration, sondern auch den der vorletzten betrachten. Dann wären 
wir erneut bei Formel 6.10. Auch höhere Ordnungen sind natürlich möglich.



248 6 Intents entwickeln
0
10
20
30
40
50
60
70
1234 5
Müsli in Gram
m
Tag
Bild 6.48 Das Moving-Average-Modell der Ordnung 1 bewegt sich durch Subtraktion bzw. Addition 
des letzten Fehlers um den Wert c, hier mit c = 40.
Der autoregressive gleitende Mittelwert ARMA
Nun sind wir in der Lage, Zeitreihen vorauszusagen. Einmal auf Basis der Fehler, die bei 
vorherigen Vorhersagen gemacht wurden (Moving Average Model) sowie auf Basis der vor herigen Werte der Zeitreihe (Autoregression). Nun kam ein schlauer Kopf auf die Idee, diese 
beiden Vorgehensweisen im ARMA-Modell zu verbinden. Dieses ergibt sich ganz einfach 
aus der Kombination der Formeln Formel 6.9 und Formel 6.10 unter Berücksichtigung der 
jeweiligen, übereinstimmenden Ordnung. Das Ergebnis sehen Sie in Formel 6.11.
e e e −−−− =+ + + + ⋅⋅⋅ + ⋅ t tt t t t 1 12 2 1 1 2 2 xc b b d d x x Formel 6.11
Wenn ein ARMA-Modell für eine Vorhersage von Zeitreihen Verwendung findet, wird es durch 
die Ordnung p der Autoregression und die Ordnung q des Moving Average parametrisiert, 
weswegen man häufig die Notation ARMA(p,q) vorfindet.
Stationarity
Dass wir uns zuvor den gleitenden Mittelwert angeschaut haben, kommt uns doch noch mal 
zugute, denn dort habe ich Ihnen in Bild 6.47 einen einjährigen Ausschnitt der Temperatur daten einer Messstation im Pfälzer Wald gezeigt. ARMA unterliegt nun der Einschränkung, 
dass die Zeitreihe stationär sein muss. Das heißt:
 Sie darf keinem Trend unterliegen und nicht über längere Zeit konstant steigen oder ab fallen.
 Es dürfen keine saisonalen Effekte auftreten, wie zeitabhängige Hebungen und Senkungen.
Bild 6.49 zeigt ein paar Beispiele für stationäre und nicht stationäre Daten. In a) sehen Sie 
ein Elektrokardiogramm, das entsprechend des gesunden Herzschlags eines Menschen re gelmäßig auf beiden Kanälen Hebungen aufweist. Diese sprechen dafür, dass ein EKG nicht 
stationär ist. Ebenfalls nicht stationär ist der Aktienkurs in b), der einen langfristigen, stei genden Trend aufweist. Als stationär zu bezeichnen wären beispielsweise 300 hintereinander 
ausgeführte Münzwürfe, die zugegebenermaßen nicht das beste Beispiel für Zeitreihendaten 
sind, aber schön illustrieren, wie ein stationärer Datensatz aussehen kann.
Was bedeutet das nun für unsere Temperaturdaten bzw. würden Sie diese anhand der 
Kriterien klassifizieren? Wie sähe es mit der Wellenform einer Spracheingabe durch einen 



6.6 Wetterabfrage 249
Benutzer unseres Sprachassistenten aus? Wäre diese stationär oder nicht stationär? Für das 
Temperaturbeispiel ist schnell begründet, warum die Temperaturdaten nicht stationär sind, 
denn sie unterliegen saisonalen Effekten. Im Sommer ist es warm, im Winter ist es kalt.
a)
b)
c)
Kopf Zahl
Bild 6.49 Beispiele für stationäre (c – zufälliger Münzwurf) 
und nicht stationäre (a – EKG, b – Aktienkurs Siemens) Daten
Bei der Wellenform von Audiodaten ist es nicht ganz so einfach. Zwar kann man feststellen, 
dass zwischen den einzelnen Wörtern und in der Regel zu Beginn und am Ende eine Stil leperiode zu finden ist, doch sind diese Muster wirklich zeitabhängig? Zum Glück gibt es 
Wege, um nachzuprüfen, ob eine Zahlenreihe stationär ist. Einer der bekanntesten ist der 
Augmented Dickey-Fuller Test und diesen wollen wir uns nun anschauen.
Der 1979 von David Dickey und Wayne Fuller entwickelte Test gehört zu der Klasse der Ein heitswurzeltests. Liegt eine Einheitswurzel vor, hat die Zahlenreihe einen Trend und ist 
nicht stationär. In diesem Fall kann ARMA nicht angewandt werden, ohne dass wir zuvor 
den Trend aus der Reihe herausrechnen. Wie unkompliziert das Verfahren anzuwenden ist, 
sehen wir in Listing 6.63. Selbiges finden Sie ebenfalls im Ordner 100_extras\100_14_arima
in der Datei main_dickeyfuller.py.



250 6 Intents entwickeln
Listing 6.63 Augmented Dickey-Fuller Test für Wetterdaten und Audiowellenformen
1. import pandas as pd
2. from statsmodels.tsa.stattools import adfuller
3. import matplotlib.pyplot as plt
4. import wave
5. import numpy as np
6. from scipy.io.wavfile import read
7.
8. # Beispiel 1 - Temperaturdaten
9. series = pd.read_csv('weather_data.csv', header=0, index_col=0, parse_dates=True)
10.
11. # Kelvin zu Celsius
12. series['temperature_air_mean_200'] = series['temperature_air_mean_200'].
 transform(lambda x: x - 273.15)
13.
14. # Lösche alle Spalten, die wir nicht benötigen
15. series = series.drop(
16. columns=['pressure_air_site', 'temperature_air_mean_005', 'humidity',
17. 'temperature_dew_point_mean_200']
18. )
19.
20. plt.plot(series)
21. plt.show()
22.
23. result = adfuller(series)
24. is_stationary = False
25. if result[1] < 0.05:
26. is_stationary = True
27. print('p für Temperaturdaten: ', result[1], " Reihe ist stationär: ", 
 is_stationary)
28.
29.
30. # Beispiel 2 - Audiowellenform
31. f = read("test.wav")
32. signal = np.array(f[1], dtype=float)
33.
34. plt.plot(signal)
35. plt.show()
36.
37. result = adfuller(signal)
38. is_stationary = False
39. if result[1] < 0.05:
40. is_stationary = True
41. print('p für Waveform: ', result[1], " Reihe ist stationär: ", is_stationary)
Das Ergebnis der Ausgabe sehen Sie in Bild 6.50 und, wie erwartet sind sowohl die Tempe raturdaten als auch die Wellenform sind stationär. Gut, bei der Wellenform waren wir uns 
nicht ganz sicher, weswegen wir uns auch den Dickey-Fuller Test angeschaut haben, aber 
bezüglich der Temperatur liegt es doch einfach auf der Hand, dass deren Messwerte einer 
Saisonalität unterliegen.



6.6 Wetterabfrage 251
Bild 6.50 Das Ergebnis des Dickey-Fuller Tests zeigt, dass sowohl die Temperaturkurve als auch die 
Wellenform einer beispielhaften Sprachaufnahme stationär sind.
Zugegeben, ich bin nicht ganz ohne Hintergedanken in dieses Fettnäpfchen getreten. Eine 
wichtige Lektion, die ich bei der Arbeit mit Daten gelernt habe, ist: Traue deiner Intuition, 
prüfe aber immer noch mal methodisch korrekt nach. Ich weiß nicht, wie oft es mir schon 
passiert ist, dass ich mit einer bestimmten Erwartung an einen Datensatz herangegangen bin 
und am Ende des Tages einfach nicht das eingetreten ist, was ich erwartet habe oder was laut 
Theorie hätte eintreten müssen. In so einer Situation hat man dann genau drei Möglichkeiten:
1. Man verbiegt die Daten so weit, dass das erwartete Ergebnis eintritt. Dadurch stehen Sie 
natürlich gut da und Sie haben für Außenstehende ein ansehnliches Resultat erarbeitet. 
Sie können aber davon ausgehen, dass Ihnen niemand, der in der Praxis mit den Folgen 
Ihrer Analyse arbeiten muss, in der Pause noch einen Kaffee spendiert oder Ihr Paper im 
Falle einer wissenschaftlichen Forschungsarbeit gut dastehen lässt.
2. Man geht noch einmal in sich und überprüft Methode und Daten auf mögliche Denkfehler, 
denn was für traditionelles Software Engineering gilt, gilt auch für Data und Machine 
Learning Engineering: Häufig handelt es sich um ein Layer-8-Problem10. Das bedeutet, 
dass das Problem häufig vor dem Bildschirm sitzt.
3. Man akzeptiert, dass man einem Irrtum unterlegen ist.
Vorbildlich, wie wir sind, wollen wir uns mit der zweiten Methode befassen, und hinter fragen unsere Daten. Man könnte schließlich vermuten, dass die Temperaturdaten jedes 
Jahr je nach Jahreszeit schwanken, wir ja nur ein Jahr betrachten, wie in Bild 6.47 zu sehen, 
und die Saisonalität bei einem größeren Informationsraum (zum Beispiel bei einer Messdauer von 
vier Jahren) an Bedeutung verliert. Im Ordner 100_14_arima befinden sich vier Datensätze:
 weather_data_1_year.csv
 weather_data_2_years.csv
 weather_data_3_years.csv
 weather_data_4_years.csv
Führen Sie den Dickey-Fuller Test über main_dickeyfuller.py gerne mal für alle vier Daten sätze aus. Dazu müssen Sie lediglich den Dateinamen im Quelltext ersetzen. Sie werden 
sehen, dass der Test robust genug ist, um für jeden Testfall die Stationarität zu bestätigen. 
Wir ändern unser Vorgehen gegenüber Methode Drei aus obiger Liste und akzeptieren, dass 
wir mit der Vermutung, dass Temperaturdaten nicht stationär sind, falsch gelegen sind – na 
gut, Sie vielleicht nicht, aber ich.
Das ist jedoch gar nicht so schlimm, denn ARMA funktioniert ja mit stationären Daten. 
Somit steht unserer Temperaturvorhersage nichts im Weg. Doch was ist, wenn wir nur 
kurze Temperaturausschnitte betrachten wollen, zum Beispiel von Januar bis Mai? Hier werden wir 
einen klaren Trend sehen, denn in der Regel wird es vom Winter zum Sommer hin wärmer. 
10 Die Layer beziehen sich auf das OSI-Modell aus dem Bereich der Netzwerkprotokolle, das lediglich sieben tech nische Schichten aufweist und spaßeshalber um eine achte Schicht – den Menschen – erweitert wurde, um auf 
ihn als eine der häufigsten Fehlerquellen hinzuweisen.



252 6 Intents entwickeln
Oder wie sieht es mit der Vorhersage von Aktienwerten aus? Auch hier wäre eine Vorhersage 
sicher für den ein oder anderen Investor interessant, doch stationär sind Aktienkurse nun 
von Natur aus nicht. Zum Glück gibt es eine Möglichkeit, den Trend in einer Zeitreihe zu 
neutralisieren. Dazu schauen wir uns das ARIMA-Modell an.
Erzeugung von Stationarität durch Differenzenbildung mittels ARIMA
Vielleicht konnten Sie es bis hierher gar nicht abwarten, eine eigene Vorhersage für Ihre Daten 
durchzuführen und haben schon fleißig recherchiert, wie ARMA denn nun über Scikit Learn
oder Statsmodels angewandt wird – und haben kein entsprechendes Modell gefunden. Statt dessen stößt man in allen Fällen auf die Funktion ARIMA, die beinahe so ähnlich klingt (das 
wissen wir, weil wir im Kopf erfolgreich eine 1 für die Levenshtein-Distanz aus Abschnitt 6.5.5
ermittelt haben), nur eben um ein „I“ ergänzt wurde. Das „I“ steht für Integrated und wird 
durch eine dem Modell vorgeschaltete Differenzenbildung repräsentiert, die den Trend aus 
einer Zeitreihe eliminiert, einfach, indem jeder Wert um seinen Vorgänger reduziert wird. 
Es gilt also für jeden Wert der neuen Zeitreihe die Formel 6.12.
∆= − −1 ttt yyy Formel 6.12
Tabelle 6.7 zeigt ein Beispiel, in dem wir den Trend aus steigenden Temperaturdaten durch 
das Bilden eines Deltas eliminieren – der Datensatz wird dadurch stationär.
Tabelle 6.7 Exemplarische Differenzenbildung für ein ARIMA-Modell für Temperaturdaten
Neue Temperatur Δyt Temperatur heute yt Temperatur gestern yt–1
1 12 11
2 14 12
2 16 14
2 18 16
2 20 18
2 22 20
Nichts anderes tut ARIMA für uns. Um dem Modell mitzuteilen, dass wir diese Differenzen bildung wünschen, müssen wir es lediglich um eine weitere Ordnung für I ergänzen. Den 
dafür notwendigen Parameter nennen wir d, sodass sich das Modell formal korrekt mit 
ARIMA(p,d,q) betiteln lässt. Setzen wir d = 0, wird auf eine Differenzenbildung verzichtet 
und es wird somit das einfache ARMA-Modell angewandt.
HINWEIS: Da wir für den ersten Wert in der Zeitreihe keinen Vorgänger ermitteln 
können, aus dem wir ein Delta bilden, ist die neue Zeitreihe um einen Wert ärmer 
als vor der Differenzenbildung.
In der Regel findet man selten Fälle, in denen bei Datensätzen, die einen Trend aufweisen, 
eine Order d > 1 verwendet wird. Sollten Sie wirklich mal auf den Fall stoßen, dass die Dif ferenzenbildung dann immer noch stetig wächst oder sinkt, ist es möglich, d = 2 zu wählen. 
Die Differenzenbildung zweiter Ordnung wird über die Formel 6.13 abgebildet.



6.6 Wetterabfrage 253
∆ = − − − = −⋅ + ( − −− −− 1 12 12 ) ( ) 2 t tt t t t t t y yy y y y y y Formel 6.13
Der letzte Schritt ist, die bereits bekannte Formel 6.11 für ARMA auf die stationäre Zeit reihe anzuwenden. Somit sind wir in der Theorie bereit, ein ARIMA-Modell zu berechnen. 
Wie aber sieht es in der Praxis aus? Öffnen Sie einmal die Datei main_arima.py, die sich im 
Ordner 100_14_arima befindet.
Wir wählen zuerst die Datei mit den Trainingsdaten aus, die wir in Abschnitt 6.6.3.2 in 
mühevoller Arbeit automatisiert beschafft haben. Sie finden im Ordner 100_14_arima die 
vier CSV-Dateien wieder, die wir zum Erproben des Dickey-Fuller-Tests verwendet haben. 
Sie beinhalten die Wetterdaten für 1, 2, 3 oder 4 Jahre. Wir entscheiden uns in Zeile 10 erst 
mal für die Daten eines Jahres.
Ab Zeile 12 beginnt nun die Logik unserer Anwendung, die zunächst die von uns gewählten 
Daten über Pandas in einen DataFrame einliest und dabei die Spalte date als Index-Spalte 
festlegt, was bei der Arbeit mit Zeitreihendaten Standard ist und auch von den meisten 
Frameworks erwartet wird. Weiterhin spezifizieren wir, dass wir noch die Spalte tempera tur_air_mean_200 verwenden und alle anderen Spalten in der CSV-Datei ignorieren möchten.
Da die Temperatur in Kelvin dargestellt wird und Nicht-{Physiker, Chemiker, Meteorologen} 
Probleme damit haben werden, die Werte ohne längeres Nachdenken lesen zu können, rechnen 
wir diese kurzerhand in Zeile 19 in Grad Celsius um, was durch eine einfache Subtraktion 
des Wertes 273,15 erledigt werden kann.
Nun dünnen wir unsere Trainings- und Testdaten etwas aus. Sie erinnern sich sicher, dass 
wir die Wetterdaten in einem 10-Minuten-Rhythmus von der API abgefragt haben. Lassen 
Sie uns die Granularität zugunsten der Laufzeit der Modellerstellung etwas verringern. Sie 
werden sehen, dass das der Güte des Modells keinen Abbruch tut. Dieses Verringern er reichen wir, indem wir einen Filter auf unsere Daten anwenden, der die Datensätze behält, 
deren Zeitstempel die Minute 0, die Sekunde 0 und die Stunde 12 aufweist. Wir wollen nur 
die Temperatur um 12 Uhr mittags betrachten – und demnach auch dort die Temperatur 
vorhersagen! Den Filter, den wir in den Zeilen 23–24 konstruieren, wenden wir in Zeile 25 
an. Da wir nun keine bestimmte Uhrzeit, sondern nur noch einen Messwert pro Tag be trachten, ändern wir kurzerhand in Zeile 28 das Datumsformat unserer Indexspalte date, 
um die Lesbarkeit zu erhöhen.
Nun trennen wir unsere vorhandenen Daten in Test- und Trainingsgruppen. Die Trainings daten bestehen aus 75 % unserer Gesamtdaten, die Testdaten dementsprechend aus dem Rest 
(Zeilen 33–35). Darauffolgend legen wir die zwei Listen history und predictions an. Erstere 
beinhaltet initial alle Temperaturdaten des Trainingsdatensatzes und predictions bleibt leer.
Was wir nun tun möchten, ist, für alle Daten in history ein ARIMA-Modell zu trainieren, uns 
den nächsten Wert vorhersagen zu lassen und diesen dem tatsächlichen Wert aus den Test daten gegenüberzustellen (Zeile 55).
Listing 6.64 Training eines ARIMA-Modells zur Vorhersage von Temperaturdaten
1. import os
2. import csv
3. import pandas as pd
4. from matplotlib import pyplot as plt
5. from loguru import logger



254 6 Intents entwickeln
6. from statsmodels.tsa.arima.model import ARIMA
7. from math import sqrt
8. from sklearn.metrics import mean_squared_error
9.
10. WEATHER_DATA = os.path.join("weather_data_1_year.csv")
11.
12. if __name__ == '__main__':
13.
14. # Lese Daten und interpretiere Spalte 'date' als Index
15. data = pd.read_csv(WEATHER_DATA, index_col='date',
16. usecols=['date', 'temperature_air_mean_200'])
17.
18. # Rechne Kelvin in Grad Celsius um
19. data['temperature_air_mean_200'] = data['temperature_air_mean_200'] - 273.15
20.
21. # Behalte nur die Tagestemperatur mittags, 12 Uhr
22. data.index = pd.to_datetime(data.index)
23. m = (data.index.minute == 0) & (data.index.second == 0) &
24. (data.index.hour == 12)
25. data = data[m]
26.
27. # Passe das Datumsformat an
28. data.index = pd.to_datetime(data.index.strftime('%Y-%m-%d'))
29.
30. logger.info("Anzahl Datensätze {}.", len(data))
31. print(data.head())
32.
33. X = data.values
34. size = int(len(X) * 0.75)
35. train, test = X[0:size], X[size:len(X)]
36. history = [x for x in train]
37. predictions = list()
38.
39. # Vorwärtsgerichtete Vorhersage jeweils des nächsten Wertes
40. for t in range(len(test)):
41.
42. # Die Ordner des Arima-Modells werden nicht optimiert auf
43. # 1,1,1 festgelegt (Siehe Hinweiskasten)
44. model = ARIMA(history, order=(1,1,1))
45. model_fit = model.fit()
46. output = model_fit.forecast()
47. yhat = output[0]
48. predictions.append(yhat)
49. obs = test[t]
50.
51. # Füge die vorhergesagte Temperatur der Datengrundlage für
52. # das nächste Training hinzu.
53. history.append(obs)
54.
55. logger.info("Vorhersage: {}, tatsächlicher Wert: {}", yhat, obs)
56.
57. # Berechne Fehler der Vorhersage
58. rmse = sqrt(mean_squared_error(test, predictions))
59. logger.info("Root-Mean-Squared Error: {}", rmse)
60.



6.6 Wetterabfrage 255
61. # Visualisierung der Güte des Modells
62. plt.plot(test)
63. plt.plot(predictions, color='red')
64. plt.show()
In Zeile 40 iterieren wir über die Menge an Testdaten und trainieren in jeder Iteration ein 
neues ARIMA-Modell auf Basis der Trainingsdaten konkateniert mit den bereits durchlau fenen Testdaten. Dieses Modell nutzen wir, um eine Vorhersage für den nächsten Wert zu 
treffen. Diese Vorhersage schreiben wir für den späteren Vergleich in die Liste predictions.
Nachdem wir in der Schleife die letzte Vorhersage gemacht haben, berechnen wir den Root Mean-Squared Error (Wurzel der mittleren Fehlerquadratsumme) – ein Maß, um die Unter schiede von Vorhersagen eines Modells zu den tatsächlichen Werten zu bestimmen. Derartige 
Maße gibt es viele – alleine 33 verschiedene im Paket sklearn.metrics, die für Klassifikationen, 
Clustering oder, wie in unserem Fall, für eine Regression herangezogen werden können.
TIPP: In zweierlei Hinsicht möchte ich noch für Klarheit sorgen:
 Wir müssen klären, wie die Metriken, wie etwa der RMSE, zur Verbesserung der 
Modellgüte beitragen können.
 Wir haben viel über die Ordnung von AR, MA, ARMA und ARIMA-Modellen ge hört, diese aber in der Implementierung einfach so aus dem Bauch heraus auf 
(p = 1, d = 1, q = 1) gesetzt.
Nützlicherweise können wir beide Fragestellungen in einem Rutsch klären, denn 
wir wollen die erstellte Metrik dafür nutzen, um die optimale Ordnung für unsere 
Daten zu finden. In der Theorie können wir dafür auf einige Funktionen zurück greifen, wie etwa ACF (Autocorrelation Function), PACF (Partial Autocorrelation 
Function), AIC (Akaike's Information Criterion) oder BIC (Bayesian Information 
Criterion), die uns im Falle von ACF und PACF einen visuellen Hinweis darauf 
geben können, welche Ordnung wir zum Beispiel für den Autoregressionsanteil p setzen 
sollten oder im Falle von AIC und BIC eine numerische Bewertung des Modells 
mit den jeweiligen Parametern zurückliefert. Die eben genannten vier Methoden 
möchte ich jedoch gerne Ihrem Selbststudium überlassen und den pragmatischen 
Weg gehen – denn wie so oft steht uns für die Anwendung und Optimierung 
unseres AR(I)MA ein fertiges Modul zu Verfügung. Zu nennen ist hier pmdarima
(manchmal auch noch als Pyramid bezeichnet), das AIC und BIC nutzt, um das 
beste Modell (neben ARMA und ARIMA existieren noch viele weitere Vertreter 
für die Vorhersage von Zeitreihen wie SARIMA oder SARIMAX) mit den optimalen 
Parametern zu finden. Ich persönlich finde pmdarima etwas zu schwergewichtig, 
denn die Parameter, die beim Training zu wählen sind, sind äußerst umfangreich.
Eine Alternative ist die Verwendung unserer Metrik für eine empirische Ermittlung 
der Parameter, die zum bestmöglichen Ergebnis führen – also durch das einfache 
Durchprobieren verschiedener Parameter für p, d und q. Da das Training eines 
ARIMA-Modells kaum mehr als ein paar Sekunden benötigt, können wir das auch 
getrost tun. Probieren Sie es doch gerne mal aus! Ich habe mit (p = 1, d = 1, q = 1) 
tatsächlich die besten Ergebnisse erzielt, aber vielleicht läuft es bei anderen 
Messdaten auf ein anderes Parameter-Set hinaus?



256 6 Intents entwickeln
Da ein Bild bekanntlich mehr als 1000 Worte sagt, visualisieren wir die Güte unseres Modells 
in den letzten drei Zeilen und zeichnen die Vorhersage in Rot gegenüber den tatsächlichen 
Temperaturen in Blau (siehe Bild 6.51). Leicht erkennt man, dass die Distanz der vorherge sagten und der tatsächlichen Werte nicht allzu groß ist, was darauf hindeutet, dass wir ein 
Modell geschaffen haben, das uns die Temperatur des nächsten Tages relativ genau vorhersagt.
Bild 6.51 Die Visualisierung der vorhergesagten (gestrichelte Linie) und tatsächlichen (durchgängige 
Linie) Temperaturdaten lässt Rückschlüsse auf die Güte unseres Modells zu.
Damit haben wir gezeigt, dass wir mit ARIMA relativ gute Vorhersagen für die Temperatur des 
nächsten Tages machen können. Versuchen Sie einmal model_fit.forecast() aus Zeile 46 
mit dem Parameter steps = 2 zu versehen. Damit werden zwei Werte vorhergesagt, in unserem 
Fall für den nächsten und den übernächsten Tag. Ein anderer, sicherlich interessanter, Ansatz 
ist, die vorhergesagten Temperaturen der Liste history hinzuzufügen (und nicht die Testdaten). 
Wie lange bleibt die Qualität unseres Modells akzeptabel? Wir sind nun am angenehmsten 
Punkt einer Datenanalyse angekommen, nämlich dort, wo unser Modell funktioniert und wir 
mit Daten und Parametern spielen können. Genießen Sie den Moment für eine Weile, bevor 
wir uns einen Deep-Learning-Ansatz für die Temperaturvorhersage anschauen.
6.6.3.4 LSTMs für Time Series Prediction
Die Architektur Long Short-Term Memory (LSTM) der Kategorie Recurrent Neural Networks
(RNN) haben wir bereits in Kapitel 3 angesprochen, als wir uns mit RNNs beschäftigt und ge lernt haben, dass sich diese prinzipiell sehr gut für Zeitreihenvorhersagen eignen. Der Grund 
dafür war (und ist immer noch), dass sie anders als ein simples Feed Forward Netzwerk, über 
das wir in den Transformers gesprochen haben und das Trainingsdaten nur in eine Richtung 
durch das Netzwerk reicht, die Fähigkeit besitzen, Informationen in einem internen Gedächt nis vorzuhalten und über mehrere Iterationen weiterzugeben. Wir haben jedoch ebenso ge lernt, dass RNNs die Schwäche besitzen, Informationen über längere Eingangssequenzen zu 
vergessen bzw. ihnen nicht mehr so viel Beachtung schenken, wie sie es eigentlich sollten. 
Deswegen setzen wir in diesem Experiment auf LSTMs, um einerseits zu prüfen, wie diese 
sich im Vergleich zu einem einfachen ARIMA-Ansatz schlagen und – und das ist mir viel 



6.6 Wetterabfrage 257
wichtiger – mal wieder praktisch zu arbeiten und ein eigenes Modell von Grund auf zu trai nieren. Um dem nachzukommen, öffnen Sie das Beispiel 100_extras/100_15_weather_forecast.
Das darin befindliche Skript main_get_data.py beschafft die Daten, die Sie ebenfalls in der 
weather_data.csv sehen. Wir haben das Skript ja bereits in Abschnitt 6.6.3.2 kennengelernt. 
Führen Sie es immer dann aus, um aktuelle Daten für das Training herunterzuladen, um eine 
reale Temperaturvorhersage für den kommenden Tag generieren zu lassen. Schauen wir nun 
in die main Punkt Pei, um zu sehen, wie wir die Wetterdaten laden und aufbereiten. Listing 6.65 zeigt 
den sequenziellen Aufruf von read_data(), standardize() und create_datasets(), die 
wir uns im folgenden Listing 6.66 anschauen. In den Zeilen 3 und 4 lassen wir uns noch kurz 
ausgeben, ob wir über eine CUDA-fähige GPU verfügen, die das Training beschleunigen kann.
Listing 6.65 Aufruf der Datenbeschaffung, Normalisierung und Formatierung
1. if __name__ == '__main__':
2.
3. logger.info("{} GPUs verfügbar.",
4. len(tf.config.list_physical_devices('GPU')))
5.
6. raw_data, temperature, dates, num_train_samples, num_val_samples =
7. read_data()
8. raw_data, temperature, mean, std = standardize(
9. raw_data,
10. temperature
11. )
12. train_dataset, val_dataset = create_datasets(
13. raw_data,
14. temperature,
15. num_train_samples,
16. num_val_samples
17. )
Wie schon früher einmal angemerkt, werden wir sehen, dass die Datenaufbereitung einen 
Großteil unserer eigentlichen Arbeit ausmacht und das Training nur einen Bruchteil. In die sem Beispiel setzen wir für den Deep-Learning-Anteil auf Keras, einer API, die auf Googles 
TensorFlow 2 aufbaut und das oft recht abstrakte und mühsam in Python gequetschte Frame work durch ein verständlicheres Interface einfacher zu verwenden macht.
In Zeile 15 beginnen wir damit, dass wir die zahlreichen Logausgaben, die wir von TensorFlow 
bekommen, auf ein Minimum reduzieren. Lediglich die Error-Meldungen sollen in der Konsole 
gezeigt werden, alles andere sparen wir uns. Es folgt die Referenz auf unsere Wetterdaten 
in Zeile 17. Ab Zeile 19 wird es dann spannend, denn hier legen wir die Parameter für die 
Datenaufteilung zwischen Test- und Trainingsdaten sowie die Parameter für das Training 
selber fest, welche wir uns etwas genauer anschauen wollen:
 SAMPLING_RATE: Beschreibt, wie viele Daten bei der Aufbereitung zu einem Sample zu sammengefasst werden. Da wir Messdaten für alle 10 Minuten haben, fassen wir jeweils 
6 davon zusammen, sodass wir stündliche Werte erhalten.
 SEQUENCE_LENGTH: Länge der auszugebenen Datensequenz bei der Vorbereitung der 
Trainingsdaten.



258 6 Intents entwickeln
 OUTLOOK: Zeitpunkt in der Zukunft, in der die Temperatur vorhergesagt werden soll. Der 
Ausblick wird nicht durch ein tatsächliches Datum repräsentiert, sondern durch einen 
Index.
 BATCH_SIZE: Die Anzahl an Samples, die in einem Batch zusammengefasst werden.
 EPOCHS: Anzahl der Epochen, die unser Modell trainiert wird.
Nun sind wir bereit, die Daten aus der Datei in WEATHER_DATA einzulesen. Die dafür 
vorgesehene Funktion read_data() hat fünf Rückgabewerte. Der erste beinhaltet die 
Trainingsdaten in der Liste raw_data, die alle Features beinhalten, die wir für das Training 
verwenden, darunter:
 temperature_air_mean_200: Gemessener Temperaturmittelwert in 200 Meter Höhe in 
Kelvin.
 temperature_air_mean_005: Gemessener Temperaturmittelwert in 5 Meter Höhe in Kelvin.
 humidity: Luftfeuchtigkeit in Prozent
 temperature_dew_point_mean_200: Gemittelter Taupunkt in 200 Meter Höhe in Kelvin.
Sie sehen, dass wir hier mehrere Features für die Vorhersage der Temperatur verwenden, 
nicht nur die Temperatur, wie wir es im ARIMA-Modell getan haben, sondern auch beispiels weise die Luftfeuchtigkeit oder den Taupunkt. Der Prozess des Feature Engineering beinhaltet 
nun, die verschiedenen Features auf ihren Einfluss auf die Zielvariable zu prüfen und zu 
schauen, ob diese unsere Vorhersage verbessern oder verfälschen. Wir wählen erst mal den 
naiven Weg und belassen alle Features in den Trainingsdaten.
In read_data() lesen wir nun die Wetterdaten über den csv.reader und verarbeiten diese 
zeilenweise, wobei wir die Header-Zeile überspringen. Die Temperatur in Spalte 2 rechnen 
wir direkt von Kelvin in Celsius um, indem wir 273.15 von jedem Wert subtrahieren. Diese 
Temperatur hängen wir dann als Gleitkommazahl an die Liste temperature an. Auch alle Werte 
in den Spalten 2, 3, 4 und 5 konvertieren wir in eine Gleitkommazahl und schreiben sie als 
Liste in die Liste raw_data. Das Datum in Spalte 0 wandert in die Liste dates. Nun berechnen 
wir abschließend noch die Anzahl der Datensätze, die wir für Training (num_training_samples) 
und die Validierung (num_val_samples) verwenden möchten (Zeilen 43–44) und geben dann 
alle unsere Listen und Integer-Werte zurück. Der Umfang der Trainingsdaten entspricht 70 % 
der Gesamtdatenmenge und der der Daten für die Validierung 20 %. Die Anzahl der Testdaten 
ergibt sich aus dem Rest, also aus den verbleibenden 10 %.
TIPP: Bei der Vorhersage von Zeitreihen ist es wichtig, dass Validierungs- und 
Testdaten neuer sind als die Trainingsdaten, um die Zukunft möglichst präzise auf 
Basis der Daten aus der Vergangenheit vorherzusagen (Chollet, 2021).
Listing 6.66 Beschaffung, Normalisierung und Formatierung der Wetterdaten
1. import os
2. import csv
3. from wetterdienst.provider.dwd.observation import DwdObservationRequest,
4. DwdObservationPeriod, DwdObservationResolution, DwdObservationDataset
5. from datetime import datetime, timedelta



6.6 Wetterabfrage 259
6. import pandas as pd
7. import numpy as np
8. from matplotlib import pyplot as plt
9. import tensorflow as tf
10. from tensorflow import keras
11. from tensorflow.keras import layers
12. from loguru import logger
13.
14. # Zeige nur Tensorflow Error-Meldungen
15. os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
16.
17. WEATHER_DATA = os.path.join("weather_data.csv")
18.
19. SAMPLING_RATE = 6
20. SEQUENCE_LENGTH = 120
21. OUTLOOK = SAMPLING_RATE * (SEQUENCE_LENGTH + 24*3 - 1) # 3 Tage in der Zukunft
22. BATCH_SIZE = 256
23. EPOCHS = 15
24.
25. def read_data():
26. temperature = []
27. raw_data = []
28. dates = []
29. with open(WEATHER_DATA, newline='') as csvfile:
30. spamreader = csv.reader(csvfile, delimiter=',', quotechar='\'')
31.
32. for i, row in enumerate(spamreader):
33. if i==0:
34. continue
35. row[2] = float(row[2]) - 273.15 # Kelvon nach Celsius
36. temperature.append(float(row[2]))
37. raw_data.append([float(x) for x in row[2:6]])
38. dates.append(row[0])
39.
40. logger.info("Rohdatenbeispiel:\n{}", raw_data[:5])
41. logger.info("Temperaturbeispiel:\n{}", temperature[:5])
42.
43. num_train_samples = int(0.7 * len(raw_data))
44. num_val_samples = int(0.2 * len(raw_data))
45.
46. logger.info("Anzahl Trainingsdaten: {}, Anzahl Validierungsdaten: {}, Daten
47. für die Vorhersage: {}",
48. num_train_samples,
49. num_val_samples,
50. num_train_samples - num_val_samples
51. )
52. return raw_data, temperature, dates, num_train_samples, num_val_samples
53.
54. def standardize(raw_data, temperature):
55. raw_data = np.array(raw_data[:])
56. mean = raw_data[:num_train_samples].mean(axis=0)
57. std = raw_data[:num_train_samples].std(axis=0)
58. logger.info("Mean {} und Standardabweichung {}.", mean, std)
59. raw_data = (raw_data - mean) / std



260 6 Intents entwickeln
60. temperature = [(x - mean[0]) / std[0] for x in temperature]
61. return raw_data, temperature, mean, std
62.
63. def create_datasets(raw_data, temperature, num_train_samples, num_val_samples):
64. train_dataset = keras.utils.timeseries_dataset_from_array(
65. raw_data[:-OUTLOOK],
66. targets=temperature[OUTLOOK:],
67. sampling_rate=SAMPLING_RATE,
68. sequence_length=SEQUENCE_LENGTH,
69. shuffle=True,
70. batch_size=BATCH_SIZE,
71. start_index=0,
72. end_index=num_train_samples)
73.
74. val_dataset = keras.utils.timeseries_dataset_from_array(
75. raw_data[:-OUTLOOK],
76. targets=temperature[OUTLOOK:],
77. sampling_rate=SAMPLING_RATE,
78. sequence_length=SEQUENCE_LENGTH,
79. shuffle=True,
80. batch_size=BATCH_SIZE,
81. start_index=num_train_samples,
82. end_index=num_train_samples + num_val_samples - 1)
83.
84. for samples, targets in train_dataset.take(1):
85. logger.info("Samples Shape: {} (num samples, sequence length, num
86. features)", samples.shape)
87. logger.info("Targets Shape: {} (normalized temperature)", targets.shape)
88.
89. return train_dataset, val_dataset
Es folgt die Normalisierung der Daten über die Funktion standardize(), die in Zeile 54 
sowohl raw_data als auch temperature entgegennimmt. Aber warum müssen wir unsere 
Daten überhaupt normalisieren? Nun, im Moment verwenden wir bekanntlich drei Features, 
die in Kelvin angegeben werden, und eines, das in Prozent angegeben wird, nämlich die 
Luftfeuchtigkeit. Damit ein neuronales Netz die Daten gleichermaßen gewichtet, müssen wir 
sie auf eine einheitliche Skala abbilden – und genau das ist der Zweck der Normalisierung. 
Diese geschieht über die Berechnung des Mittelwerts und der Standardabweichung. Der 
Mittelwert wird von jedem Feature jedes Samples subtrahiert und das Ergebnis durch die 
Standardabweichung subtrahiert (Zeile 59). Wir liefern neben den beiden normalisierten 
Listen auch den Mittelwert und die Standardabweichung zurück, um den Prozess später 
auch wieder umkehren zu können. Dazu aber gleich mehr.
Im letzten und umfangreichsten Schritt erstellen wir unsere Datensätze für Training und Vali dierung. Glücklicherweise gibt uns Keras die Funktion timeseries_dataset_from_array()
an die Hand, die uns die recht aufwendige Formatierung abnimmt. Wie aber gestaltet sich 
diese Formatierung? Nehmen wir an, wir hätten nur ein Feature, nämlich die im Abstand von 
10 Minuten gemessene Temperatur, das uns als Sequenz vorliegt. Diese könnte so aussehen:
[10.1, 11.1, 12.1, 13.1, 14.1, 15.1, 16.1, 17.1]



6.6 Wetterabfrage 261
Diese Sequenz wollen wir in viele kleine Sequenzen unterteilen, sodass wir immer einen 
ganz bestimmten Ausschnitt aus unseren Temperaturdaten sehen. Die sequence_length gibt 
dabei an, wie viele Samples in der neuen Sequenz stehen. Unter der Annahme, dass diese 
Sequenzlänge 4 beträgt, würden wir folgende Ausgabe erhalten:
[10.1, 11.1, 12.1, 13.1]
[11.1, 12.1, 13.1, 14.1]
[12.1, 13.1, 14.1, 15.1]
[13.1, 14.1, 15.1, 16.1]
[14.1, 15.1, 16.1, 17.1]
Unsere Features aus raw_data übergeben wir timeseries_dataset_from_array() als ersten 
Parameter data, gefolgt von targets, einer zweiten Liste, die die jeweiligen Zielgrößen für die 
jeweilige Sequenz aus data beinhaltet und entsprechend die gleiche Länge wie diese Liste 
haben sollte. Sehen Sie, dass wir in den Zeilen 65 und 66 bzw. in den Zeilen 75 und 76 die 
Liste auf einen bestimmten Index kürzen, nämlich um den konstanten Wert OUTLOOK, den 
wir zuvor berechnet haben? Das tun wir, damit jede von uns generierte Sequenz auf eine 
Zieltemperatur in der Zukunft zeigt! Vereinfacht gesagt, könnte man mit einem OUTLOOK
von 4 diese Formulierung anführen:
Wenn es heute 10.1, 11.1, 12.1 und 13.1 Grad waren (data), dann ist der nächste Wert 
14.1 Grad (target).
Der nächste Parameter ist sampling_rate mit dem Wert 6. Da wir bereits besprochen hat ten, dass wir unsere zehnminütigen Werte zu einer Stunde samplen, machen wir mit der 
sequence_length weiter. Dieser wird der Wert 120 zugewiesen, da wir 120 Stunden (= 5 Tage) 
zurückblicken wollen, um eine Vorhersage für die Temperatur drei Tage in der Zukunft zu 
machen. Auf Basis der beiden Werte in sequence_length und sampling_rate können wir nun 
relativ komfortabel den OUTLOOK berechnen. Sehen Sie in der Berechnung in Zeile 21 das 
Produkt aus 24*3? Dieses können Sie so verändern, dass Sie auch Modelle trainieren kön nen, die Temperaturvorhersagen für ganz andere Tage machen. Ändern Sie dazu einfach 
den maßgeblichen Faktor 3.
Shuffle gibt lediglich an, dass Sequenzen und Target durchgemischt werden, um keinen 
chronologischen Aspekt zwischen den jeweils einzelnen Sequenzen und deren Target in das 
Modell einfließen zu lassen. Jede Sequenz soll für sich selbst betrachtet werden und keinen 
Zusammenhang zu der vorherigen oder folgenden Sequenz herstellen.
Nach der batch_size, die angibt wie viele Sequenzen zu einem Batch zusammengefasst 
werden, sehen Sie noch den start_index und den end_index als Parameter im Funktions aufruf von timeseries_dataset_from_array(). Wie sich in Listing 6.66 gut sehen lässt, 
bieten uns diese Indizes die Möglichkeit, die Daten in raw_data zu unterteilen, ohne dass 
wir raw_data selbst per Doppelpunkt slicen, also unterteilen, müssten. Für das Training ver wenden wir die Samples von 0 bis num_train_samples-1 und für die Validierung die Daten 
von num_train_samples bis num_train_samples + num_val_samples - 1.
Blicken wir nun auf den Trainingsprozess, der, wie in Listing 6.67 gezeigt, nahtlos an die 
Aufrufe der Datenaufbereitung anknüpft und aus den Methoden train(), visualize_
training() und evaluate() besteht.



262 6 Intents entwickeln
Listing 6.67 Aufruf des Trainings und der Modellevaluation
1. model, loss, val_loss, epochs = train(
2. raw_data.shape[-1],
3. train_dataset,
4. val_dataset
5. )
6. visualize_training(
7. loss,
8. val_loss,
9. epochs
10. )
11. evaluate(
12. val_dataset,
13. model
14. )
Die darunterliegenden Funktionen sehen Sie in Listing 6.68. Lassen Sie uns mit train()
beginnen, in der in Zeile 2 ein Input-Tensor erzeugt wird, dem per Parameter shape eine 
Dimension von SEQUENCE_LENGTH (120) in der ersten Dimension und raw_data.shape[-1] in 
der zweiten Dimension übergeben wird. In unserem Fall ist dies 4, was der Anzahl unserer 
Features in raw_data entspricht. In Zeile 3 treffen wir auf ein neues Konzept. Hier könnten 
wir auch folgenden Code verwenden und unser Modell würde annehmbare Ergebnisse liefern:
x = keras.layers.LSTM(32)(inputs)
Ich möchte aber noch ganz kurz das Konzept bidirektionaler LSTMs vorstellen, da wir hier 
wiederum die Brücke zur Transformer-Architektur schlagen können. Die Idee dahinter ist, 
vereinfacht gesagt, zwei neuronale Netze (LSTMs) zu trainieren mit der Besonderheit, dass 
das zweite Netz auf der genau umgekehrten Eingabesequenz gefüttert wird. Das Konzept der 
Bidirectional Recurrent Neural Networks (BRNN) ist nicht neu, sondern wurde bereits 1997 
veröffentlicht (Schuster & Paliwal, 1997). Erinnern Sie sich, wo wir diese Bidirektionalität 
schon gesehen haben? Genau, in Abschnitt 6.2.1, in dem wir über BERT gesprochen haben. 
Auch hier führt die Kenntnis des Modells über den Kontext links und rechts eines Begriffs 
dazu, dass Vorhersagen über maskierte Begriffe eines Satzes besser getroffen werden können. 
Nach der LSTM-Schicht definieren wir auch schon den Dense-Layer, der die Dimension der 
vorherigen Schicht auf 1 reduziert. Wir erwarten ja auch nur eine Ausgabe des neuronalen 
Netzes, nämlich die Temperatur.
HINWEIS: Kann es auch vorkommen, dass man mehrere Ausgaben in der letzten 
Schicht benötigt? Ja, etwa im Falle eines Bildklassifikators, der zwischen einem 
Hund und einer Katze unterscheidet. Hier ist es in der Regel so, dass die letzte 
Schicht so viele Ausgaben hat, wie es Klassen gibt – in diesem Fall also zwei. 
Diese zwei Werte repräsentieren jeweils die Wahrscheinlichkeit, dass es sich 
entweder um einen Hund, oder um eine Katze handelt.



6.6 Wetterabfrage 263
In Zeile 5 initialisieren wir das eigentlich Modell mit dessen Ein- und Ausgaben und kompi lieren es anschließend über model.compile(). Diesem übergeben wir drei Parameter, die 
wir uns etwas genauer anschauen wollen:
 Optimizer: Der Optimizer ist dafür verantwortlich, die Gewichte des Modells abhängig von 
der Ausgabe der Loss-Funktion zu aktualisieren. Optimizer unterstützen bei der Minimie rung des Loss. Neben dem bekannten Gradient Decent und dessen diversen Abwandlungen 
wie Stochastic Gradient Decent oder Mini-Batch Gradient Decent existieren noch einige weite re Optimizer, wie Adam oder RMSProp. Wir wählen für einen ersten Entwurf den letzteren.
 Loss Function: Eine Funktion, die die Kosten der Abweichungen zwischen dem vorher gesagten und dem tatsächlichen Wert berechnet. Wir entscheiden uns für den mse (Mean 
Squared Error).
 Metrics: Keras bietet etwa 20 Metriken an, die während des Trainings gesammelt werden 
sollen. Da wir das Loss in Form des Mean Average Error später visualisieren wollen, teilen 
wir dem Modell mit, dass dieses mitgeschrieben und später ausgegeben werden soll.
Es folgt das eigentliche Training über model.fit() auf Basis des zuvor erstellten Datasets 
train_dataset. Zwischenergebnisse werden unter Zuhilfenahme von val_dataset geprüft. Die 
Ausgabe während des Trainings sehen Sie in Bild 6.52.
Bild 6.52 Die Textausgabe des Trainings zeigt das Loss, das Validation Loss und den Fehler der Anwen dung des Modells auf das jeweilige Dataset. Zusätzlich ist zu sehen, dass das Training jeder Epoche im 
Sekundenbereich liegt, und wir kaum warten müssen, bis unser Modell 15 Epochen lang trainiert wurde.
In Zeile 14 lassen wir uns das Modell, das Loss, das Validation Loss und die trainierten Epo chen per return zurückgeben.



264 6 Intents entwickeln
Listing 6.68 Trainings- und Evaluierungsfunktionen für die Temperaturvorhersage
1. def train(feature_count, train_dataset, val_dataset):
2. inputs = keras.Input(shape=(SEQUENCE_LENGTH, raw_data.shape[-1]))
3. x = layers.Bidirectional(layers.LSTM(32))(inputs)
4. outputs = layers.Dense(1)(x)
5. model = keras.Model(inputs, outputs)
6.
7. model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
8. history = model.fit(train_dataset, epochs=EPOCHS,
9. validation_data=val_dataset)
10.
11. loss = history.history["mae"]
12. val_loss = history.history["val_mae"]
13. epochs = range(1, len(loss) + 1)
14. return model, loss, val_loss, epochs
15.
16. def visualize_training(loss, val_loss, epochs):
17. plt.figure()
18. plt.plot(epochs, loss, "bo", label="Training MAE")
19. plt.plot(epochs, val_loss, "b", label="Validation MAE")
20. plt.title("Training and validation MAE")
21. plt.legend()
22. plt.show()
23.
24. def visualize_evaluation(plot_data, delta, title):
25. labels = ["History", "True Future", "Model Prediction"]
26. marker = [".-", "rx", "go"]
27. time_steps = list(range(-(plot_data[0].shape[0]), 0))
28. if delta:
29. future = delta
30. else:
31. future = 0
32.
33. plt.title(title)
34. for i, val in enumerate(plot_data):
35. if i: # i==1: ground_truth, i==2: prediction
36. plt.plot(future, plot_data[i][0,0], marker[i], markersize=10,
37. label=labels[i])
38. else: # i== 0: history
39. plt.plot(time_steps, [x[0] for x in plot_data[i]], marker[i],
40. label=labels[i])
41. plt.legend()
42. plt.xlim([time_steps[0], (future + 5) * 2])
43. plt.xlabel("Time-Step")
44. plt.show()
45.
46. def evaluate(val_dataset, model):
47. for x, y in val_dataset.take(1):
48. history = x[0][:, 0].numpy()
49. ground_truth = y[0].numpy()
50. prediction = model.predict(x)[0]
51.
52. history = np.array([xi * std + mean for xi in history])
53. ground_truth = np.array([ground_truth * std + mean])



6.6 Wetterabfrage 265
54. prediction = np.array([prediction * std + mean])
55.
56. visualize_evaluation([history, ground_truth, prediction], 3*24,
57. "3-Tages-Temperaturprognose")
Die Funktion visualize_training() stellt lediglich Loss und Validation Loss grafisch dar und 
hilft uns dabei, die Qualität unseres Modells einzuschätzen (siehe Bild 6.53). Das Loss (oder 
Training Loss) ist eine Metrik, die Aussagen darüber treffen lässt, wie gut das Modell auf dem 
Trainingsdatensatz performt. Das Validation Loss hingegen lässt Rückschlüsse darauf zu, wie 
gut die Ergebnisse sind, wenn das Modell auf den Validierungsdatensatz angewandt wird.
Bild 6.53 Der Fehler (hier Mean Average Error) sinkt mit jeder Epoche bei Anwendung auf die 
Trainingsdaten sichtbar.
Die Funktion visualize_evaluation() ist etwas komplexer. Hier zeichnen wir drei Dia gramme:
 Eines, das die Eingabesequenz abbildet, die der Vorhersage zugrunde liegt.
 Eines, bestehend aus nur einem Kreis, das die vorhergesagte Temperatur darstellt.
 Eines, bestehend aus einem x, das die tatsächliche Temperatur, die Ground Truth, markiert.
Der Aufruf von visualize_evaluation() findet innerhalb der Funktion evaluate() statt, 
die einen zufälligen Ausschnitt aus val_dataset holt (take(1)), daraus die Eingabesequenz 
unter dem Namen history sowie die zu erwartende Temperatur in ground_truth liest und die 
Vorhersage über model.predict() in Zeile 50 trifft.
Rufen wir uns erneut ins Gedächtnis, dass wir die Werte, mit denen wir gearbeitet haben, 
normalisiert haben. Um nun wieder reale Werte im Diagramm zu sehen, rechnen wir die 
Werte in den Zeilen 52–54 in ihren Ursprungszustand zurück. Ganz zum Schluss rufen 
wir visualize_evaluation() mit den eben erzeugten Werten auf. Wir können über den 
Parameter delta spezifizieren, wir weit die vorhergesagte Temperatur in der Zukunft liegt. 



266 6 Intents entwickeln
Da wir unser Modell auf eine Vorhersage drei Tage in der Zukunft trainiert haben, soll unsere 
Visualisierung das hier auch widerspiegeln (siehe Bild 6.54).
Bild 6.54 Die Vorhersage der Temperatur auf Basis der gezeichneten Temperaturhistorie ist besser, 
je näher der Kreis und das Kreuz beieinanderliegen. Unsere Vorhersage mit einer Abweichung von 
ca. 2 °C ist somit recht präzise.
Zum Schluss möchte ich Ihnen noch zeigen, wie wir das eben trainierte Modell anwenden 
können, um tatsächliche Vorhersagen zu machen. Dazu implementieren wir zunächst die 
Methode predict() gemäß Listing 6.69. Der Input, den diese per prediction_sequence über geben bekommt, muss eine genaue Länge von 120 Werten (5 Tage Temperaturdaten) auf weisen. Nach dem Aufruf von model.predict() rechnen wir das Ergebnis wieder in einen 
menschenlesbaren Temperaturwert um, und geben aus dem erhaltenen NumPy-Array den 
gesuchten Wert zurück. Lassen Sie sich von den verschiedenen Indizes in den Zeilen 4 und 
5 nicht durcheinanderbringen, das Ergebnis der Vorhersage ist leicht verschachtelt. Ein 
print()-Befehl auf das prediction-Objekt hilft, um dessen Struktur besser zu verstehen.
Listing 6.69 Vorhersage von Temperaturen über predict
1. def predict(prediction_sequence):
2. prediction = model.predict(prediction_sequence)
3. prediction = np.array([prediction * std + mean])
4. prediction = prediction[0]
5. return prediction[0,0]
Anwendung findet prediction() nun in Listing 6.70. Hängen Sie dieses Listing gedank lich und optional im Editor an die vorherigen Funktionsaufrufe aus Listing 6.67 an. Es 
schließt unseren Code auch damit final ab. Hier konstruieren wir in den Zeilen 1–10 die 
benötigte Sequenz aus 120 Messpunkten, die wir für eine Vorhersage benötigen. Zeile 10 
fügt eine Dimension zu unseren Daten hinzu, da predict() diese Formatierung erwartet. 



6.6 Wetterabfrage 267
Aus den Daten in der Form [[…][…]] wird lediglich ein [[[…][…]]]11. Geben Sie einen kürzeren 
oder längeren Datensatz ein, wird das Modell sich entsprechend beschweren. Wir lassen 
dann die Vorhersage durchführen, berechnen das Datum in drei Tagen in den Zeilen 14 und 
15 und geben unsere Vorhersage aus.
Listing 6.70 Durchführen einer Wettervorhersage 3 Tage in der Zukunft anhand einer gegebenen 
Temperatursequenz der letzten 5 Tage
1. # Genaue Stunde, an der die Temperatur in 3 Tagen vorhergesagt werden soll
2. prediction_hour = 12
3.
4. # Generieren der Sequenz der letzten 5 Tage (120 Stunden),
5. # um die nächsten 72 Stunden vorherzusagen
6. prediction_sequence = np.asarray(raw_data[-120-prediction_hour*6:-
7. prediction_hour*6])
8.
9. # Füge eine Dimension hinzu
10. prediction_sequence = np.expand_dims(prediction_sequence, axis=0)
11. weather_in_24h_day = predict(prediction_sequence)
12.
13. # Hole das letzte Datum des Datensatzes und füge 3 Tage hinzu.
14. tomorrow = datetime.fromisoformat(dates[-1 - prediction_hour*6]) +
15. timedelta(days=3)
16. logger.info("Temperatur am {} ist {}.", tomorrow, weather_in_24h_day)
Sollten Sie aktuelle Daten gelesen haben, schreiben Sie sich doch einen kleinen Zettel mit 
Datum und Temperatur und überprüfen Sie Ihre Vorhersage in drei Tagen! Sind wir besser 
oder schlechter als die Tagesschau oder die gängigen Wetterseiten im Internet?
TIPP: Im Kontext meiner Arbeit werden die Begriffe Simulation und Analytics häu fig zusammen verwendet. Die Wettervorhersage ist ein wunderbares Beispiel, um 
diese thematisch voneinander abzugrenzen. Das Feld der Simulation berechnet 
Wolkenbewegungen, Hochs und Tiefs, Luft- und Meeresströme und simuliert de ren Verlauf per Computer, um eine Wetterlage in den kommenden Stunden und 
Tagen zu errechnen und daraus Temperatur und Niederschlag abzuleiten. Wir 
hingegen setzen auf statistische Modelle, die den simulativen Aspekt ignorieren, 
und berechnen die Werte für Temperatur und Niederschlag direkt aus den uns vor liegenden Zahlendaten.
Nun haben wir reichlich Möglichkeiten an der Hand, mit denen wir die Temperatur eines 
Tages in der Zukunft vorhersagen können. Für mich hat sich das Vorgehen etabliert, das 
wir in diesem Abschnitt eingesetzt haben, nämlich mit der simpelsten zu beginnen und erst 
dann die komplexeren Verfahren einzusetzen, wenn die Ergebnisse des aktuellen Ansatzes 
nicht gut genug sind. So, nun schließen wir das Thema Zeitreihenvorhersagen erst einmal 
ab und begnügen uns erneut damit, unseren Sprachassistenten um eine etwas einfachere 
Funktion zu erweitern.
11 Das sind so Momente, in denen ich mir ein wenig mehr Objektorientierung wünsche, die eine transparente 
Struktur fordert. Allerdings würde ich einen solchen Wunsch nie in einer Gruppe Python-Entwickler äußern.



268 6 Intents entwickeln
■ 6.7 Steuerung von Smart-Home-Geräten
Smart-Home-Geräte wie etwa Steckdose, Rollläden oder Gartenbewässerung zu steuern oder 
deren Status und Verbrauch abzufragen, gehört schon lange zu den Fähigkeiten, die die gän gigen Geräte mitbringen und per API von beliebigen Systemen angesprochen werden können. 
Da die Geräte letzten Endes leider gar nicht so schlau sind, ist es sinnvoll und naheliegend, 
sie an unseren durchaus als intelligent zu bezeichnenden Sprachassistenten anzubinden.
HINWEIS: Ich verwende in diesem Abschnitt die Geräte von Shelly, da diese recht 
günstig sind und sich ohne Notwendigkeit einer Cloud-Anbindung einfach ins lo kale WLAN einbetten lassen. Solange das Produkt Ihrer Wahl jedoch über eine API 
verfügt, die über einen HTTP-Request angesprochen werden kann, können Sie die 
Logik hinter unserem Intent leicht abändern.
Den Code zu diesem Kapitel finden Sie in 10_g_smart_home. Wir beginnen mit der Erzeugung 
der Intent-Definition smarthome_dataset.yaml im Ordner intents\snips-nlu. Wie in Listing 6.71
zu sehen ist, beschränken wir uns auf das An- und Ausschalten von Geräten.
Listing 6.71 Der Intent zum Steuern von Lichtschaltern kennt zwei Entitäten, den Status state_ent 
und das Gerät device_ent.
1. type: entity
2. name: state_ent
3. automatically_extensible: true
4. use_synonyms: false
5. matching_strictness: 0.8
6. values:
7. - an
8. - ein
9. - aus
10.
11. type: entity
12. name: device_ent
13. automatically_extensible: true
14. use_synonyms: false
15. matching_strictness: 0.8
16. values:
17. - Küche
18. - Licht in Küche
19. - Licht im Wohnzimmer
20. - Licht
21. - Büro
22. - Licht im Büro
23.
24. ---
25. type: intent
26. name: smarthome
27. utterances:



6.7 Steuerung von Smart-Home-Geräten 269
28. - Schalte [device:device_ent](Licht) [state:state_ent](aus)
29. - Schalte [device:device_ent](Licht in der Küche) [state:state_ent](an)
30. - Schalte [device:device_ent](Büro) [state:state_ent](ein)
31. - Mache [device:device_ent](Licht im Büro) [state:state_ent](aus)
32. - Mache [device:device_ent](Licht im Wohnzimmer) [state:state_ent](an)
33. - Mache [device:device_ent](Büro) [state:state_ent](an)
Wir definierten zu Beginn die Entität state_ent, die die Zustände an, ein und aus annehmen 
kann. Im Anschluss folgt die Entität device_ent, die ein Gerät repräsentiert, welches in unse rem Fall ein ganzer Raum oder nur eine Lampe in einem Raum sein kann. Der Intent selber 
erlaubt dann verschiedene Schreibweisen, wobei ich hier die simpelsten gewählt habe, die 
da lauten Schalte Licht aus oder Mache Licht im Wohnzimmer an. Die Schlüsselwörter werden 
wie gewohnt in Snips-NLU über die Entitäten repräsentiert.
Es folgt die Anlage der Konfigurationsdatei aus Listing 6.72. Darin formulieren wir Templates, 
um per Sprache zu kommunizieren, dass ein Zustand oder ein Gerät unbekannt ist oder nicht 
erreicht werden kann, sowie die verschiedenen Zustände, die wir akzeptieren. Unter intent.
smarthome.devices lege ich nun einige Namen von Räumen fest, die ich mit einer IP-Adresse 
versehe. Diese IP repräsentiert ein jeweiliges Smart-Home-Gerät im WLAN, in dem sich auch 
unser Sprachassistent befindet oder die vom Sprachassistenten aus erreicht werden kann. 
Meine Bürolampe hat die IP 192.168.2.160.
Listing 6.72 Konfiguration des Smart Home Intents
intent:
 smarthome:
 de:
 state_unknown: "Der Zustand {} ist für den Schalter ungültig."
 device_unknown: "Das Gerät {} kenne ich nicht."
 request_failed: "Anfrage an {} fehlgeschlagen."
 state_on: ["ein", "an"]
 state_off: ["aus"]
 en:
 state_unknown: "The state {} is invalid."
 device_unknown: "The device {} is unknown."
 request_failed: "Request to {} failed."
 state_on: ["on"]
 state_off: ["off"]
 devices:
 Büro: "192.168.2.160"
 office: "192.168.2.160"
Listing 6.73 zeigt den Python-Code von intent_smarthome.py, der im Ordner intents\functi ons\smarthome abzulegen ist. Wir lesen wie gewohnt die Konfiguration in den Zeilen 12–21 
aus und daraus wiederum alle Geräte, die wir darin registriert haben. In unserem Fall ist 
das lediglich Büro, das wir samt der dazugehörenden IP im Dictionary devices speichern 
(Zeilen 26–27).



270 6 Intents entwickeln
Listing 6.73 Die Logik des Smart Home Intents
1. from loguru import logger
2. from chatbot import register_call
3. import global_variables
4. import random
5. import os
6. import yaml
7. import requests
8.
9. @register_call("smarthome")
10. def smarthome(session_id = "general", device="", state=""):
11.
12. config_path =
13. os.path.join('intents','functions','smarthome','config_smarthome.yml')
14. cfg = None
15.
16. with open(config_path, "r", encoding='utf8') as ymlfile:
17. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
18.
19. if not cfg:
20. logger.error("Konnte Konfigurationsdatei für 'smarthome' nicht lesen.")
21. return ""
22.
23. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
24. # Lese alle bekannten Smart Home Devices aus
25. devices = {}
26. for key, value in cfg['intent']['smarthome']['devices'].items():
27. devices[key] = value
28.
29. # Gibt es das angefragte Device?
30. if devices[device]:
31. s = None
32. # Interpretiere den Zustand
33. if state in cfg['intent']['smarthome'][LANGUAGE]['state_on']:
34. s = "on"
35. elif state in cfg['intent']['smarthome'][LANGUAGE]['state_off']:
36. s = "off"
37. else:
38. return
39. cfg['intent']['smarthome'][LANGUAGE]['state_unknown'].format(state)
40.
41. # Setze einen Get-Request ab, der das Gerät ein- oder ausschaltet
42. PARAMS = {'turn': s}
43.
44. try:
45. r = requests.get(url = "Hah Tee Tee Peh  Doppelpunkt Doppel-Schrägstrich" + devices[device] +
46. "/relay/0", params = PARAMS)
47. except:
48. return
49. cfg['intent']['smarthome'][LANGUAGE]['request_failed'].format(device)
50.
51. data = r.json()
52. logger.debug(data)
53. return ""
54. return cfg['intent']['smarthome'][LANGUAGE]['device_unknown'].format(device)



6.8 Q20-Ratespiel 271
Existiert das Gerät, das wir ansprechen wollen (Zeile 30), dann prüfen wir weiter anhand 
des Befehls, in welchen Zustand wir es versetzen wollen (Zeilen 33–36). Der String s dient 
gleich dazu, den HTTP-Request zu konstruieren, den wir an das Gerät schicken. Das tun wir 
auch sogleich, indem wir zuerst ein weiteres Dictionary PARAMS definieren, das den Ein trag mit dem Schlüssel turn beinhaltet und als Wert den gewünschten Zustand. Es folgt die 
Konstruktion des Requests in den Zeilen 45–46, den wir aus der IP-Adresse und dem String 
/relay/0 zusammensetzen. Letzterer ist spezifisch für den Hersteller der Shelly-Geräte und 
muss, sollten Sie einen anderen Hersteller verwenden, angepasst werden.
Die vom HTTP-Request zurückgelieferte Antwort im JSON-Format geben wir per Log aus 
(siehe Bild 6.55), sodass Sie zur Entwicklungszeit sehen können, ob das Ab- oder Anschalten 
erfolgreich war. Auf ein auditives Feedback können wir in diesem Fall verzichten, da:
a) in den meisten Fällen sichtbar ist, ob das Gerät geschaltet wurde und
b) eine Rückmeldung nur im Fehlerfall notwendig ist.
Bild 6.55 Das erfolgreiche Ausschalten des Geräts wird durch eine Nachricht im JSON-Format quittiert, 
in der der Schlüssel ison auf False gesetzt ist.
Wie immer gilt jedoch: Wenn Sie die Rückmeldung möchten, bauen Sie sie doch einfach ein, 
indem Sie in Listing 6.73 in Zeile 53 eine Rückmeldung ausgeben. Das ist ja das Schöne an 
einer eigenen Implementierung. Wir machen die Welt, wie sie uns gefällt.
■ 6.8 Q20-Ratespiel
In diesem Kapitel schlagen wir zwei Fliegen mit einer Klappe. Erstens kümmern wir uns um 
die längst überfällige Funktion unseres Sprachassistenten, Dialoge zu führen, die aus mehr 
als nur einer Frage und einer Antwort bestehen. Ein Beispiel wäre:
 Benutzer: Was soll ich morgen kochen?
 Assistent: Was hast du denn im Kühlschrank?
 Benutzer: Eier und Paprika
 Assistent: Dann schlage ich ein Omelette vor.
Das wollen wir erreichen, indem wir einen Kontext vorhalten, über den wir prüfen, ob der zeit ein Dialog im Gange ist und fortgesetzt werden sollte, oder ob wir einen neuen Dialog 
beginnen. Nun wäre so ein interaktives Kochbuch wie im gezeigten Beispiel sicher auch 



272 6 Intents entwickeln
interessant, jedoch habe ich ein Beispiel herausgesucht, das für mich den Begriff der künst lichen Intelligenz prägte und auf so manchem Schulhof die Frage aufwarf: Wie zum Teufel 
kann dieser kleine Ball erraten, an was ich gerade denke? Die Überschrift des Abschnitts sagt 
ja schon, dass es um das Spiel Q20 geht. Ein kleines, elektronisches Spielzeug in Form einer 
Kugel, eines Displays und einiger Knöpfe, das in der Lage ist, Personen und Gegenstände 
zu erraten, indem es dem Spieler Fragen dazu stellt, etwa Ist es kleiner als eine Ente? Der 
Name suggeriert bereits, dass der gesuchte Gegenstand oder die gesuchte Person innerhalb 
von 20 Fragen gefunden wird. Über unseren Kontext wollen wir uns nun merken, dass der 
Benutzer ein Spiel begonnen hat. Für das Speichern des Spiel-Zustands wird jeder Intent in 
Zukunft selber verantwortlich sein. In unserem Fall hält der Intent vor, wie die vorherigen 
Antworten des Spielers ausgefallen sind.
Den Code des Spiels finden Sie in 010_h_question_game12. Den Anfang soll der globale 
Kontext machen, den wir in die global_variables.py im Hauptverzeichnis unseres Assisten ten aufnehmen. Dort ist bisher lediglich die Zeile voice_assistant = None zu finden. Wir 
fügen darunter context = None hinzu, um aus allen Modulen auf context zuzugreifen, die 
global_variables einbinden. Die Datei können wir schließen und öffnen stattdessen die main Punkt Pei.
Da wir nicht jeden Satz eines Dialogs mit dem Aktivierungswort beginnen möchten, müssen 
wir nun auch auf Benutzereingaben lauschen, wenn das Aktivierungswort nicht fällt. Die 
Umsetzung erfolgt prompt. In der Methode run() im Else-Zweig der If-Abfrage if self.is_
listening: fügen wir Listing 6.74 hinzu.
Listing 6.74 Wenn ein aktueller Kontext existiert und der Sprachassistent nicht spricht, lausche auf 
die Eingabe des Benutzers.
# Reaktiviere is_listening, wenn der Skill weitere Eingaben erfordert
if not global_variables.context is None:
 # ... aber erst, wenn ausgeredet wurde
 if not global_variables.voice_assistant.tts.is_busy():
 global_variables.voice_assistant.is_listening = True
else:
Den Rest des Else-Zweigs rücken wir entsprechend ein, sodass der Code unter dem Else 
ausgeführt wird, wenn kein Kontext vorhanden ist.
Die nächste Änderung an unserem Framework erfolgt in der intentmgmt.py in der Methode 
process(). Zugunsten der Lesbarkeit ist in Listing 6.75 die ganze Methode zu sehen, die 
Neuerungen finden sich jedoch nur in den Zeilen 14–21. Wir speichern den aktuellen Kon text in old_context und lassen uns wie gewohnt eine Antwort über self.chat.respond()
generieren. Den Kontext müssen wir zwischenspeichern, da er sonst von respond() neu 
oder zurückgesetzt werden würde! Nun prüfen wir, ob old_context nicht None ist. Ist das der 
Fall, rufen wir diesen auf und übergeben ihm den aktuell vom Benutzer gesprochenen text. 
Moment mal, wir rufen ihn auf? Nun offenbart sich, was der Kontext eigentlich ist, nämlich 
ein Zeiger auf eine Funktion! Ganz konkret auf die des Intents, mit dem noch eine Weile 
kommuniziert werden soll. Den Rückgabewert von context(text) liefern wir zurück. Wenn 
old_context None ist, liefern wir wie gewohnt unsere generierte response.
12 Der ursprüngliche Code des Spiels stammt von Fergus Griggs und wurde auf Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichgithub.com/FergusGriggs/20q
unter der MIT-Lizenz veröffentlicht.



6.8 Q20-Ratespiel 273
Listing 6.75 Die Methode process() kann auf einen existierenden Kontext zurückgreifen, sofern 
einer gespeichert wurde.
1. def process(self, text, speaker):
2.
3. # Evaluiere ChatbotAI, wenn keines der strikten Intents greift, wird die
4. # Eingabe über die dialogs.template
5. # automatisch an snips nlu umgeleitet.
6. intent_name = self.chat.get_intend_name(text)
7. if intent_name == "default_snips_nlu_handler":
8. intent_name = get_snips_nlu_intent(text)
9.
10. # Überprüfe, ob der Benutzer diesen Intent ausführen darf
11. if global_variables.voice_assistant.user_management.
12. authenticate_intent(speaker, intent_name):
13.
14. old_context = global_variables.context
15.
16. response = self.chat.respond(text)
17.
18. if not old_context is None:
19. return global_variables.context(text)
20. else:
21. return response
22. else:
23. # In diesem Beispiel lassen wir den Assistenten antworten. In Zukunft
24. # wird er einfach nicht reagieren, um das Abspielen gar nicht erst zu
25. # unterbrechen
26. response = speaker + " darf den Befehl " + intent_name +
27. " nicht ausführen."
28.
29. return response
Damit haben wir die notwendigen Änderungen an unserem Framework vorgenommen. Zeit, 
ein Template zu erstellen. Das geschieht in der Datei intents\chatbotai\questiongame.template, 
die sogleich den Inhalt aus Listing 6.76 zugewiesen bekommt.
Listing 6.76 Template des Fragespiels
{% block %}
 {% client %}((starte fragespiel)|(starte frage spiel)){% endclient %}
 {% response %}{% call startQuestionGame: 0 %}{% endresponse %}
{% endblock %}
Der Intent springt ein, wenn starte fragespiel gesagt wird, und ruft die Funktion 
startQuestionGame() mit einem Dummy-Parameter auf. Bevor wir diese Methode nun 
runterschreiben, wollen wir uns zunächst die Datenbasis anschauen, die unserem Assistenten 
helfen soll, die richtigen Fragen zu den richtigen Gegenständen zu stellen. Dazu kopieren 
wir aus dem Beispielprojekt die Dateien items_de.txt und questions_de.txt und legen sie in 
den neuen Ordner intents\functions\questiongame. Schauen wir uns exemplarisch einen 
Gegenstand aus der items_de.txt an.



274 6 Intents entwickeln
Listing 6.77 Der Begriff Apfel mit den entsprechenden Gewichten
apfel:4:
 [-1.0, 0.9587, 0.7929, 1.0, -0.8308, 0.2918,
 -0.9714, -0.996, -0.9762, -1.0, -0.449, -1.0,
 -1.0, 0.9721, 0.5685, 0.4802, -0.9736, -0.9606,
 -0.9773, -1.0, 0.9846, -0.8482, 0.7015, -0.5798,
 -0.9927, 0.6231]
Dem Apfel wird eine Liste von Gewichten zugeordnet, die in der Reihenfolge mit den Fragen 
übereinstimmen müssen, die wir in Listing 6.78 sehen. Stimmt nun beispielsweise die Ant wort, zum Beispiel Nein, auf die erste Frage Ist es lebendig? mit der Gewichtung überein, dann erhält 
der Begriff eine entsprechend hohe Bewertung für diese Frage. Die Summe aller Wertungen 
ergeben dann die finale Auswahl des am wahrscheinlichsten zutreffenden Gegenstands.
Listing 6.78 Auszug der Fragen, die dem Spieler gestellt werden
Ist es lebendig?
Ist es kleiner als eine Ente?
Ist es bunt?
...
Hilft es dir?
Trägt man es?
Benutzt es Wasser?
Es folgt nun noch die Anlage der Konfiguration für diesen Intent. Dessen Inhalt (Listing 6.79) 
besteht lediglich aus den Templates zum Geben der finalen Antwort und der Bitte, ein neues 
Spiel zu starten. Wozu wir erstere benötigen, wird gleich klarer.
Listing 6.79 Konfigurationsdatei config_questiongame.yml für das Fragespiel
intent:
 questiongame:
 de:
 please_start_new_game: "Bitte starte erst ein neues Spiel mit 'Starte
 Fragespiel'."
 i_guess: "Ich bin mir zu {} Prozent sicher, dass du an den Begriff {} denkst."
 en:
 please_start_new_game: "Please start a new game first by saying 'start question
 game'."
 i_guess: "I'm sure to {} percent, that you think about a {}."
Nun kommen wir endlich dazu, die intent_questiongame.py zu schreiben (siehe Listing 6.80). 
Wir beginnen mit einigen Konstanten in den Zeilen 9 bis 12, die die vier Antwortmöglich keiten beinhalten können, die ein Spieler auf eine der oben aufgeführten Fragen geben 
kann. Die Antworten Ja und nein werden besonders stark, vielleicht und eher nicht werden 
abgeschwächt gewichtet.
In der Variablen question_game_session in Zeile 14 speichern wir das aktuelle Spiel, dessen 
Verlauf ja über die gesamte Existenz des Kontexts nachgehalten werden muss. Dazu erstellen 



6.8 Q20-Ratespiel 275
wir gleich in Listing 6.82 eine eigene Klasse. Ab Zeile 16 implementieren wir den eigent lichen Intent, der ein neues Spiel startet. Dazu holen wir uns darin Zugriff auf die globale 
Variable question_game_session und initialisieren diese in Zeile 20. In Zeile 21 erfolgt dann 
die Magie, die es uns möglich macht, über global_variables.context einen Zeiger auf eine 
Funktion zu speichern, in unserem Fall auf questionGameAnswer(). Solange context auf diese 
Funktion gesetzt ist, geht unser Sprachassistent davon aus, dass wir uns noch in einem Spiel 
befinden. Der Rückgabewert unseres Intents ruft nun aus der Instanz der Klasse Q20Session
askQuestion() auf und stellt damit die erste Frage.
Die nächste Funktion questionGameAnswer() prozessiert nun eine erhaltene Antwort. Zu erst wird in Zeile 28 geprüft, ob bereits ein Spiel gestartet wurde. Ist das der Fall, wird die 
Methode evaluateAnswer() aufgerufen. Diese liefert, wie wir gleich sehen, ein Update auf 
die Wahrscheinlichkeiten aller Gegenstände zurück, die das Spiel über items_de.txt kennt. 
Dieses Update lassen wir in den Zeilen 30–32 einfließen und holen uns die nächste zu 
stellende Frage. Liefert askQuestion() in Zeile 34 noch eine Frage zurück, stellen wir sie 
sogleich, indem wir sie in Zeile 36 zurückgeben.
Listing 6.80 Intents des Fragespiels
1. from loguru import logger
2. from chatbot import register_call
3. import random
4. import sys
5. import os
6. import global_variables
7. import yaml
8.
9. YES = ["JA", "J", "YES", "Y"]
10. NO = ["NEIN", "N", "NO"]
11. PROBABLY = ["VIELLEICHT", "PROBABLY"]
12. PROBABLY_NOT = ["WAHRSCHEINLICH NICHT", "EHER NICHT", "PROBABLY NOT"]
13.
14. question_game_session = None
15.
16. @register_call("startQuestionGame")
17. def startQuestionGame(session_id = "general", dummy=0):
18. logger.info("Starte neues Fragespiel.")
19. global question_game_session
20. question_game_session = Q20Session()
21. global_variables.context = questionGameAnswer
22. return question_game_session.askQuestion()
23.
24. def questionGameAnswer(answer=""):
25. answer = answer.strip()
26. logger.debug("Antwort '" + str(answer) + "' erhalten.")
27. global question_game_session
28. if not question_game_session is None:
29. answer_value = question_game_session.evaluateAnswer(answer)
30. for i in range(len(question_game_session.items)):
31. question_game_session.items[i].updateCertainty(answer_value,
32. question_game_session.current_question,
33. len(question_game_session.questions))



276 6 Intents entwickeln
34. question = question_game_session.askQuestion()
35. if question:
36. logger.info("Die nächste Frage ist {}.", question)
37. return question
38. else:
39. logger.info("Das war die letzte Frage.")
40. final_answer = question_game_session.getAnswer()
41. logger.info("Ermittelte Antwort für Fragespiel ist: {} ",
42. str(final_answer))
43. question_game_session.clearSession()
44. global_variables.context = None
45. question_game_session = None
46. return final_answer
47. else:
48. return question_game_session.PLEASE_START_NEW_GAME
Wird keine Frage mehr geliefert, berechnen wir in Zeile 40 über getAnswer() die Antwort, 
bereinigen die Session und löschen den Kontext. Keine Sorge, die Funktionen dazu lernen 
wir gleich noch kennen. Dann liefern wir die Antwort in Zeile 45 zurück.
Listing 6.81 ist recht einfach zu verstehen, bildet es doch lediglich die Klassen ab, die unsere 
Fragen und Gegenstände speichern, die es zu erraten gilt. Item verfügt lediglich noch über 
die Methode updateCertainty(), die den Wert der Wahrscheinlichkeit, dass der Spieler an 
diesen Gegenstand denkt, neu berechnet.
Listing 6.81 Klassen zum Einlesen der Fragen und Gegenstände
1. class Question:
2. def __init__(self, string, id):
3. self.string=string
4. self.index=id
5.
6. class Item:
7. def __init__(self, name, guessNum, questionFloats, id):
8. self.name=name
9. self.guessNum=guessNum
10. self.questionFloats=questionFloats
11. self.index=id
12. self.certainty=0
13.
14. def updateCertainty(self, val, current_question, num_questions):
15. self.certainty+=(1-abs(val-self.questionFloats[
16. current_question
17. ]))/num_questions
Die genaue Berechnung der Wahrscheinlichkeit des Begriffs Apfel, wollen wir uns in 
Tabelle 6.8 für die ersten fünf beispielhaft beantworteten Fragen anschauen.
Es ist zu sehen, dass die Wahrscheinlichkeit von Frage zu Frage ansteigt, wenn wir tatsäch lich an einen Apfel denken und die Fragen wahrheitsgemäß beantworten. Man kann hier 
also kaum von Magie oder einer Blackbox reden, sondern von sorgfältig gewählten, jedem 
Begriff zugeordneten Gewichten, die auf das Endergebnis Einfluss nehmen.



6.8 Q20-Ratespiel 277
Tabelle 6.8 Exemplarische Berechnung der Wahrscheinlichkeit eines Begriffs anhand der ersten 
fünf Fragen
Frage i Antwort (Wert) Gewicht in 
questionFloat[i]
Update-Wert Wahrschein lichkeit
Ist es lebendig? Nein (–1) –1,0 0,05 0,05
Kleiner als eine Ente? Ja (1)  0,9587 0,047935 0,097935
Ist es bunt? Ja (1)  0,7929 0,0475 0,145435
Kann man es essen? Ja (1)  1,0 0,05 0,195435
Wiegt es mehr als ein 
Stück Seife?
Wahrscheinlich 
nicht (–0,2)
–0,8308 0,01846 0,213895
… … … … …
Listing 6.82 beschreibt nun die Klasse Q20Session. Zu Beginn der Funktion __init__() legen 
wir zwei Listen an, questions und items, die alle bekannten Fragen und Antworten in struk turierter Form beinhalten, und erstellen weiterhin einen Zähler, nämlich current_question, 
um speichern zu können, bei der wievielten Frage wir uns im Verlaufe unseres Spiels be finden. In der Initialisierung lesen wir weiterhin die Konfiguration ein, laden die Konstanten 
for PLEASE_START_NEW_GAME und GUESS und prüfen, wo unsere items_de.txt und die 
questions_de.txt liegen. Letztere lesen wir sogleich ein, die Gegenstände in den Zeilen 29–37 
und die Fragen in den Zeilen 39–44. Hier offenbart sich der Vorteil, mit JSON- oder YAML Dateien zu arbeiten, denn das zeilenweise Einlesen mehrerer Variablen der Gegenstände 
bringt es mit sich, dass die Werte in den Zeilen mühevoll getrennt und geparst werden 
müssen. Wir erinnern uns, dass ein Eintrag der Gegenstände so aussah:
apfel:4:[-1.0, 0.9587, 0.7929, 1.0, -0.8308, 0.2918, -0.9714, -0.996, -1.0, ...]
Der Aufruf von rstrip('\n') schneidet zuerst die Zeile rechts des New Line Character \n ab. 
Das Resultat teilen wir dann in drei Strings per split(':'). Der Eintrag mit dem Index 2 
in subdata beinhaltet die Liste der Gewichte, allerdings als String, nicht als Liste, weswegen 
wir die eckigen Klammern per Slicing wegschneiden. Das geschieht in subdata[2][1:-1]. 
Das Resultat ist eine kommaseparierte Liste von Strings, die wir in Floats umwandeln, auf 
die vierte Stelle runden und in die Liste questionFloats schreiben. Mit dem Namen des Gegen stands (subdata[0]), der GuessNum (einem Parameter, den wir in unserer Implementierung 
nicht verwenden) und den questionFloats erzeugen wir nun ein neues Item. Vergleichsweise 
einfach gestaltet sich das Einlesen der Fragen, die lediglich zeilenweise verarbeitet und als 
Question-Objekt samt Index der Frage in die Liste questions geschrieben werden.
Die Methode askQuestion() prüft, ob von den 20 Fragen, die wir in unserem Repertoire 
haben, noch welche übrig sind. Wenn ja, liefert es diese zurück und setzt intern den Index 
current_question einen Zähler hoch. Ist keine Frage mehr vorhanden, liefert die Methode 
None zurück.
Die in den Zeilen 54–58 beschriebene Methode getAnswer() liefert, wie wir schon ge sehen haben, den wahrscheinlichsten Gegenstand zurück. Die Auswertung geschieht über 
evaluateCertainties() und wird hier lediglich über einen Template-Satz formatiert.



278 6 Intents entwickeln
Wir haben schon öfter gesehen, dass Computer Gewichte besser in Form von Zahlen hand haben als in Form von Worten. Dafür rechnet evaluateAnswer() in Zeile 60 auch die Ant worten in einen Gleitkommawert um. Die Antwort ja zählt 1, die Antwort vielleicht nur 0.2.
Listing 6.82 Die Klasse Q20Session handelt die Spielelogik und den Spielstand ab.
1. class Q20Session:
2.
3. def __init__(self):
4. self.questions = []
5. self.items = []
6. self.current_question = 0
7.
8. # Lese die Konfiguration
9. config_path = os.path.join('intents','functions','questiongame',
10. 'config_questiongame.yml')
11. cfg = None
12. with open(config_path, "r", encoding='utf8') as ymlfile:
13. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
14.
15. # Holen der Sprache aus der globalen Konfigurationsdatei
16. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
17.
18. self.PLEASE_START_NEW_GAME = cfg['intent']['questiongame'][LANGUAGE]
19. ['please_start_new_game']
20. self.GUESS = cfg['intent']['questiongame'][LANGUAGE]['i_guess']
21.
22. items_path = os.path.join(
23. 'intents','functions','questiongame', 'items_' + LANGUAGE + '.txt'
24. )
25. questions_path = os.path.join(
26. 'intents','functions','questiongame', 'questions_' + LANGUAGE + '.txt'
27. )
28.
29. itemData=open(items_path, encoding="utf-8")
30. data=itemData.readlines()
31. for i in range(len(data)):
32. subdata=data[i].rstrip("\n").split(":")
33. questionFloats=subdata[2][1:-1].split(",")
34. for i in range(len(questionFloats)):
35. questionFloats[i]=round(float(questionFloats[i]),4)
36. self.items.append(Item(subdata[0],int(subdata[1]),questionFloats,
37. len(self.items)))
38.
39. questionData=open(questions_path, encoding="utf-8")
40. data=questionData.readlines()
41. for i in range(len(data)):
42. self.questions.append(Question(data[i].rstrip("\n"),
43. len(self.questions)))
44. questionData.close()
45.
46. def askQuestion(self):
47. if self.current_question < len(self.questions):
48. question = self.questions[self.current_question].string



6.8 Q20-Ratespiel 279
49. self.current_question += 1
50. return question
51. else:
52. return None
53.
54. def getAnswer(self):
55. selectedData = self.evaluateCertainties()
56. result = self.GUESS.format(str(round(100*selectedData[1],2)),
57. self.items[selectedData[0]].name.capitalize())
58. return result
59.
60. def evaluateAnswer(self, answer):
61. if answer.upper() in YES:
62. a = 1
63. elif answer.upper() in NO:
64. a = -1
65. elif answer.upper() in PROBABLY:
66. a = 0.2
67. elif answer.upper() in PROBABLY_NOT:
68. a = -0.2
69. else:
70. a = 0
71. return a
72.
73. def evaluateCertainties(self):
74. maxi=0
75. selected=-1
76. for i in range(len(self.items)):
77. if self.items[i].certainty>maxi:
78. maxi=self.items[i].certainty
79. selected=i
80. return [selected, maxi]
81.
82. def clearSession(self):
83. for i in range(len(self.items)):
84. self.items[i].certainty=0
Die vorletzte Methode evaluateCertainties() geht Gegenstand für Gegenstand durch und 
sucht denjenigen heraus, dessen Wahrscheinlichkeit am höchsten ist. Diese Wahrschein lichkeit liefert die Methode zusammen mit dem Index des Gegenstands in der Item-Liste 
zurück. Zu guter Letzt benötigen wir noch eine Methode, die die Wahrscheinlichkeiten aller 
Gegenstände für das nächste Spiel zurücksetzt. Das übernimmt clearSession() für uns.
Nun sind wir an dem Punkt angelangt, an dem wir das Spiel ausprobieren können! Starten Sie 
es über Starte Frage Spiel, werden Sie feststellen, dass Sie mit dem Assistenten einen Dialog 
führen können – zwar sehr eingeschränkt, aber technisch steht uns nun nichts mehr im 
Wege, die Mechanik auf andere Intents zu übertragen. Am Ende bliebe noch die Überlegung, 
ob man eine Art Timer implementiert, der den Kontext nach einer Weile wieder zurücksetzt, 
falls der Benutzer keine Eingaben mehr macht. Das ist aber fast mehr eine fachliche als eine 
technische Anforderung, denn vielleicht möchten Sie ja, dass Sie ein Gespräch zu einem 
späteren Zeitpunkt immer wieder aufnehmen können, statt es zwischenzeitlich zu beenden. 
Eine Frage ist jedoch noch offen: Können wir unseren Kontext zwischenzeitlich verlassen? 
Aber ja, durch das Aktivierungswort gelangen Sie zu jeder Zeit zu den anderen Intents.



280 6 Intents entwickeln
■ 6.9 Passwortverwaltung
Der letzte Intent, den wir zusammen schreiben werden, war einer meiner Ursprungsgedanken 
für die Verwendung eines Sprachassistenten als Anwendung auf meinem PC. Die Menge an 
Passwörtern, die wir uns mittlerweile als internetaffine Menschen merken müssen, steigt 
stetig an – bei mir zumindest. Und ebenso wächst die Verlockung, lediglich ein Passwort für 
alle Konten im Netz zu verwenden, was sicherlich aus Sicherheitsaspekten kaum zu recht fertigen ist. Wie wäre es denn, wenn Sie sich mit Ihrer Stimme authentifizieren könnten? 
Das wollen wir angehen!
Statt jedoch selber einen Passwortmanager zu schreiben, wollen wir auf KeePass aufsetzen, 
eine offene Lösung, die sicher der ein oder andere kennt und für dessen Key-Store-Format 
auch Python-Bibliotheken existieren, um diese zu entsperren, zu lesen und zu ändern. Der 
Intent soll so funktionieren, dass wir ein konkretes Passwort abfragen können, das dann 
nicht etwa laut ausgesprochen, sondern von unserem Assistenten direkt getippt wird. So ist 
es für Außenstehende weder zu hören noch zu sehen, wenn die Maus bereits im Passwort feld platziert wurde.
Das Projekt finden Sie im Ordner 010_i_password. Zur Erinnerung: Legen Sie wie immer ein 
neues Environment an, installieren Sie die Abhängigkeiten über pip install -r requirements.txt, 
das Snips-NLU-Sprachpaket über python -m snips_nlu download de (Sie müssen den Anaconda 
Prompt dafür als Administrator starten) und ffmpeg über conda install ffmpeg.
Dann ist es an der Zeit, sich KeePass herunterzuladen, zu installieren und darin über File/New
eine neue Passwortdatenbank Namens db.kdbx in intents\functions\password anzulegen. 
Darauf öffnet sich ein Dialog mit dem Titel Create Master Key, in dem wir das Häkchen bei 
Master Password entfernen und stattdessen bei Show expert options setzen (siehe Bild 6.56). 
Dort sollte ein Haken bei Key file / provider gesetzt sein, um statt eines Passworts eine Datei 
zum Öffnen unserer Passwortdatenbank zu verwenden. Dieses Key file legen wir auch gleich 
durch einen Klick auf Create… an und wählen im neu erscheinenden Dialog unter Format 
version die Version 1.0. Ein Klick auf OK führt uns zum nächsten Fenster, in dem wir einen 
Random Bit Stream erzeugen müssen, indem wir die Maus über das linke Fenster bewegen 
oder wild im rechten Fenster herumtippen. Ist der Balken unten neben Generated bits grün, 
können wir den Dialog mit OK verlassen. Nun sollte Ihnen angeboten werden, die Datei zu 
speichern, was wir unter dem Namen secret.keyx in intents\functions\password auch tun. Ein 
erneuter Klick auf OK führt weiter zum nächsten Dialog, in dem wir im Reiter General unter 
Database name den Namen db eintragen und die Erstellung damit abschließen. Drucken Sie 
sich gerne den Notfallbogen aus, wenn Sie den Bedarf sehen.



6.9 Passwortverwaltung 281
Bild 6.56 Anlage einer neuen Passwortdatenbank in KeePass
Wir fahren mit der Anlage des Templates in intents\chatbotai\password.template fort. 
Dieses sehen Sie in vollendeter Form in Listing 6.83. Im ersten Block ermöglichen wir es 
dem Benutzer, den Begriff Passwort gefolgt von einem benamten Eintrag in unserer Pass wortdatenbank zu sprechen, sodass der Assistent mit der Funktion getPasswort() darauf 
reagieren kann. Der zweite Block liefert nicht das Passwort, sondern den Benutzernamen 
über getUsername() zurück.
Listing 6.83 Template für den Passwort-Intent
{% block %}
 {% client %}(Passwort) (?P<entry>.*){% endclient %}
 {% response %}{% call getPassword: %entry %}{% endresponse %}
{% endblock %}
{% block %}
 {% client %}(Benutzer|Benutzername) (?P<entry>.*){% endclient %}
 {% response %}{% call getUsername: %entry %}{% endresponse %}
{% endblock %}



282 6 Intents entwickeln
Wie üblich kümmern wir uns nun um die zum Intent gehörende Konfiguration in intents\
functions\password\config_password.yml (zu sehen in Listing 6.84). Die Einträge sprechen 
weitestgehend für sich selbst, wir wollen sie dennoch kurz durchsprechen, da die Funktions weise des Intents dadurch klarer wird:
 unknown_entry: Jedem Eintrag in der Passwortdatenbank ist ein Bezeichner, ein Benutzer name und ein Passwort zugeordnet. Wir wollen unseren Intent so aufbauen, dass wir fragen 
können, wie das Passwort für Bezeichner xy ist, zum Beispiel für Heise Online. Wird kein Eintrag 
mit diesem Bezeichner gefunden, geben wir unknown_entry zurück.
 typed_pw: Da unser Sprachassistent das Passwort nicht laut aussprechen oder anzeigen 
soll, wollen wir die Bibliothek pynput benutzen, um die Tastenanschläge zu simulieren, 
die wir sonst tätigen müssten, um unser Passwort einzugeben. Als akustisches Feedback, 
dass die Eingabe erfolgt ist, wird typed_pw gesprochen.
 db_not_found: Die Datei db.kdbx, die unsere Passwortdatenbank enthält, wurde nicht 
gefunden.
 key_not_found: Die Datei secret.keyx, die unser Key File enthält, wurde nicht gefunden.
 could_not_access_keystore: Sollte die Passwortdatenbank nicht geöffnet werden können, 
zum Beispiel da das Key File nicht zu der Datenbank passt, wird dieses Template gesprochen.
 no_voice_match: Konnte unsere Stimme nicht verifiziert werden, wird die Passwortdaten bank nicht geöffnet und no_voice_match wird gesprochen.
Ganz am Ende der Datei finden Sie noch die Verweise auf die Dateien der Passwortdatenbank 
und des Schlüssels, die wir zuvor angelegt haben. Achten Sie darauf, dass das Verzeichnis 
des Intents mit ./ referenziert wird und es sich nicht um das Root-Verzeichnis des Sprach assistenten handelt!
Listing 6.84 Konfigurationsdatei des Passwort-Intents
intent:
 password:
 de:
 unknown_entry: ["Ich habe keine Zugangsdaten für {} gefunden."]
 typed_pw: "Passwort für {} getippt"
 db_not_found: "Die Passwortdatenbank wurde nicht gefunden."
 key_not_found: "Der Schlüssel für die Datenbank wurde nicht gefunden."
 could_not_access_keystore: "Konnte Passwortdatenbank nicht öffnen."
 no_voice_match: "Du darfst leider nicht auf die Passwortdatenbank zugreifen."
 en:
 unknown_entity: ["I could not find any credentials for {}."]
 typed_pw: "Typed password for {}"
 db_not_found: "The key database was not found."
 key_not_found: "The secret key to access the database was not found."
 could_not_access_keystore: "Could not open keystore"
 no_voice_match: "You are not allowed to access the keystore"
 db_file: ./db.kdbx
 key_file: ./secret.keyx
In Listing 6.85 benennen wir eben noch unsere drei Requirements, die speziell für den 
Passwort-Intent benötigt werden. pykeepass erlaubt uns die KeePass-Datenbank zu öffnen, 



6.9 Passwortverwaltung 283
zu lesen und zu schreiben, pynput simuliert die Tastenanschläge zur Eingabe der Passwörter 
und fuzzywuzzy hilft uns dabei, unsere gesprochenen Texte auf die mit Sicherheit manch mal etwas kryptischen Bezeichner in der Passwortdatenbank abzubilden. Heißt einer Ihrer 
Einträge etwa web.de, wird VOSK sicher den Punkt in der Mitte nicht erahnen. Wir sollten 
derartige Fälle mit einplanen und eine Unschärfe in unseren String-Vergleichen zulassen.
Listing 6.85 Requirements des Passwort-Intents
pykeepass==3.2.1
pynput==1.7.3
fuzzywuzzy==0.18.0
Wir registrieren in Listing 6.86 zunächst die Funktion getPassword() und implementieren 
die dazugehörige Logik, die als Parameter entry entgegennimmt, den Bezeichner, den der 
Benutzer aus der Passwortdatenbank abfragen möchte. Die Zeilen 15–43 sollten uns mittler weile nicht mehr überraschen, da sie lediglich dafür verantwortlich sind, dass die Konfigura tion eingelesen wird und die darin referenzierten Dateien auf ihre Existenz geprüft werden.
In den Zeilen 48–49 wird nun versucht, die Passwortdatenbank mit den gegebenen Dateien 
zu öffnen. Schlägt dieser Versuch fehl, wird in Zeile 51 eine entsprechende Meldung ausge geben. In Zeile 54 fragen wir die Passwortdatenbank ab, indem wir den Eintrag _fingerprint
suchen. Können Sie sich denken, was hier passiert? Genau, wir haben zuvor den Finger abdruck unserer Stimme als Eintrag in der Passwortdatenbank im Kommentarfeld (Notes) 
des jeweiligen Eintrags hinterlegt. Von da lesen wir den uns bereits bekannten numerischen 
Wert und gleichen ihn mit dem Stimmabdruck des aktuellen Sprechers ab (Zeilen 57–60). 
Sind die beiden Fingerabdrücke zu unterschiedlich, wird der Benutzer in Zeile 62 darüber 
informiert. Passen die Stimmen, holen wir alle Einträge aus der Passwortdatenbank und 
gehen diese Titel für Titel durch (Zeile 66), um über Fuzzywuzzy den jeweiligen Titel mit dem 
angefragten Begriff zu vergleichen. Haben wir eine Übereinstimmung von mehr als 70 %, 
bereiten wir den Assistenten auf die Tastatureingabe vor.
Dazu initialisieren wir das Objekt keyboard in Zeile 73 und lassen in der folgenden Zeile das 
Passwort über keyboard.type() tippen. Zurück geben wir nun eine Sprachmeldung, dass das 
Schreiben für den per Fuzzy-Vergleich gefundenen Eintrag erfolgreich war.
Listing 6.86 Intent zum Abfragen und automatischen Tippen eines Passworts
1. from loguru import logger
2. from chatbot import register_call
3. import global_variables
4. import yaml
5. import random
6. import os
7. from pykeepass import PyKeePass
8. from pynput.keyboard import Key, Listener, Controller as keyboard_controller
9. from fuzzywuzzy import fuzz
10. import json
11. import numpy as np
12.



284 6 Intents entwickeln
13. @register_call("getPassword")
14. def getPassword(session_id = "general", entry="none"):
15. cfg = None
16.
17. # Laden der Intent-eigenen Konfigurationsdatei
18. config_path =
19. os.path.join('intents','functions','password','config_password.yml')
20. with open(config_path, "r", encoding='utf-8') as ymlfile:
21. cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)
22.
23. # Holen der Sprache aus der globalen Konfigurationsdatei
24. LANGUAGE = global_variables.voice_assistant.cfg['assistant']['language']
25.
26. db_file = cfg['intent']['password']['db_file']
27. key_file = cfg['intent']['password']['key_file']
28. typed_pw = cfg['intent']['password'][LANGUAGE]['typed_pw']
29.
30. db_file = os.path.join('intents','functions','password',db_file)
31. key_file = os.path.join('intents','functions','password',key_file)
32.
33. if not os.path.exists(db_file):
34. return cfg['intent']['password'][LANGUAGE]['db_not_found']
35.
36. if not os.path.exists(key_file):
37. return cfg['intent']['password'][LANGUAGE]['key_not_found']
38.
39. UNKNOWN_ENTRY =
40. random.choice(cfg['intent']['password'][LANGUAGE]['unknown_entry'])
41. UNKNOWN_ENTRY = UNKNOWN_ENTRY.format(entry)
42.
43. NO_VOICE_MATCH = cfg['intent']['password'][LANGUAGE]['no_voice_match']
44.
45. # Konnte die Konfigurationsdatei des Intents geladen werden?
46. if cfg:
47. try:
48. kp = PyKeePass(os.path.abspath(db_file),
49. keyfile=os.path.abspath(key_file))
50. except Exception as e:
51. return cfg['intent']['password'][LANGUAGE]['could_not_access_keystore']
52.
53. # Verifiziere Stimme
54. fp_entry = kp.find_entries(title='_fingerprint', first=True)
55. if fp_entry:
56. a = json.loads(fp_entry.notes)
57. b = global_variables.voice_assistant.current_speaker_fingerprint
58. nx = np.array(a)
59. ny = np.array(b)
60. cosDist = 1 - np.dot(nx, ny) / np.linalg.norm(nx) / np.linalg.norm(ny)
61. if (cosDist >= 0.3):
62. return NO_VOICE_MATCH
63.
64. entries = kp.entries
65.



6.9 Passwortverwaltung 285
66. for title in entries:
67. ratio = fuzz.ratio(title.title.lower(), entry.lower())
68. logger.info("Übereinstimmung von {} und {} ist {}%", title.title, entry,
69. ratio)
70. if ratio > 70:
71.
72. if (title):
73. keyboard = keyboard_controller()
74. keyboard.type(title.password)
75. return typed_pw.format(title.title)
76.
77. return UNKNOWN_ENTRY
78. else:
79. logger.error("Konnte Konfiguration für Intent 'password' nicht laden.")
80. return
Die Logik der beiden Intents zur Abfrage eines Passworts zu einem Eintrag in der Passwort datenbank und der Abfrage eines Benutzernamens in selbiger sind so ähnlich, dass ich für 
letztere nur den sich ändernden Ausschnitt in Listing 6.87 zeige. Statt das Passwort wie eben 
gesehen zu tippen, geben wir lediglich den Benutzernamen über title.username zurück, der 
dann von unserem Assistenten laut ausgesprochen wird.
Listing 6.87 Intent zum Abfragen eines Benutzers zu einem Eintrag in der Passwortdatenbank
1. ... [Ab Zeile 72]
2. if (title):
3. return title.username
4. ...
Das war es auch schon! Legen Sie zunächst den Fingerabdruck Ihrer Stimme, den Sie sich 
gerne per Log-Meldung ausgeben lassen können, als Eintrag unter _fingerprint in der Pass wortdatenbank ab und machen Sie einen weiteren Eintrag, den Sie dann abfragen können.
HINWEIS: Wie wäre es, wenn Sie sich mal daran versuchen, den Intent dahinge hend zu erweitern, dass dieser Sie auffordert, einen automatisch generierten Satz 
zu sprechen und nur, wenn Ihr Stimmabdruck und der Inhalt des Satzes überein stimmen, wird das Passwort eingetragen. So verhindern Sie das Angriffsmuster 
der Stimmaufzeichnung.



286 6 Intents entwickeln
■ 6.10 Weitere Ideen
Nun haben wir nicht nur eine annehmbare Grundausstattung an Funktionalitäten, die unsere 
Benutzer jederzeit abrufen, sondern auch ein Framework, auf das Sie und andere Entwick ler aufsetzen können, um weitere Intents zu schreiben und dynamisch einzubinden. Ihrer 
Kreativität sind keine Grenzen gesetzt, so sagt man doch so schön. Tabelle 6.9 liefert dazu 
ein paar kreative Denkanstöße.
Tabelle 6.9 Ideen für weitere Intents
Übersetzung – 
Was bedeutet Käfer 
auf Englisch?
Taschenrechner – 
Was ist 12 im Quadrat?
Nachrichten wiedergeben – 
Spiele die Tagesschau.
Stundenplan der Kinder – 
Wann kommt Kevin heute 
nach Hause?
Abfallkalender – 
Morgen wird die graue Tonne 
geleert.
Zufallsgenerator – 
Würfel eine Zufallszahl.
Rezeptvorschläge – 
Was koche ich die Woche?
Barkeeper – 
Wie mixe ich einen Cuba Libre?
Fernsehprogramm –
Was läuft im Moment auf ARD?
Kinoprogramm – 
Was läuft heute im Kino in 
Landau?
Workout – 
Starte mein Anfänger-Workout.
Einschlafgeräusche – 
Spiele 10 Minuten Meeres rauschen.
Zähneputzuhr – 
Starte Zähneputzuhr.
Schlaue Sprüche – 
Was ist die Weisheit des Tages?
Witze – 
Erzähle einen Witz für Kinder/
für Erwachsene.
City Guide – 
Was sind die 3 beliebtesten 
Ausflugsziele in Mannheim?
Börsenkurse – 
Wie steht der DAX heute? 
Wie denkst du, wird er sich 
morgen entwickeln?
Taxi rufen – 
Bestelle ein Taxi für heute, 
14 Uhr.
Benzinpreise – 
Wo tanke ich heute im Radius 
von 10 km am Günstigsten?
Inhaltsstoffe/Nutriscore – 
Welche Inhaltsstoffe hat 
Nutella?
Geburtstage – 
Wer hat demnächst Geburts tag? (Bekannte/Promis)
Preisalarm – 
Benachrichtige mich, wenn 
der Raspberry Pi weniger als 
50 € kostet.
Fußballergebnisse – 
Sage mir die Fußballergebnisse 
vom Wochenende.
Zufallsnamen – 
Wie sollen wir unseren Sohn 
nennen?
Sie sehen, es ist nicht besonders schwierig, eigene Ideen zu entwickeln. Häufig lohnt es 
sich, ein offenes Ohr und Auge für die Alltagsherausforderungen zu haben und daraus 
eine Digitalisierungsherausforderung zu machen. Denn die wenigsten bahnbrechenden 
Neuerungen in Technik und IT basieren auf bahnbrechenden Ideen. In der Regel sind diese 
und die Probleme, die sie lösen, schon seit Jahren bekannt. Man muss sie nur erkennen. Die 
Umsetzung ist für Menschen wie uns meist trivial.



7 Ein simples 
User Interface
Zugegeben, bei einem Sprachassistenten denken nur die wenigsten an ein User Interface
(UI), denn eigentlich wollen wir Ein- und Ausgabe ausschließlich über unsere eigene und die 
synthetisierte Sprache geschehen lassen. Mein erster Gedanke bei der Entwicklung des Assis tenten war jedoch, dass er als kleines, unscheinbares Programm auf meinen Arbeitslaptops 
laufen sollte, um mir dort müßige Arbeit abzunehmen. Da die gängigen Betriebssysteme 
aber häufig noch eine grafische Oberfläche haben und auch die Programmsteuerung über 
diese geschieht, brauchen wir zumindest eine minimalistische Form, um den Zustand des 
Assistenten sehen und diesen schließen zu können. Diesen Minimalismus möchte ich mit 
Ihnen so umsetzen, dass wir lediglich ein Tray Icon, zum Beispiel in Windows neben der Uhr oder in 
der Menüleiste unserer Linux-Distribution, sehen. Dieses Icon soll neben einem Menü, das 
bei einem Mausklick gezeigt wird und uns den Assistenten beenden lässt, Auskunft über den 
Zustand der Anwendung geben. Diese wird durch die vier Grafiken in Bild 7.1 repräsentiert.
a) b) c) d)
Bild 7.1 Grafiken, die die Zustände „laden“ (a), „bereit“ (b), „zuhören“ (c), „sprechen“ (d) repräsen tieren.
Weiterhin wollen wir uns anschauen, wie wir die Konsole verbergen, aus der wir den Assisten ten bisher immer gestartet haben, denn eine Konsolenanwendung hat – zumindest für mich 
– immer etwas von einem Batch-Prozess, der irgendwann enden und das Fenster schließen 
sollte. Wenn wir jedoch die Konsole verbergen, müssen wir ebenfalls die Log-Ausgaben um leiten, um auch später noch sehen zu können, ob im Assistenten Fehler aufgetreten sind, 
wie etwa dass Befehle missinterpretiert oder Konfigurationen, Modelle oder Intents nicht 
geladen werden. Die Logs sollen dazu in eine Datei umgeleitet werden.



288 7 Ein simples User Interface
■ 7.1 Einrichten eines Tray-Icons
Die Arbeit, die wir in diesem gesamten Kapitel verrichten, findet fast ausschließlich in der 
main Punkt Pei statt, denn diese müssen wir auf ein anderes Gerüst stellen, nämlich auf das von 
wxPython, auch wenn wir lediglich das TaskBarIcon daraus verwenden werden. Was auf den 
ersten Blick vielleicht etwas zu over engineered wirkt, erfüllt jedoch unsere Anforderungen an 
die Plattformunabhängigkeit. Denn unter den Frameworks, die ich während der Entwicklung 
evaluiert habe, funktioniert wxPython hier am besten.
TIPP: Wenn Sie irgendwann in die Situation kommen sollten, dass Sie eine kom plexe GUI-Anwendung in Python schreiben müssen, dann sollte die Auswahl des 
Frameworks auf PyQt (oder PySide2; der Unterschied besteht hauptsächlich in 
der Lizenz und ein paar Namen in der API, beide sind jedoch Wrapper um das 
Qt-Framework) fallen. Zwar existieren noch viele weitverbreitete Frameworks wie 
Tkinter, Kivy oder libavg, doch keines ist so mächtig wie PyQt. Dazu zähle ich nicht 
nur das API-Design, sondern auch das Tooling, um zum Beispiel UIs per Editor zu erstellen 
(siehe Qt Designer), das Implementieren von Threads innerhalb des Anwendungs gerüsts und dem gegenseitigen Austausch von Daten oder dem Styling. Qt-Mate rial beispielsweise lässt Ihre Anwendung sehr schnell, sehr professionell aus sehen, indem es ihr per Stylesheet einen neuen Look verleiht - der mitunter auch 
mal zu bunt ausfallen kann, jedoch können Sie auch selber sehr schnell eigene 
Styles erzeugen. Lange Rede, kurzer Sinn: Beginnen Sie mit PyQt. Wenn sich wäh rend der Entwicklung ein Grund ergibt, auf ein anderes Framework umzusteigen, 
dann natürlich gerne.
Das Beispiel dieses Kapitels finden Sie in 11_ui. Legen Sie das entsprechende Environment 
an und installieren Sie alle Abhängigkeiten. Beginnen wir dann damit, dass wir den Inhalt 
von Listing 7.1 in die main Punkt Pei aufnehmen. In den Zeilen 1 und 2 werden die Imports für das 
Anwendungsgerüst in wxPython ins Projekt geholt, gefolgt von einem Modul constants, das 
wir gleich in Listing 7.2 betrachten. Nun beginnen wir, uns die Klasse TaskBarIcon anzu schauen, die von wx.adv.TaskBarIcon abgeleitet wurde und neben der Methode __init__()
noch vier weitere beinhaltet:
 set_icon(): Ändert das Icon unseres TaskBarIcons und stellt einen Hinweistext (Tooltip) 
bereit, der gezeigt wird, wenn wir mit der Maus über das Icon fahren.
 __init__(): Initialisiert unser TaskBarIcon, indem es die __init__()-Methode von 
wx.adv.TaskBarIcon aufruft, den Frame speichert, dem das Icon angehört (dazu kommen wir 
gleich) und set_icon() aufruft, um Icon a) aus Bild 7.1 anzuzeigen. Die sich in constants.
TRAY_ICON_INITIALIZING verbergende Zeichenkette beherbergt den Dateinamen der 
Grafik, wie wir gleich sehen werden. Der Hinweistext wiederum setzt sich zusammen aus 
constants.TRAY_TOOLTIP und dem String Initialisiere… Wir setzen genau diesen Hinweis text, da er angezeigt werden soll, solange der Assistent startet. Sobald er bereit ist, Befehle 
entgegenzunehmen, updaten wir Icon und Text.
 create_menu_item(): Fügt dem Menü in menu einen Menüeintrag hinzu, beschriftet diesen 
mit label und hinterlegt eine Funktion func, die bei einem Klick darauf aufgerufen wird.



7.1 Einrichten eines Tray-Icons 289
 CreatePopoupMenu(): Diese Methode aus wx.adv.TaskBarIcon überschreiben wir, sodass 
sie uns ein Menü anlegt, das aufgerufen wird, wenn wir das Icon mit der rechten Maustaste 
anklicken. Dieses Menü beinhaltet lediglich einen Eintrag mit der Beschriftung Beenden, 
der bei einem Klick den Sprachassistenten schließt.
 on_exit(): Wird aufgerufen, wenn der Menüeintrag Beenden angeklickt wurde. In diesem 
Fall wird der Sprachassistent über dessen Methode terminate() sauber heruntergefah ren. Per wx.CallAfter() definieren wir eine Funktion, hier self.Destroy(), die nach 
Abarbeitung des aktuellen Events aufgerufen wird. In diesem Fall sorgt self.Destroy()
dafür, dass die Existenz des TaskBarIcon ein Ende findet. Darauf wird der Frame, dessen 
Referenz wir in der Initialisierung übergeben bekommen, geschlossen, womit sich die 
gesamte Anwendung schließt.
Listing 7.1 Einrichten des Tray Icons
1. import wx.adv
2. import wx
3.
4. # Konstanten für das Tray Icon
5. import constants
6.
7. CONFIG_FILE = "config.yml"
8.
9. # Eine Klasse, die die Logik unseres Tray Icons abbildet.
10. class TaskBarIcon(wx.adv.TaskBarIcon):
11. def __init__(self, frame):
12. self.frame = frame
13. super(TaskBarIcon, self).__init__()
14. self.set_icon(constants.TRAY_ICON_INITIALIZING, constants.TRAY_TOOLTIP +
15. ": Initialisiere...")
16.
17. # Methode, um Menüeinträge hinzuzufügen.
18. def create_menu_item(self, menu, label, func):
19. item = wx.MenuItem(menu, -1, label)
20. menu.Bind(wx.EVT_MENU, func, id=item.GetId())
21. menu.Append(item)
22. return item
23.
24. # Wir erstellen einen Menüeintrag, der bei einem Rechtsklick gezeigt wird.
25. def CreatePopupMenu(self):
26. menu = wx.Menu()
27. self.create_menu_item(menu, 'Beenden', self.on_exit)
28. return menu
29.
30. # Ändern des Icons und des Hilfetextes
31. def set_icon(self, path, tooltip=constants.TRAY_TOOLTIP):
32. icon = wx.Icon(path)
33. self.SetIcon(icon, tooltip)
34.
35. # Beenden der Applikation über das Menü
36. def on_exit(self, event):
37. if global_variables.voice_assistant:
38. global_variables.voice_assistant.terminate()
39. wx.CallAfter(self.Destroy)
40. self.frame.Close()



290 7 Ein simples User Interface
Die Definition unserer eigenen TaskBarIcon-Klasse ist abgeschlossen. Werfen wir einen ganz 
kurzen Blick in die constants.py in Listing 7.2. Sie sehen, dass hier lediglich Zeichenketten 
definiert werden, die wir theoretisch auch direkt in die main Punkt Pei hätten aufnehmen können, 
doch so können wir auch aus anderen Modulen darauf zugreifen, weswegen das Auslagern 
durchaus sinnvoll ist. Der TRAY_TOOLTIP beinhaltet den Text, der beim Hover der Maus über 
das Icon gezeigt wird, TRAY_ICON_INITIALIZING, TRAY_ICON_IDLE, TRAY_ICON_LISTENING
und TRAY_ICON_SPEAKING halten lediglich den Dateinamen der jeweiligen Grafiken vor, 
sodass Sie diese nach Belieben austauschen können.
Listing 7.2 Definieren der Konstanten in constants.py zur UI-Darstellung
TRAY_TOOLTIP = 'Voice Assistant'
TRAY_ICON_INITIALIZING = 'initializing.png'
TRAY_ICON_IDLE = 'idle.png'
TRAY_ICON_LISTENING = 'listening.png'
TRAY_ICON_SPEAKING = 'speaking.png'
In Listing 7.3 geht es nun darum, eine Klasse zu erzeugen, die die eigentliche Applikation 
darstellt. Diese baut auf wx.App auf und ist für die gesamte Verwaltung des Lebenszyklus 
der Anwendung und deren Dialoge verantwortlich sowie für die Hauptschleife selbiger und 
das Verteilen von Events an diverse Elemente der GUI. Beginnen wir mit dem Überschreiben 
von OnInit() in Zeile 2. Darin erstellen wir zunächst einen Frame, was nichts anderes ist 
als ein Fenster, auf dem wir theoretisch beliebige Komponenten, wie Buttons oder Labels, 
platzieren könnten. Indem als erstes Argument ein None übergeben wird, legen wir fest, dass 
unser Fenster keinen Parent hat, dem es zugeordnet ist. Mit SetTopWindow(frame) machen 
wir das eben erzeugte (aber nicht sichtbare) Fenster zum Hauptfenster der Anwendung. Es 
folgt der Aufruf unseres Konstruktors von TaskBarIcon, dem wir, wie vorher festgelegt, den 
Frame als Argument übergeben und die Referenz auf das erzeugte Icon in self.icon ablegen. 
In Zeile 6 binden wir den Event, der geworfen wird, wenn das Fenster geschlossen wird, an 
die Methode onCloseWindows(), die Sie in Zeile 18 sehen.
Nun wird es interessant. Wir erzeugen einen Timer, der in einem noch festzulegenden Intervall 
eine bestimmte Funktion aufruft. Welche, sehen Sie in Zeile 10, nämlich self.update(). Dort 
wird dem Event EVT_TIMER unseres eben erzeugten Timers besagte Funktion zugewiesen. 
Zuletzt sind wir angehalten, eine positive Rückmeldung in Form eines True zu geben, damit 
die Methode als erfolgreich durchlaufen verbucht wird.
Kommen wir gleich zu update(), in der nichts anderes geschieht, als dass die Methode 
loop() unseres Sprachassistenten aufgerufen wird. Hier deutet sich schon an, dass wir uns 
in der Klasse VoiceAssistant von der Hauptschleife verabschieden und die Koordination der 
Aufrufe in die Obhut des Timers übergeben. Dazu wird später die uns bekannte Funktion 
run() in loop() abgeändert werden, aber alles der Reihe nach.
In Zeile 18 sehen Sie zu guter Letzt den Aufruf von onCloseWindow(), der das Icon aus 
dem Tray entfernt und dafür sorgt, dass der EVT_CLOSE nicht weiter behandelt wird, indem 
evt.Skip() in Zeile 20 verwendet wird. Denn wir wollen ja zunächst, dass der Sprach assistent sauber heruntergefahren wird, um etwa dessen Lautstärke zu speichern. Erst dann 
kümmern wir uns in TaskBarIcon darum, dass das Hauptfenster und somit die Anwendung 
geschlossen wird.



7.1 Einrichten eines Tray-Icons 291
Listing 7.3 Die MainApp dient als Dreh- und Angelpunkt unserer UI
1. class MainApp(wx.App):
2. def OnInit(self):
3. frame = wx.Frame(None)
4. self.SetTopWindow(frame)
5. self.icon = TaskBarIcon(frame)
6. self.Bind(wx.EVT_CLOSE, self.onCloseWindow)
7.
8. # Erstelle einen Timer, der die Hauptschleife unseres Assistenten ausführt
9. self.timer = wx.Timer(self)
10. self.Bind(wx.EVT_TIMER, self.update, self.timer)
11.
12. return True
13.
14. def update(self, event):
15. if global_variables.voice_assistant:
16. global_variables.voice_assistant.loop()
17.
18. def onCloseWindow(self, evt):
19. self.icon.Destroy()
20. evt.Skip()
Den etwas sperrigen Umbau von VoiceAssistant haben wir nun lange genug vor uns her geschoben. Lassen Sie uns diesen in Listing 7.4 angehen. Zunächst werden wir gleich zu 
Beginn eine Instanz der von uns geschriebenen Klasse MainApp anlegen (Zeile 7). Werfen 
wir doch mal einen Blick auf die nicht ganz so intuitiven Parameter:
 clearSigInt: Die Signale, die eine Anwendung empfängt, etwa wenn der Benutzer diese per 
STRG+C beenden möchte. In diesem Fall schalten wir ab, indem wir clearSigInt auf False
setzen, denn wir wollen lediglich zulassen, dass der Assistent über das Menü im Tray Icon
geschlossen werden kann.
 redirect: Die Ausgabe der Logmeldungen sollen umgeleitet und nicht weiter in die Konsole 
ausgegeben werden.
 filename: Konkret soll das Ziel der Logmeldungen die Datei log.txt sein, die wir später im 
Hauptverzeichnis unserer Anwendung finden werden, wenn diese einmal gestartet wurde.
Nun überspringen wir einen großen Teil der Logik in der __init__()-Methode, denn diese 
bleibt so, wie sie ist. Erst an deren Ende (Zeile 12) greifen wir auf set_icon() zu, um per 
Grafik und Tooltip zu signalisieren, dass der Sprachassistent bereit ist, Befehle entgegen zunehmen. In den Zeilen 14 und 15 starten wir dann den Timer, der in einem Intervall von 
einer Millisekunde immer wieder aufgerufen werden soll. Das immer wieder legen wir über 
den Parameter wx.TIMER_CONTINUOUS fest.
Gänzlich neu mag terminate() in Zeile 21 erscheinen, doch tatsächlich habe ich hier nur 
sämtliche Aufräumarbeiten, die in der Methode run() im Finally-Block ganz am Ende platziert 
waren, hereingezogen. Hier stoppen wir den Timer, speichern unsere Konfiguration und 
schließen sämtliche Audio-Streams. Aufgerufen wird terminate() in der Methode on_exit()
des TaskBarIcons, die wir in Listing 7.1 ausformuliert und besprochen haben.



292 7 Ein simples User Interface
Listing 7.4 Anpassen der Klasse VoiceAssistant an das neue GUI-Framework
1. class VoiceAssistant:
2.
3. def __init__(self):
4. logger.info("Initialisiere VoiceAssistant...")
5.
6. logger.info("Initialisiere UI...")
7. self.app = MainApp(clearSigInt=False, redirect=True, filename='log.txt')
8.
9. logger.debug("Lese Konfiguration...")
10. ...
11.
12. self.app.icon.set_icon(constants.TRAY_ICON_IDLE, constants.TRAY_TOOLTIP +
13. ": Bereit")
14. timer_start_result = self.app.timer.Start(milliseconds=1,
15. oneShot=wx.TIMER_CONTINUOUS)
16. logger.info("Timer erfolgreich gestartet? {}", timer_start_result)
17.
18. def __detectSpeaker__(self, input):
19. ...
20.
21. def terminate(self):
22. logger.debug('Beginne Aufräumarbeiten...')
23.
24. # Stoppe den Timer
25. self.app.timer.Stop()
26.
27. # Speichern der Konfiguration
28. self.cfg["assistant"]["volume"] = self.volume
29. self.cfg["assistant"]["silenced_volume"] = self.silenced_volume
30. with open(CONFIG_FILE, 'w') as f:
31. yaml.dump(self.cfg, f, default_flow_style=False, sort_keys=False)
32.
33. if self.porcupine:
34. self.porcupine.delete()
35.
36. if self.audio_stream is not None:
37. self.audio_stream.close()
38.
39. if self.audio_player is not None:
40. self.audio_player.stop()
41.
42. if self.pa is not None:
43. self.pa.terminate()
44.
45. # Jegliche Logik aus der Main-Methode wird nun in die Update-Methode verlagert
46. def loop(self):
47. pcm = self.audio_stream.read(self.porcupine.frame_length)
48. pcm_unpacked = struct.unpack_from("h" * self.porcupine.frame_length, pcm)
49. keyword_index = self.porcupine.process(pcm_unpacked)
50. if keyword_index >= 0:
51. logger.info("Wake Word {} wurde verstanden.",
52. self.wake_words[keyword_index])
53. self.is_listening = True
54.



7.1 Einrichten eines Tray-Icons 293
55. # Spracherkennung
56. if self.is_listening:
57. if not self.tts.is_busy():
58. self.app.icon.set_icon(constants.TRAY_ICON_LISTENING,
59. constants.TRAY_TOOLTIP + ": Ich höre...")
60. ...
61. self.tts.say(output)
62. self.app.icon.set_icon(constants.TRAY_ICON_SPEAKING,
63. constants.TRAY_TOOLTIP + ": Ich spreche...")
64.
65. # Wird derzeit nicht zugehört?
66. else:
67.
68. # Reaktiviere is_listening, wenn der Skill weitere Eingaben erfordert
69. if not global_variables.context is None:
70. ...
71. else:
72. if not self.tts.is_busy():
73. self.app.icon.set_icon(constants.TRAY_ICON_IDLE,
74. constants.TRAY_TOOLTIP + ": Bereit")
75. ...
76. self.tts.say(output)
77. self.app.icon.set_icon(constants.TRAY_ICON_SPEAKING,
78. constants.TRAY_TOOLTIP + ": Ich spreche...")
79. ...
In Zeile 26 sehen wir nun unsere Hauptschleife, genannt loop(), die eigentlich genau das 
tut, was zuvor run() erledigt hat, bloß dass loop() selber keine Schleife mehr beinhaltet. 
Es entfällt zu Beginn nur das try: while True:. Am Ende kürzen wir den Finally-Block 
heraus, der ja bereits in die Methode terminate() gewandert ist und ebenso das except 
KeyboardInterrupt, da wir ja über clearSigInt bereits festgelegt haben, dass wir kein Signal 
zum Schließen der Anwendung mehr zulassen wollen. Dadurch brauchen wir auch gar nicht 
erst zu versuchen, ein KeyboardInterrupt abzufangen. Die Logik von loop() bleibt run()
sehr ähnlich. Auch wenn sich augenscheinlich viel verändert hat, sind die Änderungen eher 
unserer Software-Architektur geschuldet. Hinzu kommen lediglich die beiden Aufrufe von 
set_icon() in den Zeilen 58–59, 62–63, 73–74 und 77–78, die abhängig vom Zustand des 
Assistenten die Grafik im Tray verändern.
Schauen wir uns zuletzt in Listing 7.5 an, wie wir unseren Assistenten nun starten. Die 
Initialisierung des VoiceAssistant bleibt gleich, lediglich der Start der Hauptschleife unserer 
Anwendung wird nun über MainLoop() in app vorgenommen.
Listing 7.5 Starten des Assistenten
1. if __name__ == '__main__':
2. multiprocessing.set_start_method('spawn')
3.
4. # Initialisiere den Voice Assistant
5. global_variables.voice_assistant = VoiceAssistant()
6.
7. # Starten der Hauptschleife der UI, die auch unseren Timer beinhaltet.
8. global_variables.voice_assistant.app.MainLoop()



294 7 Ein simples User Interface
Wenn Sie die Anwendung nun starten und Ihnen kein Fehler unterlaufen ist, sollten Sie – 
natürlich abhängig vom Betriebssystem – ein kleines, orangenes Icon sehen, das eine Eieruhr 
zeigt. Wenn der Initialisierungsprozess durchlaufen ist, bekommen Sie neben dem akustischen 
Signal ebenfalls ein visuelles in Form des gelben Icons, das ein Ohr zeigt (siehe Bild 7.2).
Bild 7.2 Eine Toast Notification in Windows 10 gibt bekannt, dass der Sprachassistent erfolgreich 
initialisiert wurde.
Eine weitere Neuerung ist die Umleitung der Logausgaben in die log.txt, die wir per redirect
als Ziel unserer Ausgaben festgelegt haben und die bei genauerer Betrachtung genau die 
Ausgaben zeigt, die wir sonst in der Konsole nachlesen könnten. Leider geht die farbige 
Formatierung verloren, sodass die Textdatei nicht so übersichtlich ist wie die Ausgabe, die 
uns Loguru in der Konsole bietet. Versuchen Sie zum Schluss einen Rechtsklick auf dem Icon 
auszuführen, um das Menü mit dem Eintrag Beenden angezeigt zu bekommen. Ein Klick 
darauf sollte den Assistenten nun sauber beenden.
Verbergen der Konsole beim Ausführen der Anwendung
Um nicht unsere Benutzer mit einem Konsolenfenster zu konfrontieren, wollen wir uns noch 
schnell anschauen, wie wir dieses für die Ausführung unserer main Punkt Pei verbergen können. 
Das ist relativ simpel. Führen Sie doch mal folgenden Befehl aus:
pythonw main Punkt Pei
Der Unterschied ist tatsächlich nur der Buchstabe w hinter dem python, der jedoch dafür 
sorgt, dass eine gänzlich andere Ausführung erwirkt wird. Die Unterschiede möchte ich 
Ihnen kurz mit auf den Weg geben:
 Die Streams (sys.stdin, sys.stdout, sys.stderr), die wir für die Ein- und Ausgabe verwenden 
können, sind in der python.exe mit der Konsole verbunden und wir können diese nutzen, 
um etwa eine Eingabe durch den Benutzer zu erwirken oder Text oder Fehler über diese 
auszugeben. Wird pythonw.exe ausgeführt, sind diese Streams nicht verfügbar und wir 
müssen, wie etwa im Fall von wx.App, dafür sorgen, dass die Ausgabe in eine Datei umge leitet wird. Alternativ kann das auch direkt über den Aufruf pythonw.exe main Punkt Pei 1>stdout.
txt 2>stderr.txt erfolgen.
 In der python.exe ist die Ausführung synchron. Das heißt, die Konsole ist über den gesam ten Lebenszyklus der Anwendung blockiert. Wohingegen pythonw.exe eine asynchrone 



7.2 Anzeigen von Notifications 295
Ausführung bewirkt, d. h. sofort nach dem Starten der Anwendung wird die Konsole für 
die nächste Eingabe freigegeben.
 Treten Fehler in der Anwendung auf, kann es bei der Ausführung mit der pythonw.exe
dazu kommen, dass die Anwendung still und heimlich beendet wird, ohne dass es der 
Benutzer mitbekommt, sollte die Anwendung über kein User Interface verfügen, das sich 
im Fehlerfall schließt.
Tatsächlich ist der Aufruf über pythonw nur eine Übergangslösung, da wir unsere Anwen dung in Kapitel 8 sowieso neu paketieren, um sie später als EXE auszuliefern. Sollten Sie 
den Schritt jedoch gar nicht gehen wollen, da Sie eine Distribution als Executable sowieso 
nicht anstreben, ist pythonw sicher eine schicke Lösung, um unsere Anwendung ohne lästige 
Konsolenfenster ausführen zu können.
■ 7.2 Anzeigen von Notifications
Vielleicht reicht Ihnen das Tray Icon als visuelles Feedback für den Zustand unseres Assisten ten. Für den Fall, dass Sie jedoch dadurch erst auf den Geschmack gekommen sind, möchte 
ich mit Ihnen noch kurz Notifications einbauen. Das sind kleine Fenster, die eine kurze In formation über ein neues Event anzeigen – etwa die letzte Ausgabe unseres Assistenten in 
Textform. Das Erste, was einem vielleicht bei einer visuellen Information in den Sinn kommt, 
ist die klassische Message Box, die einen Text und ein Logo zeigt und per OK weggeklickt 
werden kann. Da diese jedoch etwas aufdringlich wirken können und wir den Arbeitsfluss 
am PC so wenig wie möglich stören wollen, muss eine andere Lösung her. Glücklicherweise 
sind die Entwickler der gängigen Betriebssysteme da sehr findig gewesen.
In Windows XP und Windows 7 gab es die Ballon Tips, die ein Tray Icon hervorrufen konnten 
und die eine Sprechblase mit einem Text und einem optionalen Icon gezeigt haben und 
sich selbst nach einer frei definierbaren Dauer ausblenden konnten. In Windows 10 ver schwanden die Ballon Tips und die nach meinem Geschmack etwas sperrig wirkenden Toast 
Notifications traten auf den Plan. Diese rechteckigen Fenster schieben sich von rechts unten 
ins Bild, verweilen dort ein wenig und verschwinden dann in das Notification Center, das sich 
zeigt, wenn man auf die Sprechblase unten rechts im Desktop klickt. In Ubuntu hingegen 
erscheinen Notifications losgelöst oben rechts in der Ecke des Desktops. Klingt nach einer 
Menge Individualentwicklung, oder? Zum Glück gibt es, wie so oft, eine Bibliothek, die uns 
das alles abnimmt, sodass wir lediglich ein Interface zum Erstellen einer Benachrichtigung 
auf einem beliebigen Betriebssystem nutzen müssen. Diese nennt sich notify-py und wird 
auch sogleich von uns installiert.
TIPP: wxPython verfügt ebenfalls über eine Methode, um Benachrichtigungen 
anzuzeigen. Leider blockieren diese den Thread, sodass der Assistent im Hinter grund keine Sprachbefehle mehr annimmt, was für unseren Fall ein absolutes und 
unzumutbares No-Go wäre.



296 7 Ein simples User Interface
Ergänzen Sie die Konfiguration config.yml um den in Listing 7.6 gezeigten Eintrag show_ballon.
Listing 7.6 show_ballon legt fest, ob Benachrichtigungen angezeigt werden sollen oder nicht
assistant:
 ...
 show_ballon: true
Nun erstellen wir im Hauptverzeichnis der Anwendung eine neue Datei namens notification.
py, die wir mit dem Inhalt von Listing 7.7 füllen. In der Klasse selber befindet sich die stati sche Methode show(), die eine Notification zeigt, auf Wunsch einen Sound dazu spielt und 
ein Icon zeigt. Was ist das Besondere an der Methode? Genau, sie ist statisch, was daran zu 
erkennen ist, dass der erste Parameter nicht self lautet. Das bedeutet, dass wir die Klasse 
nicht initialisieren müssen, um show() zu verwenden. Es genügt der Aufruf Notification.
show('Titel', 'Nachricht').
Listing 7.7 Die Klasse Notification bietet eine statische Methode zum Anzeigen von Benachrichti gungen.
1. from notifypy import Notify
2. import constants
3.
4. class Notification:
5.
6. def show(title, message):
7. notification = Notify()
8. notification.title = title
9. notification.message = message
10. notification.audio = "empty.wav"
11. notification.icon = "va.ico"
12. notification.application_name = "Sprachasssistent"
13. notification.send()
Zwei kleine Hinweise noch: Bei jedem Aufruf der Notification ertönt unter Windows 10 ein 
Standardsound, der irgendwann sehr nervig werden kann, wenn man ihn alle paar Minuten 
einmal hört. Aus diesem Grund übergebe ich in Listing 7.7 in Zeile 10 eine stumme WAV Datei, die dann statt des Sounds abgespielt wird.
HINWEIS: Wenn Sie einen hörbaren Hinweis auf eine Benachrichtigung wünschen, 
ersetzen Sie einfach das empty.wav durch eine eigene Audiodatei. Als Nostalgiker 
verwende ich natürlich das unverwechselbare uhoh von ICQ.
Weiterhin sehen Sie, dass in Zeile 11 eine ico-Bilddatei übergeben wird. Diese habe ich Ihnen 
natürlich im Repository bereitgestellt, falls Sie jedoch eine eigene anlegen möchten, bietet 
sich GIMP dazu an, PNG- oder JPG-Dateien in ein ICO zu konvertieren.
Nun ist es auch schon an der Zeit, die Notifications in unsere main Punkt Pei einzubauen. Fügen Sie 
dazu folgenden Import hinzu:



7.2 Anzeigen von Notifications 297
from notification import Notification
Nun lesen wir die Konfiguration aus und setzen in der __init__()-Methode nach dem Aus lesen der Sprache die Klassenvariable show_ballon wie unten gezeigt:
self.show_ballon = self.cfg['assistant']['show_ballon']
Dadurch wissen wir, ob der Benutzer überhaupt Benachrichtigungen angezeigt bekommen 
möchte oder nicht.
HINWEIS: Warum schreiben Sie nicht einen kurzen Intent, um Benachrichtigungen 
ein- und auszuschalten?
Überall dort, wo nun die Methode tts.say() aufgerufen wird, fügen wir nun den Aufruf von 
Notification.show() ein; etwa in der Methode __init__() unserer Klasse VoiceAssistant
bei erfolgreicher Initialisierung (siehe Listing 7.8).
Listing 7.8 Anzeigen einer Benachrichtigung, nachdem der Sprachassistent gestartet wurde
...
self.tts.say("Initialisierung abgeschlossen")
if self.show_balloon:
 Notification.show('Initialisierung', 'Abgeschlossen')
...
Oder natürlich bei der Ausgabe der vom Assistenten generierten direkten Antworten oder 
in einem Callback in loop():
Listing 7.9 Exemplarische Benachrichtigung bei der Sprachinteraktion in loop()
...
 global_variables.voice_assistant.tts.say(output)
 if self.show_balloon:
 Notification.show("Interaktion", "Eingabe (" + speaker + "): " + sentence +
 ". Ausgabe: " + output)
...
 if self.show_balloon:
 Notification.show('Callback', output)
...
Das erfolgreiche Einbinden unseres Moduls bestätigt sich durch eine erste Benachrichtigung 
ähnlich Bild 7.2, die die entsprechende Toast Notification in Windows 10 kurz anzeigt und 
bei einem Klick auf das kleine Kreuz oder nach einigen Sekunden automatisch verschwin den lässt.
So können wir auch im Falle ausgeschalteter oder nicht vorhandener Lautsprecher mit 
unserem Assistenten interagieren.






8 Paketieren 
der Anwendung
Die gute Nachricht ist, dass wir mit der Entwicklung unseres Sprachassistenten fertig sind 
und ihn so verwenden und sogar erweitern können, wie wir es in den letzten Kapiteln getan 
haben. Die schlechte Nachricht ist, dass andere Benutzer, die vielleicht nicht so affin sind, 
was Softwareentwicklung angeht, mit dem Aufsetzen eines Environments und der Installation 
von Paketen Probleme bekommen werden. Es muss also eine Lösung her. Die Anwendung 
muss in ein selbstständig laufähiges Binary umgewandelt werden, ihr müssen alle Dateien 
mitgegeben werden, die sie zur Ausführung benötigt, und im Optimalfall wird sie auch gleich 
noch in einen Installer gepackt.
In einer C#-Anwendung würde man in diesem Fall in Visual Studio auf den Knopf Build
klicken, um eine Executable der Anwendung zu erzeugen, die wir so überall verteilen und 
ausführen könnten1
. In Python ist dieser Vorgang nicht ganz so trivial. Bevor wir uns aber 
anschauen, wie wir ihn dennoch erfolgreich durchlaufen, wollen wir kurz einen Blick darauf 
werfen, wie wir unser Anaconda Environment persistieren, sodass andere Entwickler zügig 
darauf aufsetzen können. Vielleicht möchte ja jemand den Assistenten weiterentwickeln und 
dabei neben den Abhängigkeiten, die wir über die requirements.txt verwalten, noch einige 
weitere Spezifikationen, etwa die Installation von ffmpeg oder die Python-Version, hinzufü-
gen. Schauen wir uns vor dem eigentlichen Finale noch schnell den Environment Export an. 
Dieser Prozess ist allerdings vorrangig für technisch versierte Anwender gedacht und mag 
einen Normalanwender überfordern.
■ 8.1 Exportieren und Wiederherstellen 
eines Environments
Vorbereitend wollen wir wie immer ein Environment anlegen, da wir auch in diesem Fall 
einige Änderungen am Code vornehmen müssen, vorrangig dafür, damit nach dem Erzeu gen der ausführbaren Anwendung alle benötigten Dateien gefunden werden. Das kommt 
uns aber zugute, brauchen wir doch eine Umgebung, die wir für diesen Abschnitt nutzen 
können, um zu sehen, wie wir ein fertiges Environment in eine YAML-Datei exportieren und 
im Umkehrschluss ein Environment aus einem solchen Export neu aufsetzen. Erstellen Sie 
1 Sofern der Zielrechner im Falle von C# über die entsprechende .NET-Runtime verfügt.



300 8 Paketieren der Anwendung
eine Umgebung mit dem Namen 12_paketieren. Der Befehl für den Export des aktivierten 
Environments ist denkbar einfach:
conda env export > env.yaml
Dadurch legt conda eine entsprechende Datei in unserem Projektordner an, aus der wir 
einen exemplarischen Auszug in Bild 8.1 sehen. Diese untergliedert sich in den Namen der 
Umgebung, die Kanäle, die bei der Installation der Conda Packages verwendet wurden (hier 
nur defaults) und die Abhängigkeiten, die in Gruppen von Bibliotheken unterteilt werden, 
die entweder von Conda oder von Pip installiert wurden. Letztere habe ich in der Grafik aus 
Platzgründen gekürzt.
Bild 8.1 Die exportierte Definition der Umgebung erlaubt es uns, eine identische Umgebung auf 
einem fremden Entwicklungs- oder Benutzerrechner aufzusetzen.
Auch die Anlage eines neuen Environments aus env.yaml ist simpel.
conda env create -n myenv -f env.yaml
Dadurch erhalten wir eine neue Umgebung mit Namen myenv und den vorinstallierten Biblio theken aus der yaml-Datei. Falls Sie bereits eine Umgebung angelegt haben und dort hinein 
alle Bibliotheken aus der env.yaml installieren möchten, verwenden Sie folgenden Befehl:
conda env update -n 12_paketierung -f env.yaml
Damit wissen wir nun, wie wir komplette Umgebungen importieren und exportieren können.



8.2 Erstellen von Binaries 301
■ 8.2 Erstellen von Binaries
Tatsächlich gibt es diverse Frameworks in Python, die uns dabei unterstützen, aus unserem 
Code eine eigenständig lauffähige Anwendung zu machen, die sogar ohne ein zuvor instal liertes Python auskommt. Diese möchte ich Ihnen in Tabelle 8.1 vorstellen und damit meine 
Auswahl begründen.
Tabelle 8.1 Anwendungen und Methoden zum Paketieren bzw. Weitergeben von Python-Anwendungen
Vorteile Nachteile
Conda 
Constructor
 Abgeschlossenes Paket mit 
allen benötigten Dateien.
 Anaconda wird mitgeliefert.
 Nur das Environment wird paketiert, 
nicht die Anwendungslogik.
 Wenig Konfigurationsmöglichkeiten
PyInstaller  Erstellen einer Windows-EXE, 
die die Anwendungslogik be inhaltet.
 Vielzahl von Konfigurations möglichkeiten.
 Anaconda wird nicht benötigt.
 Unterstützt eine Vielzahl von 
Paketen.
 Bibliotheken müssen dafür konzipiert 
sein, um mit PyInstaller zu funktionie ren und benötigen einen sogenann ten Hook. Für einige unserer Abhän gigkeiten fehlt dieser.
Git Repository  Mit allen Systemen kompatibel.
 Updates der Anwendung sind 
leicht umzusetzen.
 Anaconda muss separat installiert 
und ein Environment eingerichtet 
werden. Bei der Installation wird eine 
Internetverbindung vorausgesetzt, 
um die Bibliotheken herunterzuladen.
cx_Freeze  Erzeugt Builds als Binary oder 
Installer.
 Anaconda wird nicht benötigt.
 Python-Interpreter wird als DLL 
mit ausgeliefert.
 Einige Bibliotheken sind nicht kom patibel (können aber nachgepflegt 
werden).
Vielleicht haben Sie es schon aus der Tabelle herausgelesen, dass die Entscheidung ledig lich auf cx_Freeze fallen kann. Was bedeutet, dass wir unser Ziel, einen Installer für eine 
autonom lauffähige Anwendung zu erzeugen, erreichen werden, aber dass dieser Prozess 
mit Arbeit verbunden ist.
Wir beginnen erst mal damit, dass wir drei Zeilen in unserem Anwendungsaufruf in der main.
py gemäß Listing 8.1 hinzufügen. Zeile 2 dient dazu, die Funktionsweise von multiprocessing
zu gewährleisten, auch wenn die Anwendung eingefroren wird.



302 8 Paketieren der Anwendung
TIPP: Das Einfrieren, oder freezing, bedeutet in Python, dass eine ausführbare 
Datei erzeugt wird, die sowohl Anwendungslogik als auch den Python-Interpreter 
beinhaltet, sodass ein Endbenutzer die Anwendung ohne vorherige Installation 
anderer Software ausführen kann.
Weiterhin wird von multiprocessing vorausgesetzt, dass wir die Ausgabekanäle definieren, 
konkret sys.stdout und sys.stderr, was wir auch gerne in den Zeilen 3 und 4 tun. Alle Aus gaben werden nun an die Dateien x.out und x.err angehängt. Das Anhängen spezifizieren 
wir mit dem Character a.
Listing 8.1 Änderungen an der main Punkt Pei, um das Einfrieren von Anwendungen zu unterstützen, 
um eine Windows-EXE zu erzeugen.
1. if __name__ == '__main__':
2. multiprocessing.freeze_support()
3. sys.stdout = open('x.out', 'a')
4. sys.stderr = open('x.err', 'a')
5. multiprocessing.set_start_method('spawn')
6. global_variables.voice_assistant = VoiceAssistant()
7. global_variables.voice_assistant.app.MainLoop()
Wir können die main Punkt Pei nun erst mal bis auf Weiteres schließen.
Schreiben der setup.py
Ein Setup Script, das wir im nächsten Schritt im Hauptverzeichnis unserer Anwendung 
anlegen, ist verantwortlich für das Zusammenstellen, den Build und die Installation eines 
Moduls oder einer Anwendung und kommt in der Regel unter dem Namen setup.py daher. 
Es beschreibt zu Beginn die Dateien, die mit dem Installer oder neben dem Executable aus geliefert werden sollen (Zeilen 5–8). Darunter sehen wir folgende Dateien und Ordner:
 Die Konfiguration der Anwendung als config.yml
 Den Sound empty.wav der Benachrichtigung
 Die Logos, die das Tray Icon annehmen kann
 Unsere Benutzerdatenbank users.json
 Das Icon va.ico zur Darstellung in den Toasts
 Die Modelle von VOSK für Sprach- und Sprechererkennung
 All unsere Intents, die wir bisher geschrieben haben, in intents/
Weiterhin geben wir einige Pakete in build_exe_options.packages an, die als Abhängig keit mitgeliefert werden müssen (wir sehen gleich, dass cx_Freeze viele, aber eben nicht 
alle, selbstständig erkennt) und die Plattform, für die die Anwendung gebaut werden soll 
(Zeile 19). Indem wir hier Win32GUI angeben, öffnet sich die Anwendung später nicht in 
einer Konsole. Bei der alternativen Spezifikation von base=None (als Kommentar in Zeile 20 
zu sehen) würde sich hingegen eine Konsole öffnen. In bdist_msi_options können wir einige 
Optionen für den Installer (Zeilen 22–27), der optional erzeugt werden kann, angeben und 
zu guter Letzt die Eigenschaften unserer Anwendung wie Name, Version, Beschreibung etc. 
in den Zeilen 29–36 von Listing 8.2.



8.2 Erstellen von Binaries 303
Listing 8.2 Die setup.py beschreibt die Distribution Ihres Codes
1. import sys
2. from cx_Freeze import setup, Executable
3. import os
4.
5. includefiles = ['config.yml', 'empty.wav', 'idle.png',
6. 'initializing.png', 'listening.png', 'speaking.png',
7. 'users.json', 'va.ico', 'vosk-model-de-0.6/',
8. 'vosk-model-spk-0.4/', 'intents/']
9.
10. build_exe_options = {
11. "packages": ["pyttsx3.drivers.sapi5",
12. "pip", "idna", "geocoder", "text2numde", "fuzzywuzzy",
13. "dateutil", "pyowm", "wikipedia", "pycountry", "pycrfsuite",
14. "pip._vendor.distlib", "pip._internal.commands.install",
15. "pykeepass", "pynput", "scipy"],
16. "excludes": [],
17. "include_files": includefiles}
18.
19. base = "Win32GUI"
20. #base = None
21.
22. bdist_msi_options = {
23. "install_icon":"va.ico",
24. "summary_data": {
25. "author": "Jonas Freiknecht"
26. }
27. }
28.
29. setup( name = "Sprachassistent",
30. version = "1.0",
31. description = "Mein erster Sprachassistent",
32. options = {
33. "build_exe": build_exe_options,
34. "bdist_msi": bdist_msi_options
35. },
36. executables = [Executable("main_12.py", base=base)])
Wir können die setup.py nun schließen.
Manuelles Bereitstellen von Dateien
Leider befinden sich nicht alle Dateien, die wir in unserer Distribution paketieren müssen, 
in unserem Anwendungsordner. Zwar sorgen cx_Freeze und die setup.py dafür, dass fast alle 
dort hineinkopiert werden, doch zwei Ausnahmen gibt es, bei denen wir selber tätig werden 
müssen.
 libsndfile_64bit.dll: Diese Datei finden Sie im Ordner des Anaconda-Environments, das sich 
bei der Standardinstallation unter Windows hier befindet: C:\Users\admin\Anaconda3. In 
der Regel ist es immer der Ordner mit Namen Anaconda3, egal ob unter Windows oder 
Linux. Darin wiederum sollte ein Ordner envs liegen, in dem Sie dann all Ihre Environments 
finden, darunter auch das aktuelle 12_paketierung. Bemühen Sie nun darin die Suche 



304 8 Paketieren der Anwendung
nach libsndfile_64bit.dll, finden Sie es im Unterordner Lib\site-packages\_soundfile_data\ 
libsndfile_64bit.dll. Kopieren Sie nun den Ordner _soundfile_data samt der DLL in das 
Hauptverzeichnis unserer Anwendung.
 ffmpeg_bin: Des Weiteren benötigen wir die Binaries von ffmpeg. Diese können wir 
relativ einfach von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichanaconda.org/conda-forge/ffmpeg/files herunterladen. Die 
korrekte Dateibezeichnung lautet bei Drucklegung dieses Buches win-64/ffmpeg-4.4.2-
gpl_h36dc936_107.tar.bz2. Entpacken Sie das Archiv (mit 7zip unter Windows muss es 
zweimal entpackt werden, erst das bz2, dann noch einmal die tar-Datei), holen Sie den 
Ordner Library/bin heraus und legen Sie alle Dateien aus diesem Ordner im Hauptver zeichnis unserer Anwendung ab.
Behandlung relativer Pfade bei eingefrorenen und nicht eingefrorenen 
Anwendungen
Nun folgt ein Abschnitt zu einem Thema, bei dem ich mir unsicher war, wie ich dessen 
Inhalt elegant unterbringe, um Ihnen möglichst viel Schreibarbeit zu ersparen. Es geht jetzt 
darum, dass wir die Dateipfade aller Dateien, die wir zur Laufzeit unseres Sprachassistenten 
einlesen, wie Grafiken, Audiodateien, Konfigurationsdateien und Intents, so anpassen, dass 
sie auch gefunden werden, wenn die Anwendung eingefroren ist. Das ist nämlich so nicht 
der Fall, was ärgerlich, aber zu beheben ist. Wir werden dazu die Datei constants.py um eine 
Funktion ergänzen, die Sie in Listing 8.3 sehen.
Listing 8.3 find_data_file() sorgt dafür, dass relative Pfade zur Laufzeit unabhängig vom Anwendungs zustand richtig aufgelöst werden.
1. import sys
2. import os
3.
4. # Source: Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichcx-freeze.readthedocs.io/en/latest/faq.html#using-data-files
5. def find_data_file(filename):
6. if getattr(sys, "frozen", False):
7. # Die Anwendung ist eingefroren
8. datadir = os.path.dirname(sys.executable)
9. else:
10. # Die Anwendung ist nicht eingefroren
11. datadir = os.path.dirname(__file__)
12. return os.path.join(datadir, filename)
13. TRAY_TOOLTIP = 'Voice Assistant'
14. TRAY_ICON_INITIALIZING = find_data_file('initializing.png')
15. TRAY_ICON_IDLE = find_data_file('idle.png')
16. TRAY_ICON_LISTENING = find_data_file('listening.png')
17. TRAY_ICON_SPEAKING = find_data_file('speaking.png')
Diese Funktion ist, wie Sie dem Kommentar in der vierten Zeile entnehmen können, kein 
Konstrukt, das exklusiv unserer Anwendung zugutekommt, sondern eine Empfehlung der 
Dokumentation von cx_Freeze2
. Darin wird in Zeile 6 geprüft, ob die Anwendung eingefroren 
ist. Ist das der Fall, wird die Variable datadir auf den Pfad gesetzt, in dem sich unser Executable
2 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichcx-freeze.readthedocs.io/en/latest/faq.html#using-data-files



8.2 Erstellen von Binaries 305
befindet. Ist die Anwendung hingegen nicht eingefroren, zeigt datadir auf den Namen unserer 
aufgerufenen Python-Datei. Die Rückgabe in Zeile 12 wird dann aus dem eben ermittelten, 
absoluten Pfad und dem eigentlich referenzierten Dateinamen per join() zusammengestellt.
Die Funktion find_data_file() findet dann auch sogleich in den Zeilen 13–17 Anwendung, 
da wir darüber aus dem bisher relativen Pfad in TRAY_ICON_IDLE einen absoluten Pfad er zeugen, sodass idle.png in jedem Fall gefunden wird, egal von wo die Logik der Anwendung 
aufgerufen wird.
Und nun beginnt der Spaß, der eigentlich gar keiner ist, sondern etwas eintönige Arbeit: 
Wir müssen überall dort in unserem Code, wo wir Dateien über relative Pfade referenzieren, 
find_data_file() mit dem jeweiligen Dateinamen als Parameter aufrufen. Statt nun alle 
Änderungen in eigenen Listings aufzuführen, soll es genügen, auf die einzelnen Stellen in 
unserem Code zu verweisen. Diese Stellen werde ich in den folgenden Tabellen, beginnend mit 
Tabelle 8.2, aufführen. Achten Sie darauf, dass ich jeder Datei, in der wir find_data_file()
verwenden, den Import from constants import find_data_file hinzufüge.
Tabelle 8.2 Anpassung relativer Pfade in der main Punkt Pei
Relativer Pfad Absoluter Pfad
CONFIG_FILE = 'config.yml' CONFIG_FILE = 
find_data_file('config.yml')
stt_model = Model
('./vosk-model-de-0.6')
stt_model = Model
(find_data_file('vosk-model-de-0.6'))
speaker_model = SpkModel
('./vosk-model-spk-0.4')
speaker_model = SpkModel
(find_data_file('vosk-model-spk-0.4'))
Es folgt die Anpassung der Intents, die ich exemplarisch in Tabelle 8.3 für intent_animalsound.
py zeige. Denken Sie an den oben angesprochenen Import.
Tabelle 8.3 Anpassung relativer Pfade in intent_animalsound.py
Relativer Pfad Absoluter Pfad
config_path = os.path.join
('intents','functions',
'animalsounds',
'config_animalsounds.yml')
config_path = find_data_file
(os.path.join('intents','functions',
'animalsounds',
'config_animalsounds.yml'))
ogg_path = os.path.join
('intents','functions',
'animalsounds','animals')
ogg_path = find_data_file
(os.path.join('intents','functions',
'animalsounds','animals'))
Auch hier suchen wir alle relativen Pfade heraus und betten Sie in unsere Funktion ein. Das 
gilt auch für ogg_path, den wir erst später mit der eigentlichen Audiodatei konkatenieren.
Diese Anpassungen sind nun in folgenden Dateien vorzunehmen:
 intent_gettime.py – Pfad der Konfigurationsdatei
 intent_musicstream.py – Pfad der Konfigurationsdatei



306 8 Paketieren der Anwendung
 intent_password.py – Pfad der Konfigurationsdatei, der Passwortdatenbank und der Schlüs seldatei. Achten Sie hier darauf, dass Sie die Änderungen sowohl in getPassword() als 
auch in getUsername() vornehmen.
 intent_questiongame.py – Pfad der Konfigurationsdatei, der Items- und der Questions-Datei
 intent_reminder.py – Pfad der Konfigurationsdatei
 intent_smarthome.py – Pfad der Konfigurationsdatei
 intent_stop.py – Pfad der Konfigurationsdatei
 intent_volume.py – Pfad der Konfigurationsdatei
 intent_weather.py – Pfad der Konfigurationsdatei
 intent_wiki.py – Pfad der Konfigurationsdatei
Die letzte Datei, die es nun anzupassen gilt, ist unsere Intent-Verwaltung in der intentmgmt.
py. Die erforderlichen Änderungen zeige ich in Tabelle 8.4. Hier finden wir die Besonder heit vor, dass wir find_data_file() in die Wildcard-Suche von glob.glob() einbinden 
müssen. Aber auch das ist schnell gelöst. Wir lagern den Wildcard-Teil einfach aus, sodass 
find_data_file() nicht damit arbeiten muss, und hängen ihn am Ende einfach wieder an 
den Suchpfad an.
Tabelle 8.4 Anpassungen der relativen Pfade in intentmgmt.py
Relativer Pfad Absoluter Pfad
self.functions_folders = 
[os.path.abspath(name) 
for name in glob.glob
('./intents/functions/*/')]
self.functions_folders = 
[os.path.abspath(name) 
for name in glob.glob(find_data_file
("intents/functions") + '/*/')]
snips_files = glob.glob
(os.path.join('./intents/
snips-nlu', '*.yaml'))
snips_files = glob.glob
(find_data_file(os.path.join
('intents/snips-nlu', '*.yaml')))
chatbotai_files = glob.glob
(os.path.join('./intents/
chatbotai', '*.template'))
chatbotai_files = glob.glob
(os.path.join(find_data_file
('intents/chatbotai'), '*.template'))
WILDCARD_FILE = './intents/
chatbotai/wildcard.template'
WILDCARD_FILE = find_data_file
('intents/chatbotai/wildcard.template')
MERGED_FILE = './intents/
chatbotai/_merger.template'
MERGED_FILE = find_data_file
('intents/chatbotai/_merger.template')
Sie sind erlöst, das war der letzte Handgriff, um unsere Anwendung zu befähigen, Dateien 
im eingefrorenen Zustand zu finden. Kümmern wir uns nun endlich darum, aus unserem 
Python-Code einen Executable zu erzeugen.
Ausführen des Builds
Da wir in Listing 8.2 bereits das setup.py formuliert haben, bleibt nichts anderes zu tun, als 
bei aktiviertem Environment cx_Freeze in Version 6.9 per pip zu installieren und im Haupt 


8.2 Erstellen von Binaries 307
verzeichnis unserer Anwendung python setup.py build auszuführen. Treten in diesem Prozess 
Fehler auf, können Sie davon ausgehen, dass diese mit einer nahezu hundertprozentigen 
Wahrscheinlichkeit in der Definition des Builds in der setup.py zu beheben sind. Ist der 
Prozess erfolgreich durchlaufen, was eine Weile dauern kann, sehen Sie im Verzeichnis der 
Anwendung einen Ordner build. Darin finden Sie wiederum zwei Ordner:
 bdist.win-amd64
 exe.win-amd64-3.8
In letzteren, der ja schon durch seinen Namen andeutet, dass sich darin unsere Anwendung 
befindet, navigieren wir nun.
HINWEIS: Der Name des Ordners deutet nicht nur darauf hin, dass sich darin die 
Executable befindet, sondern ebenfalls, dass diese für die x64-Architektur kompi liert und dass der Python-Interpreter 3.8 in die Anwendung eingebettet wurde.
In diesem Ordner finden Sie nun die main.exe, die unsere eigentliche Anwendung darstellt, 
nur eben nicht mehr als py-Datei, sondern als eine von Windows ausführbare EXE. Schauen 
wir uns den Ordner weiter an, finden wir die Datei python38.dll, die, einmal mitgeliefert, 
dafür sorgt, dass unsere Benutzer nicht extra einen Python-Interpreter installieren müssen. 
Weiterhin sehen wir alle Ordner und Dateien, die wir über includefiles in der setup.py definiert 
haben. Diese kopiert das Setup-Skript automatisch in unseren Build-Ordner.
Bevor unsere Anwendung nun lauffähig ist, müssen wir noch alle Dateien aus dem Ordner 
ffmpeg_bin aus dem Hauptverzeichnis unserer Anwendung in das Verzeichnis exe.win amd64-3.8 kopieren. Man könnte sie auch in der Liste includefiles spezifizieren, da sich die 
Dateinamen jedoch über den Entwicklungszeitraum des Sprachassistenten ein paarmal ge-
ändert hatten, bin ich dazu übergegangen, diesen Schritt manuell vorzunehmen.
Weiterhin müssen wir den gesamten Ordner _soundfile_data aus unserem Hauptverzeich nis mit in den Build-Ordner kopieren, ganz konkret nach exe.win-amd64-3.8/lib. Wenn wir 
schon im Ordner lib sind, können wir auch gleich einen Blick darauf werfen, welche Ordner 
cx_Freeze hier für uns anlegt und mit welchen Dateien es sie versieht. Tatsächlich finden 
wir darin für jede unserer Abhängigkeiten einen Ordner, etwa für Loguru, die deren Logik 
in kompilierter Form als pyc-Dateien enthalten.
HINWEIS: Häufig wird davon ausgegangen, dass Python eine Interpretersprache
ist, was zu einem gewissen Grad auch stimmt. Allerdings widerspricht dieser Aus sage die Tatsache, dass der Python-Interpreter Code zu Bytecode kompiliert, der 
dann wiederum von der Python Virtual Machine ausgeführt wird. Dieser Vorgang 
spräche eigentlich dafür, dass Python eine Compilersprach ist. Pythons Dokumen tation liefert eine Erklärung und begründet, dass Python eine Interpretersprache 
ist, damit, dass ein Quelltext direkt ausgeführt werden kann, ohne dass zuvor eine 
Executable daraus erzeugt werden muss3
.
3 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichdocs.python.org/3/glossary.html#term-interpreted



308 8 Paketieren der Anwendung
Haben wir den letzten manuellen Kopiervorgang abgeschlossen, steht dem Starten der main.
exe nichts mehr im Weg. Klicken Sie sie doppelt an und beobachten Sie, wie unser Sprach assistent hochfährt. Es sollte nun alles eigenständig funktionieren, sodass Sie diesen auch 
auf einen PC kopieren können, der nicht über unsere Entwicklungswerkzeuge verfügt.
Entfernen der requirements.txt
Wenn Sie die Anwendung ausführen und einen Blick in die log.txt werfen, werden Sie fol genden Fehler beim dynamischen Installieren der Requirements der einzelnen Intents zu 
Gesicht bekommen.
pip._vendor.distlib.DistlibException: Unable to locate finder for 'pip._vendor.distlib'
Dieser rührt daher, dass das Nachinstallieren von Paketen bei einer eingefrorenen Anwendung 
nicht vorgesehen ist. Da wir diese aber explizit implementiert haben, um zur Laufzeit belie bige Intents hinzufügen zu können, müssen wir sie nun nach dem Einfrieren deaktivieren. 
Das tun wir, indem wir die requirements.txt, die unseren Intents beiliegen, umbenennen 
oder löschen. Da wir alle Abhängigkeiten bereits in den Build haben einfließen lassen und 
diese in Form von Ordnern in unserem Ausgabeordner vorliegen, hat das keine negativen 
Auswirkungen auf die Funktionsweise unserer Anwendung. Führen Sie im Ordner exe.win amd64-3.8 eine Suche nach requirements.txt aus und löschen Sie alle gefundenen Dateien, 
sodass unser Intent Management diese nicht mehr findet und versucht, die darin beinhalteten 
Bibliotheken zu installieren.
TIPP: Sie sehen, dass wir hier an einem Scheideweg stehen: Halten wir unsere 
Anwendung als Code vor, nicht eingefroren und nicht durch einen Build in ein 
Executable gewandelt, dann können wir dynamische Erweiterungen einfach per 
Copy-and-paste eines neuen Intent-Ordners in unseren Sprachassistenten aufneh men. Allerdings zu dem Preis, dass der Assistent dann über einen komplizierteren 
Prozess installiert werden muss, der das Aufspielen eines Python-Interpreters, 
der Anlage eines Environments und der Installation aller initialen Requirements 
beinhaltet. Frieren wir unsere Anwendung ein, bleibt uns das erspart, es nimmt 
uns allerdings die Option, Intents nachzuinstallieren; beziehungsweise jeder neue 
Intent erfordert, dass ein neuer Build ausgeführt wird.
Am Ende dieses Kapitels haben wir nun einen Build-Ordner, den wir zippen und weiter geben können. Wäre es jedoch nicht schöner, wenn der Sprachassistent in einem Installer 
daherkäme?



8.3 Erstellen eines Installers 309
■ 8.3 Erstellen eines Installers
Nichts leichter als das, denn wir haben in Listing 8.2 in den Zeilen 22–27 bereits alles dafür 
vorbereitet, indem wir das Dictionary bdist_msi_options anlegen, welches lediglich einen 
Eintrag für das Icon enthält, das im Installationsdialog gezeigt wird, sowie den Autoren der 
Anwendung in summary_data. Die Dokumentation von cx_Freeze4
 ist hinsichtlich weiterer 
Optionen sehr ergiebig und lässt Sie den Installationsvorgang sehr dynamisch gestalten. 
Besagtes Dictionary haben wir bereits in Zeile 34 von Listing 8.2 referenziert, sodass die 
setup.py keiner weiteren Anpassung bedarf.
Um den Installer letztendlich zu generieren, führen Sie im Hauptverzeichnis der Anwendung 
python setup.py bdist_msi --skip-build aus. Da dieser Vorgang durch das Argument --skip-build
auf dem Ordner build/exe.win-amd64-3.8 aufsetzt, ist es wichtig, dass Sie alle oben be schriebenen Schritte durchgeführt haben. Hier nochmals alle notwendigen Maßnahmen im 
Schnelldurchlauf:
 Build über python setup.py build ausführen
 Ordner _soundfile_data in build/exe.win-amd64-3.8/lib kopieren
 Alle Dateien aus ffmpeg_bin nach build/exe.win-amd64-3.8 kopieren
 Löschen aller Dateien namens requirements.txt aus build/exe.win-amd64-3.8
Ist die Erzeugung des Installers erfolgreich verlaufen, finden Sie im Hauptverzeichnis der 
Anwendung einen Ordner dist, der die Datei Sprachassistent-1.0-amd64.msi beinhaltet. Er schrecken Sie nicht ob der Größe der Datei, diese enthält alle nötigen Daten (auch die Sprach modelle), die wir in der setup.py referenziert haben.
TIPP: Nebem bdist_msi existieren auch die Befehle bdist_rpm, um RPM-Pakete 
für die gängigen Linux-Distributionen wie Red Hat oder SuSE zu erzeugen; oder 
bdist_mac, um ein Mac Application Bundle erstellen zu lassen.
Starten Sie den Installer per Doppelklick, führt Sie ein eleganter Wizard durch die Installation. 
Nach Fertigstellung ist unsere Anwendung in AppData\Local\Programs\Sprachassistent zu 
finden. Ein Klick auf die main.exe führt wie gehabt zum Start.
4 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichcx-freeze.readthedocs.io/en/latest/setup_script.html#bdist-msi






9 Abschließende Worte
Nun sind wir am Ende angelangt und ich freue mich, dass Sie mich bei der Reise durch eine 
Themenvielfalt begleitet haben, die ich mir vorher, ehrlich gesagt, gar nicht erhofft hatte. 
Vielleicht war der ein oder andere Ausflug für den ein oder anderen sehr exotisch (ich denke 
hier an Quantum Machine Learning in Abschnitt 6.5.4), jedoch bevorzuge ich es, wenn man 
nicht nur über Themen spricht, sondern sie auch praktisch erforscht und selbst für sich 
erschließt, statt dem zu glauben, was das Internet vollmundig verspricht. Ein vollmundiges 
Versprechen habe ich Ihnen auch am Anfang des Buches gegeben, nämlich, dass wir unseren 
eigenen Sprachassistenten entwickeln, der losgelöst von datensammelwütigen Cloud-Diens ten funktioniert – und ich denke, das ist uns auch gelungen. Wir haben immerhin etwas in 
der Hand, das wir weitergeben können, etwas Eigenes, das wir vom ersten Funktionsaufruf 
verstehen und das wir nun nach Belieben erweitern können. Im Optimalfall konnte ich Ihnen 
helfen ihre Entwicklerfähigkeiten auszubauen und zu festigen und Ihr Interesse für das 
maschinelle Lernen zu wecken, das wir ja mit unseren Sprachmodellen oder der Vorhersage 
von Zeitreihendaten bis in tiefste Tiefen behandelt haben.
Sollten Sie jetzt noch nicht geistig vollkommen ausgelaugt vor Ihrer IDE sitzen, dann haben 
wir bereits im vorherigen Kapitel noch einige Erweiterungen gesehen, die für zusätzliche 
Intents in Frage kommen. Ich würde mir sehr wünschen, dass sich um unseren Assistenten 
eine Community aufbaut, die gegenseitig Funktionen zu Verfügung stellt und den Assistenten 
nahe an das heranführt, was einen menschlichen Gesprächspartner ausmacht.
Einige Themen haben es leider aber aufgrund der zur Verfügung stehenden Zeit nicht mehr 
ins Buch geschafft. Ich verspreche aber, dass ich diese, sollte es zu einer weiteren Auflage 
des Buches kommen, nachreichen werde. Dazu gehören:
 Kompilieren des Sprachassistenten für den Raspberry Pi
 Mehrsprachigkeit
 Automatisierte Tests für Framework und Intents
 Einbinden von Modellen wie DialoGPT für einfachen Smalltalk
 Einbinden von REST-Service-Intents, deren Logik auf einem Fremdsystem läuft
 Konzeption einer einfachen UI ähnlich Amazon Echo, die etwa die Gefühle unseres Assis tenten zum Ausdruck bringen kann oder proaktiv Informationen zeigt.
Sollte Sie eines der genannten Themen ansprechen, können Sie natürlich auch direkt mit 
der Umsetzung beginnen. Wenn Sie wie ich während der Entwicklung unseres Assistenten 
immer wieder gedanklich zu Amazon Alexa, Google Home oder Apples Siri abgedriftet sind, 
können wir nun aber am Ende des Buches behaupten, dass wir etwas erreicht haben, was den 
Marktgrößen nahekommt. Wir können mit einer Maschine per Sprache kommunizieren und 



312 9 Abschließende Worte
sie für uns einfache Aufgaben erledigen lassen. Etwas, das noch vor einigen Jahren undenk bar gewesen wäre und was wir erst durch unsere Fähigkeiten, uns KI nutzbar zu machen, 
erreichen konnten. Darauf können wir zurecht stolz sein.
■ 9.1 Wohin mit meinen neuen Fähigkeiten?
Wir kommen in diesem Buch mit vielen verschiedenen Tätigkeitsfeldern in Berührung, allen 
voran aber sicherlich den drei folgenden:
 Software Engineering,
 Data Engineering,
 Data Science.
Die Fähigkeiten aus der Domäne Software Engineering befähigen uns, professionelle Anwen dungen zu entwickeln, zu testen und auszurollen. Sie kommen überall dort zum Tragen, wo 
wir das Framework unseres Sprachassistenten entwickelt haben, Module aussuchen, die wir 
für geeignet halten, in unsere Anwendung eingebunden zu werden, Redundanzen vermeiden, 
Fehlern vorbeugen und den Code so lesbar gestalten, dass er auch von anderen verstanden 
und angepasst werden kann. Das Berufsfeld der Software Engineers ist in der Informatik 
sicher eines der gefragtesten und sicher auch mit das verantwortungsvollste. Denn neben 
dem Coding müssen Sie, wenn Sie nicht gerade wie in diesem Buch beschrieben ein Projekt 
selber stemmen, mit Entwicklern, Projektleiten, Scrum Mastern, Product Ownern und auch 
Kunden und dem Vertrieb kommunizieren. Sie müssen Anforderungen abstimmen, die Ihre 
Anwendung erfüllen soll, oder Feedback zu Ihrem Produkt einholen. Die Message, die ich 
hier rüberbringen möchte, ist: Keine erfolgreiche Softwareentwicklung ohne Kommunikation.
Ein weiterer, wesentlicher Faktor sind die zu beherrschenden Programmiersprachen. Hier 
ist die Auswahl jedoch so groß, dass es kaum Sinn ergibt, alle aufzuzählen und hinsichtlich 
ihrer Vor- und Nachteile zu besprechen. Wir haben in Kapitel 2 bereits die bevorzugten 
Programmiersprachen von Data Scientists kennengelernt. Einen guten Überblick über die 
populärsten, allgemeinen Sprachen verschafft der TIOBE Index. Das Ranking ergibt sich 
aus den gefundenen aufsummierten Treffern in bekannten Suchmaschinen (siehe Bild 9.1).
Wir haben mit Python eine gute Wahl getroffen, ist es doch die am häufigsten gesuchte 
Sprache im August 2022. Eine von der bekannten Webseite stackoverflow.com durchge führte Umfrage1
 unter allen Entwicklern lässt Python hingegen erst auf Platz 3 erscheinen, 
nach JavaScript auf Platz 1 und HTML/CSS auf Platz 2. Welche Sprachen Sie lernen sollten, 
ist jedoch abhängig von dem Geschäftsfeld und der Technologie, mit der Sie gerne arbeiten 
möchten. Entwickeln Sie bevorzugt komplexe Desktopanwendungen, wird es wohl Richtung 
C#, React und Electron oder C++ gehen. Für Backendanwendungen, womöglich im Banken oder Versicherungswesen, werden Sie häufig auf Java treffen, und möchten Sie mobile 
Apps entwickeln, dann ist Kotlin, Swift, Objective-C oder React für hybride oder Progressive 
Web Apps die richtige Wahl. Für Beginner ist es aus meiner Erfahrung immer ratsam, mit 
1 Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichinsights.stackoverflow.com/survey/2021#most-popular-technologies-language



9.1 Wohin mit meinen neuen Fähigkeiten? 313
einer verbreiteteren Hochsprache zu beginnen, also einer Sprache, die so weit weg von der 
Maschine agiert, wie möglich. Hat man die Prinzipien der Entwicklung erst mal verstanden, 
ist die nächste Sprache nicht mehr so schwer zu erlernen.
Python C Java C++ C#
Visual Basic JavaScript Assembly SQL PHP
Swi Classic Visual Basic Delphi/Pascal Objecve-C Go
R MATLAB Ruby Fortran Perl
Bild 9.1 Der TIOBE Index zeigt die Popularität der bekanntesten Programmiersprachen 
(Verteilung in %; Auswertung von August 2022, Quelle: www.tiobe.com).
Neben dem Software Engineering haben wir uns auch im Bereich des Data Engineering 
weitergebildet, wenn auch nur am Rande. Der Data Engineer ist für die Datenbereitstellung 
in Unternehmen verantwortlich und sorgt u. a. für deren Eingang, Bereinigung und Trans formation. In den letzten Jahren vollzog sich auf diesem Berufsfeld ein starker Wandel. Was 
früher vorrangig in relationalen Datenbanken und wenigen, ausgewählten ETL-Werkzeugen 
geschah, ist heute zu einem eigenständigen Berufsfeld geworden. Der Grund dafür ist die 
Erkenntnis, dass der Informationsgewinn aus den Daten, die wir bereits haben, wesentlich 
höher sein kann als er häufig ist. Entweder durch modernere Auswertungsmethoden oder 
durch die Verknüpfung verschiedener Datenquellen, etwa unseren Wetterdaten und den Ver kaufszahlen von Regenschirmen. Durch Betrachtung beider können wir sicherlich viel genauer 
vorhersagen, wie viele Regenschirme wann verkauft werden. Diese Zusammenschlüsse zu 
ermöglichen, obliegt dem Data Engineering. Häufig müssen dabei die Daten bereinigt werden, 
also Ausreißer gefunden, Informationslücken interpoliert und Formatierungen korrigiert 
werden. Dann erfolgt die Transformation, in der verschiedene Skalen angepasst werden, 
so wie wir es beim zeitlichen Verlauf unserer Wetterdaten gemacht haben und letztendlich 
werden die Daten bereitgestellt und aktuell gehalten, etwa indem sogenannte Delta Loads
neue Daten den alten Daten hinzufügen.
Die dabei entstehenden ETL-Strecken (Extract, Transform, Load) sind häufig in einer der 
gängigen Hochsprachen programmiert und machen exzessiv Gebrauch von Frameworks 
wie etwa Apache Spark, die fertige Funktionen für Batch- oder Datenstromverarbeitung bie ten. Einmal erstellt, stellt der Data Engineer diese Strecken in ein Orchestrationswerkzeug 
ein, etwa Apache Airflow, und sorgt dafür, dass diese regelmäßig fehlerfrei laufen, dass die 
Datenqualität überwacht werden kann und dass sie anderen Abteilungen oder Personen zu 
Verfügung gestellt werden können. Für die Auflistung der Datenquellen dient häufig ein 



314 9 Abschließende Worte
Data Catalog, für die Ablage hat sich der Begriff Data Lake etabliert. In letzterem stellen die 
Entwickler häufig auch high-level APIs für den Datenzugriff bereit, damit sich domänen fremde Kollegen nicht mit den häufig recht kryptischen Daten auseinandersetzen müssen. 
Unseren Wetterdaten würde ein Data Engineer ein Python-Modul beilegen, das Funktionen 
wie get_weather_between(startDate, endDate) enthält.
HINWEIS: Seit einiger Zeit propagieren einige Unternehmen den Begriff der 
Data Factory als Nachfolger sowie als Ergänzung zum Data Lake. Data-Factory Produkte seien in der Lage, die Datenqualität der gespeicherten Daten durch 
Datenintegrationskomponenten zu sichern, damit der Lake nicht zum Data Swamp
(Datensumpf) verkommt. Für mich persönlich ist die Data Factory eher ein 
Marketingbegriff, da die Datenintegration bereits zum Data Lake gehört. Für die 
Datenpflege sind sowohl im Lake als auch in der Factory die Data Stewards und 
die Data Engineers verantwortlich.
Seit einigen Jahren gibt es nun das Berufsfeld des Data Scientist, dessen Arbeit häufig auf die 
des Data Engineers aufsetzt. Sind die Daten nämlich erst mal bereitgestellt, ist die Aufgaben stellung, mit allen verfügbaren Methoden neue Informationen daraus zu ziehen. Dazu gehört 
es etwa, Muster zu finden, zum Beispiel dass im Sommer viele Klimaanlagen verkauft werden und 
im Winter nicht. Oder aber auch das Treffen von Vorhersagen auf Basis bekannter Daten, die 
dem Unternehmen oder den Kunden mehr Planungssicherheit verschaffen. Nicht selten sind 
Data Scientists in einer Domäne zuhause, sie kennen sich entweder im Bereich der Medizin, 
der Pharmazeutik, der Verteidigung oder dem Einzelhandel so gut aus, dass sie die Prozesse 
und Daten dahinter verstehen und wissen, wie sie diese zu interpretieren haben. Der zweite 
Teil ist häufig recht einfacher statistischer Natur, wie wir es meiner Meinung nach zur Ge nüge gesehen haben. Erfahrungsgemäß kann ein guter Projektleiter oder Product Owner das 
fehlende Domänenwissen substituieren, sodass Data Scientists nicht immer in einer Domäne 
zuhause sein müssen. Die Erkenntnisse der geleisteten Arbeit führen häufig zu Handlungs empfehlungen, die der Unternehmensleitung oder den Kunden vorgestellt werden – was 
sicherlich eine der schwersten Aufgaben ist, denn seine Erkenntnis zu begründen ist eine 
Herausforderung, weiß der CEO doch nicht immer unbedingt, wie ARIMA funktioniert. Daher 
ist es weiterhin unabdingbar, die Ergebnisse visuell aufzubereiten, eine Fähigkeit, die man 
aber fast immer zusammen mit der Datenanalyse erlernt. Seitdem es seit etwa 2014 Jupyter 
Notebooks des Project Jupyter gibt, haben wir Werkzeuge zur Hand, mit denen wir nicht nur 
unsere Analysen besser erklären, sondern diese auch visuell darstellen können.
Wenn Sie mit dem Gedanken spielen, sich in einem der drei Berufsfelder umzutun, sind Sie 
am Arbeitsmarkt sicher noch viele Jahre herzlich willkommen, denn die Zahl der offenen 
Stellen auf den gängigen Plattformen ist bald nicht mehr zählbar. Ich hoffe jedenfalls, dass 
ich Ihnen in diesem Fall mit dem vorliegenden Buch etwas Wissen mitgeben konnte, um 
sich weiterzubilden. Falls Sie nicht mit dem Gedanken spielen, in die IT zu wechseln, dann 
ist meine Hoffnung, dass ich Sie wenigstens ein wenig unterhalten konnte.



9.2 Danksagungen 315
■ 9.2 Danksagungen
Es ist doch recht zeitintensiv, ein Buch zu schreiben, auch wenn man meint, dass man be reits das dafür vorgesehene Material gedanklich geordnet vorliegen hat. Es passieren immer 
unvorhergesehene Dinge, etwa, dass man ein oder zwei Monate doch mal keine Motivation 
aufbringen kann, um weiterzumachen, da einen das Thema gerade frustriert oder das Kapitel 
nicht wirklich rund werden möchte. Umso wichtiger ist es, dass man bei allen anderen Auf gaben des täglichen Lebens Unterstützung bekommt. Da das Schreiben meines dritten Buches 
neben meinem Dasein als Familienvater doch irgendwie in den Tagesablauf gepasst hat, muss 
meine Frau Sarah wohl einen fantastischen Job machen. Dafür möchte ich mich bedanken.
Ebenso bedanke ich mich bei Sylvia Hasselbach vom Hanser Verlag für die freundliche Be treuung und entschuldige mich für das maximale Ausreizen der Abgabefrist. Dass es nicht 
wieder vorkommen wird, habe ich schon beim letzten Buch gesagt, also lasse ich es diesmal.






Literatur
(Amritendu, 2020) Amritendu, R. (12. Juli 2020). A perspective on the 
history of Artificial Intelligence (AI). 
Von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichmedium.com/@amritenduroy/a-perspective on-the-history-of-artificial-intelligence-ai-13fbb5a53ec7
abgerufen
(Chuvpilo, 2020) Chuvpilo, G. (20. Dezember 2020). AI Research Rankings 
2020: Can the United States Stay Ahead of China? 
Von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichchuvpilo.medium.com/ai-research rankings-2020-can-the-united-states-stay-ahead-of-china-
61cf14b1216 abgerufen
(Devlin, Chang, Lee, & Toutanova, 2018) Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). 
BERT: Pre-training of Deep Bidirectional Transformers 
for Language Understanding. arXiv.
(Jia, et al., 2018) Jia, Y., Zhang, Y., Weiss, R. J., Wang, Q., Shen, J., 
Ren, F., et al. (2018). Transfer Learning from Speaker 
Verification to Multispeaker Text-To-Speech Synthesis. 
Advances in Neural Information Processing Systems 31, 
S. 4485–4495.
(Kaggle, 2021) Kaggle. (2021). 2020 Kaggle Machine Learning & Data 
Science Survey.
(Kelly & Statt, 2019) Kelly, M., & Statt , N. (3. Juli 2019). 
Von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichwww.theverge.com/2019/7/3/20681423/
amazon-alexa-echo-chris-coons-data-transcripts-recording privacy abgerufen
(Kong, Wang, & Nichol, 2021) Kong, X., Wang, G., & Nichol, A. (2021). Conversational 
AI with Rasa. Packt Publishing Limited.
(Marsden, 2017) Marsden, P. (21. August 2017). Artificial Intelligence 
Timeline Infographic – From Eliza to Tay and beyond. 
Von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichdigitalwellbeing.org/artificial-intelligence timeline-infographic-from-eliza-to-tay-and-beyond/
abgerufen
(Rajpurkar, Zhang, Lopyrev, & Liang, 2016) Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. 
(16. Juni 2016). SQuAD: 100,000+ Questions for 
Machine Comprehension of Text. Proceedings of the 
2016 Conference on Empirical Methods in Natural 
Language Processing (EMNLP).
(Rohrer, 2021) Rohrer, B. (29. Oktober 2021). Transformers from Scratch. 
Von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstriche2eml.school/transformers.html abgerufen



318 Literatur
(Roosendaal, 2018) Roosendaal, T. (4. Juni 2018). Ton Roosendaal Twitter. 
Von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichtwitter.com/tonroosendaal/
status/1003590417848455168 abgerufen
(Schuchmann, 2019) Schuchmann, S. (12. Mai 2019). History of the first AI 
Winter. 
Von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichtowardsdatascience.com/history-of-the-first-ai winter-6f8c2186f80b abgerufen
(Schuster & Paliwal, 1997) Schuster, M., & Paliwal, K. (1997). Bidirectional 
recurrent neural networks. IEEE Transactions on Signal 
Processing, pp. 2673–2681.
(Statista, 2020) Statista. (Mai 2020). Statista. Von Smart speaker use 
case frequency in the United States as of January 2020: 
Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichwww.statista.com/statistics/994696/united-states smart-speaker-use-case-frequency/ abgerufen
(Tan, Qin, Soong, & Liu, 2021) Tan, X., Qin, T., Soong, F., & Liu, T.-Y. (2021). 
A survey on neural speech synthesis. arXiv preprint 
arXiv:2106.15561. 
Von arXiv preprint arXiv:2106.15561 abgerufen
(Tunstall, von Werra, & Wolf, 2022) Tunstall, L., von Werra, L., & Wolf, T. (2022). 
Natural Language Processing with Transformers. 
Sebastopol: O’Reilly.
(Vaswani, et al., 2017) Vaswani, A., Shazeer, N., Parmer, N., Uszkoreit, J., 
Jones, L., Gomez, A. N., et al. (2017). Attention Is All 
You Need.
(Watzlawik, Beavin & Don, 2000) Watzlawik, P., Beavin, H. J., & Don, D. J. (2000). 
Menschliche Kommunikation, Formen, Störungen, 
Paradoxien. Verlag Hans Huber.
(Wikipedia, 2021) Wikipedia. (06. Juni 2021). 
Von Hah Tee Tee Peh Es Doppelpunkt Doppel-Schrägstrichde.wikipedia.org/wiki/Spracherkennung
abgerufen
(Zen, 2018) Zen, H. (2018). [Invited] Generative Model-Based Text-to Speech Synthesis. 2018 IEEE 7th Global Conference on 
Consumer Electronics (GCCE), S. 327–328.



Stichwortverzeichnis
A
ACF 255
Activation Function 181
Adam 263
Administrator 119
Aggregationsfunktion 241
AIC 255
Akaike’s Information Criterion siehe AIC
Alignments 53
Anaconda 13, 21
– Environment anlegen 21
animalsound siehe Tierstimmen
Apache Airflow 313
Apache Spark 313
API Gateway 137, 140
ARIMA 243, 252
ARMA 248
Attention 37, 42
Attention Filter 177
Attention Head 172
Attention Mask 159
Audacity 60
Augmented Dickey-Fuller Test 249
Autocorrelation Function siehe ACF
Autoregression 243
Autoregressiver gleitender Mittelwert 
siehe ARMA
B
Back Propagation 51
Ballon Tip 295
Barack Obama 149
Batch 158
Bayesian Information Criterion siehe BIC
bdist_msi_options 302, 309
Benutzerverwaltung 90
BERT 154
bertviz 43
Bias 155
BIC 255
Bidirectional Encoder Representations from 
Transformers siehe BERT
Bidirectional Recurrent Neural Network 
siehe BRNN
Bildunterschrift 40
Bra-Ket Notation siehe Dirac Notation
BRNN 262
Build 306
C
C# 312
Callback 185, 193, 206
cased 157
Channel (Audio) 207
Character Vector 156
Chatbot 40, 67
ChatbotAI 108, 130, 147, 200, 225
Chatbot-Framework 108
ChatterBot 108
Checkpoint 162
CIRQ 217
Classification 43
clearSigInt 291
[CLS] 159 siehe auch Classification
CNN 157
CNOT Gatter 216
Computer Vision 178
Compute Unified Device Architecture 46
Conda Constructor 301
Confidence Score 152
Configuration Management 20
Confusion Matrix 100
Convergence 52
Convolutional Neural Network siehe CNN
coqui.ai 35
CUDA 151 siehe auch Compute Unified 
Device Architecture
cx_Freeze 301



320 Stichwortverzeichnis
D
Data Catalog 314
Data Collator 159
Data Engineering 37
Data Factory 314
DataFrame 240
Data Lake 314
Data Scientist 314
Data Steward 314
Data Swamp 314
Datenschutz 4
datetime.now() 125
Decoder 41, 168, 182
de_core_news_sm 166
Deep Learning 28, 149
DeepPavlov 108
deepset.ai 157
Default Handler 131
Delta Load 313
Dense-Layer 262
Dependency Parsing 174
Dependency Visualizer 104
Determinationskoeffizient 234
Dialog Engine 20, 108
Dialogflow 108
DialoGPT 109, 311
Dictionary 144
Differenzenbildung 252
Dirac Notation 216
displacy 165
DWD 238
E
Editierdistanz siehe Levenshtein-Distanz
Einfrieren (Environment) 302
Electron 312
ELIZA 68
Embedding 36, 54, 57, 89, 169
Embedding Space 172
Encoder 36, 41, 49, 168
Encoder-Decoder-Architektur 41
End-Of-Sequence siehe EOS
Entanglement 216
environement.yml 62
Environment 299
EOS 168
Epoche 158
Erinnerung 185
eSpeak 28, 30, 39
ETL siehe Extract, Transform, Load
Excel (Tabellenkalkulation) 232
Executable 299
Extract, Transform, Load 313
F
F1-Score 98
False Negative 99
False Positive 99
Fast Fourier Transformation siehe FFT
Feature Engineering 258
Feed Forward 172, 181
Feed Forward Netzwerk 256
FFmpeg 15, 204, 299
FFT 72
Fibonacci-Folge 232
__file__ 130
Fine Tuning 62, 157
Fingerabdruck (Stimme) 89, 283
Finite State Machine siehe FSM
fit() 234
Flask 135
Fragetypen 150
FSM 87
Funktionen von Sprachassistenten 149
Fuzzy 204
Fuzzylogik 214
fuzzywuzzy 211, 283
G
Game Engine 141
GAN siehe Generative Adversarial Network
Gated Recurrent Units 42
Gaussian Error Linear Units siehe GELU
GELU 182
Generative Adversarial Network 28, 38
Generative Pre-Trained Transformer siehe GPT
Gensim 172
Geocoder 229
Geradengleichung 234
german-gpt2 167
getattr() 132
GIMP 296
Git 15
– git add 17
– git checkout 18
– git clone 16
– git commit 17
– git init 16
– git merge 19
– git pull 18



Stichwortverzeichnis 321
– git push 17
– git revert 19
Github 15
Github Repository 2
Gleitender Mittelwert 245
glob 127, 306
globals() 129
GPT 167
GPT-2 109
GPU 157, 162, 168
Gradient Decent 263
Griffin-Lim 54, 62
Groß- und Kleinschreibung 105
Ground Truth 183
Ground Truth Answers 151
GRU siehe Gated Recurrent Units
H
Hadamard Gatter 216
hasattr() 195
HfArgumentParsers 158
Hidden State 41, 168, 174
Holdout Dataset siehe Test Dataset
HTTP 135
HTTP-Request 271
Hugging Face 97, 109, 152
Hypertext Transfer Protocol siehe HTTP
I
IDE 9
Inference 156
Installer 309
install_requirements() 126
Intent 93, 141
Intent Classifier 119
Intent Management 120, 126, 148, 195
Intent Parser 120
Intent Parsing 104
Intent Processing 20
Intent Recognition 94
Intents einschränken 224
Interpreter 307
J
JavaScript 312
Joseph Weizenbaum 68
jsonify() 135
Jupyter Notebook 12, 314
K
Kaggle 10
KeePass 280
Kelvin 253
Keras 257
Key 176
keyboard 283
KeyboardInterrupt 85
Key Matrix 177
KI-Sommer 67
Kivy 288
KI-Winter 68
Klassifikation 95
Kommunikation 94
Kosinusähnlichkeit 171, 176
Kotlin 312
L
Lag Variable 244
Language Model 155
Language Modelling 154
Lautstärke 186, 197
Learning Rate 158
Levenshtein-Distanz 219
libavg 288
LibriSpeech 53
Licht 269
Linear Layer 175
Linguistic Feature 173
LOC 166
Logging 23
– loguru 23
Logischer Operator 215
Logisches Oder 127
Long Short-Term Memory 36, 42 
siehe auch LSTM
Look Ahead Mask 183
Lose Kopplung 140
Loss 50, 51, 183
Loss Function 263
LSTM 256 siehe auch Long Short-Term 
Memory
Luftfeuchtigkeit 241
M
<mask> 155
Masked Multi-Head Attention 182
Matrixmultiplikation 177
mean() 245
Mean Average Error 263



322 Stichwortverzeichnis
Mean Squared Error 51, 263
MelGAN 37
Mel Spectrogram 28, 37
Menü (UI) 288
MERGED_FILE 130
Meta Demo Lab 39
Microservice 135
Microsoft TTS 28
Mini-Batch Gradient Decent 263
Miniconda 15
MISC 166
mixer 144, 208
MLR 234
Model Hub 152
Modularisierung 120
Moving Average Model 243, 245
Mozilla TTS 35
Multi-Head Attention 174
Multiple Linear Regression siehe MLR
multiprocessing 204, 302
– fork 33
– spawn 33
Multiprocessing 31
N
__name__ 130
Named Entity Recognition siehe NER
Natural Language Processing 27, 39
Natural Language Understanding siehe NLU
NER 165
Next Sentence Prediction 154
NGINX 139
NLLB-200 siehe No Language Left Behind
nlnet 151
NLP 109, 149 siehe auch Natural Language 
Processing
NLTK 109, 154
NLU 96, 109
No Language Left Behind 39
Normalisierung 172
Notification 295
notify-py 295
NSSpeechSynthesizer 28
num2words 190
NumPy 222
O
Objective-C 312
ogg 143
on-premises 3
Open Source 10
Open Weather Map 223, 229
Optimizer 263
Ordinalia 190
Ordnung 244
ORG 166
Ortssuche 107
OSI-Modell 251
os.path.join() 123
Overfitting 238
P
PACF 255
Padding 59
Pad-Token 157
Paketieren (Anwendung) 299
Pandas 240
Partial Autocorrelation Function siehe PACF
Passwortverwaltung 280
Pauli X Gatter 216
PEP 125
PER 165
Phase Shift Gatter 216
Phonem 39
Phoneme 28
pip 29
– pip freeze 31
– pip install 31
pipeline 152, 167
pip freeze 62
pipwin 83
pmdarima 255
Polynomiale Regression 236
porcupine 82
Positional Embedding siehe Positional Encoding
Positional Encoding 172
position-wise Feed-Forward Layer 182
pprint 130
Precision 98
Preprocessing 50
Pretty Print siehe pprint
Programmiersprachen 9
Progressive Web Apps 312
Prosodie 27
Punktwolke 238
PyAudio 82
Pygame 141
PyInstaller 301
pykeepass 282
PyOWM 229



Stichwortverzeichnis 323
PyQt 288
PySide2 288
Python 9
– __init__() 24
– Klasse 22
– Klasseninstanz 22
python-dateutil 190
Python Enhancement Proposal 22
Python Virtual Machine 307
pythonw 294
PyTorch 46, 151
pyttsx3 28
pytz 118
Q
Q20 271
QML 217
Qt Designer 288
Qt-Material 288
Quantengatter 217
Quantum Computing 214
Quantum Machine Learning siehe QML
Qubit 215
Query 176
Query Matrix 177
Question Answering 149
QuestionAnsweringTrainer 161
Queue 205
R
Radiosender 212
Rasa NLU 108
Raspberry Pi 15, 109, 311
React 312
Real Time Voice Cloning 35
Reasoning 166
Recall 98
Rectified Linear Unit siehe ReLU
Recurrent Neural Network 40
redirect (Log) 291
@register_call 144
Regressand 233
Regression 233
Regressor 233
Regular Expression 108, 116, 121
Relative Pfade 304
ReLU 181
Representational State Transfer siehe REST
requirements.txt 62, 126, 148, 151, 308
resemble.ai 36
Residual Connection 169, 180
respond() 133
REST 135
RMSProp 263
RNN 168, 172, 256 siehe auch Recurrent 
Neural Network
Root-Mean-Squared Error 255
S
Sample Rate 60, 207
SARIMA 255
SARIMAX 255
Scikit-Learn 233
Scrum 312
seed 158
Self-Attention 174
[SEP] 159 siehe auch Separator
Separator 43
Seq2Seq siehe Sequence-To-Sequence
Sequence-To-Sequence 39, 97, 168
Sequence-To-Vector 40
series (DataFrame) 245
session_id 144
setup.py 302
sigmoide Aktivierungsfunktion 181
Simulation und Analytics 267
Skalarprodukt 171
sklearn.metrics 255
Slicing 277
Smalltalk 111
Smart Home 268
Snips-NLU 108, 115, 116, 130, 147, 210, 
226, 280
Softmax 178
Software Engineering 312
sounddevice 204
soundfile 204
SpaCy 103, 109, 154, 165, 170
Speaker Identification 20
Spracherkennung 67, 87
Sprachsynthese 27
Sprechererkennung 67
SQuAD 151
SSL 137
Standort (IP) 229
Stanford Question Answering Dataset 2.0 
siehe SQuAD
stationär 247
Stationarität 252
Stationarity 248



324 Stichwortverzeichnis
Station (Wetter) 240
Statsmodels 252
stderr siehe stdin
stdin 294
stdout 207, 302 siehe auch stdin
Stephen Hawking 27
Stimme klonen 60
Stochastic Gradient Decent 263
Stopword 164
Stream (Musik) 204
Stride 158
Subword Tokenization 156, 184
Superposition 215
Support 98
Swift 312
Synonyme 117
Synsets 117
Synthesizer 37, 53
T
tabulate 240
Tacotron 37
Tacotron2 37
Tangent Hyperbolic Function siehe tanh
tanh 181
Template 146, 148
TensorFlow 257
TensorFlow Quantum 217
Test Dataset 51
text2numde 199, 211
Text Extraction 153
Text-To-Speech 20, 27
– Articulatory Synthesis 27
– Concatenative Synthesis 27
– Formant Synthesis 27
– Neural Speech Synthesis 28
– Statistical Parametric Synthesis 27
– TTS-Engine 35
Textzusammenfassung 40
Thread 31
Tierstimmen 141
Time Series 232
TinyDB 89, 190
TIOBE Index 312
Tkinter 288
Toast Notification 295
Token 40, 41, 154
Tokenizer 97, 152, 159
Token Vector 156
Tooltip 288
train_and_predict() 234
Training Dataset 51
Trainingsdaten 44, 48, 106, 120, 168
Transfer Learning 154, 156
Transformer 39, 168
transformers 152
Transformers 108
transponieren (Matrix) 175
Tray Icon 287
True Negative 99
True Positive 99
TTS siehe Text-To-Speech
Tunneleffekt 215
Turing-Test 67
type() 125
U
Übersetzung 96
UI siehe User Interface
UMAP Projection 57
Unschärfelogik siehe Fuzzylogik
User Interface 287
User Management 20
V
Validation Dataset 51
Value 176
Value Matrix 177
Vanishing Gradient Problem 44
Vector-To-Sequence 40
Verbergen der Konsole 294
Verlustrate siehe Loss
Vertrauen in KI 155
virtualenv siehe Virtuelle Umgebung
Virtuelle Umgebung 13
visdom 50
Vocoder 37, 55
Vorverarbeitung siehe Preprocessing
VOSK 87, 283
W
Wake Word 3, 20, 82
WaveNet 37
Webradio 210
Wetter 223
Wetterdaten 238
wetterdienst 238
Wheel 14, 82
Wikipedia 146, 166
WILDCARD_FILE 130



Stichwortverzeichnis 325
wildcard.template 121
Word2Vec 172
Wordnet 117
Word Tokenization 169
Write With Transformer 167
wxPython 288
Y
YAML 78
Z
Zeitzone 118, 122
Zirkuläre Referenzen 105



 Sprachassistenten werden vermehrt in Bereichen wie 
Kundenkommunikation, Smart Home oder Automotive 
eingesetzt. Dieses Buch zeigt Ihnen, wie Sie in Python 
Schritt für Schritt einen eigenen Sprachassistenten 
komplett selbst entwickeln – von der Architektur bis zur 
Paketierung der Anwendung.
Sie lernen, wie Sprachanalyse, -synthese und das Erken nen einer Benutzerintention funktionieren und wie Sie 
diese praktisch umsetzen. Und Sie kommen mit vielen 
Themen aus der professionellen Python-Entwicklung 
in Berührung, u. a. mit Logging, dem dynamischen Ins tallieren von Paketen, dem »Einfrieren« einer Anwen dung oder der Überführung in einen Installer.
Ein weiteres wichtiges Thema ist der Datenschutz. 
Wenn Sie einen eigenen Assistenten programmiert
haben, wissen Sie genau, welche Daten Sie raus geben und welche auf Ihrem Gerät verarbeitet werden. 
Schreiben Sie Intents, denen selbst sensible Daten 
anvertraut werden können. Das schafft auch Vertrauen 
beim Anwender.
Darüber hinaus kann Ihr selbstprogrammierter Assis tent ein paar Dinge mehr als die Marktführer. So können 
Sie ihm zum Beispiel erlauben, nur auf Ihre Stimme zu reagieren 
und andere Personen zu ignorieren.
Dr. Jonas 
FREIKNECHT 
arbeitet als 
Lead Expert 
für KI und 
Big Data bei 
einem größeren 
IT-Systemhaus. Er hat in prak tischer Informatik promoviert, 
gibt Kurse und schreibt Fach bücher zu diversen IT-Themen.
AUS DEM INHALT //
■ Aufsetzen einer Umgebung für 
professionelle Python-Projekte
■ Training einer eigenen 
Text-to-Speech-Engine
■ Identifi kation der Benutzer 
anhand ihrer Stimme
■ Klassifi kation und Interpreta tion von Sprachbefehlen
■ Erklärung und Verwendung 
von Transformers für NLP und NLU-Aufgaben
■ Führen langer Dialoge über 
ein Kontextgedächtnis
■ Entwicklung von zehn 
funktionsfähigen Skills
■ Training einer eigenen 
Wettervorhersage
■ Build und Paketierung einer 
Python-Anwendung
KI-SPRACHASSISTENTEN 
MIT PYTHON ENTWICKELN //
■ Entwicklung eines eigenen Sprachassistenten 
von Beginn an
■ Erlernen fortgeschrittener Konzepte in Python 
anhand des Entwicklungspfads
■ Verzicht auf Cloud-APIs für die Kernfunktio nalität – alles entsteht in Eigenregie
■ Verwendung verschiedener Machine- und 
Deep-Learning-Ansätze zur Integration von KI 
(u. a. LSTMs, ARIMA und Transformers)
■ Berücksichtigt auch die Themen Datenschutz 
und IT-Sicherheit
ISBN 978-3-446-47231-0
9 783446 472310
FREIKNECHT
KI-SPRACHASSISTENTEN 
mit Python entwickeln
EXTRA
E-Book
INSIDE
www.hanser-fachbuch.de/computer


